#!/usr/bin/env python3
"""
RAGTrace CLI ì¸í„°í˜ì´ìŠ¤

RAGAS í‰ê°€ë¥¼ ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” CLI ë„êµ¬
í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ ê¸°ëŠ¥ í¬í•¨
"""

import argparse
import sys
from typing import Optional

from src.config import settings, PROMPT_TYPE_HELP, set_prompt_type_for_session
from src.container import create_container_with_prompts, container
from src.domain.prompts import PromptType
from src.utils.paths import get_available_datasets


def create_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="RAGTrace - RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€ ì‹¤í–‰
  python cli.py evaluate evaluation_data

  # í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
  python cli.py evaluate evaluation_data --prompt-type korean_tech
  
  # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°
  python cli.py list-datasets
  
  # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì •ë³´ ë³´ê¸°
  python cli.py list-prompts
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    
    # evaluate ì„œë¸Œì»¤ë§¨ë“œ
    eval_parser = subparsers.add_parser("evaluate", help="RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹¤í–‰")
    eval_parser.add_argument(
        "dataset",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (í™•ì¥ì ì œì™¸)"
    )
    eval_parser.add_argument(
        "--prompt-type", 
        choices=[pt.value for pt in PromptType],
        default=None,
        help="ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì…"
    )
    eval_parser.add_argument(
        "--output",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    eval_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    # list-datasets ì„œë¸Œì»¤ë§¨ë“œ
    list_parser = subparsers.add_parser("list-datasets", help="ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°")
    
    # list-prompts ì„œë¸Œì»¤ë§¨ë“œ
    prompts_parser = subparsers.add_parser("list-prompts", help="ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ë³´ê¸°")
    
    return parser


def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥"""
    print("ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
    print("-" * 40)
    
    datasets = get_available_datasets()
    if not datasets:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   data/ ë””ë ‰í† ë¦¬ì— JSON í˜•ì‹ì˜ í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    for i, dataset in enumerate(datasets, 1):
        print(f"{i}. {dataset}")
    
    print(f"\nì´ {len(datasets)}ê°œì˜ ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤.")


def list_prompts():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ëª©ë¡ ì¶œë ¥"""
    print("ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì…:")
    print("-" * 50)
    
    for prompt_type in PromptType:
        description = PROMPT_TYPE_HELP.get(prompt_type, "ì„¤ëª… ì—†ìŒ")
        status = "âœ… ê¸°ë³¸ê°’" if prompt_type == PromptType.DEFAULT else "ğŸ”§ ì»¤ìŠ¤í…€"
        print(f"{status} {prompt_type.value:20} : {description}")
    
    print(f"\ní˜„ì¬ ê¸°ë³¸ ì„¤ì •: {settings.get_prompt_type().value}")
    
    if settings.is_custom_prompt_enabled():
        print("ğŸ’¡ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ --prompt-type ì˜µì…˜ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def evaluate_dataset(dataset_name: str, prompt_type: Optional[str] = None, 
                    output_file: Optional[str] = None, verbose: bool = False):
    """ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰"""
    
    # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ì •
    if prompt_type:
        try:
            selected_prompt_type = PromptType(prompt_type)
            set_prompt_type_for_session(selected_prompt_type)
        except ValueError:
            print(f"âŒ ì˜ëª»ëœ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ì…: {[pt.value for pt in PromptType]}")
            return False
    else:
        selected_prompt_type = settings.get_prompt_type()
    
    print(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {selected_prompt_type.value}")
    print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")
    
    # ë°ì´í„°ì…‹ í™•ì¸
    available_datasets = get_available_datasets()
    if dataset_name not in available_datasets:
        print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
        for ds in available_datasets:
            print(f"  - {ds}")
        return False
    
    try:
        # ì»¨í…Œì´ë„ˆ ìƒì„±
        if selected_prompt_type == PromptType.DEFAULT:
            eval_container = container
        else:
            eval_container = create_container_with_prompts(selected_prompt_type)
        
        # í‰ê°€ ì‹¤í–‰
        print("\nğŸš€ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        evaluation_use_case = eval_container.get_run_evaluation_use_case(dataset_name)
        
        if verbose:
            print("ğŸ“ ìƒì„¸ ë¡œê·¸ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        result = evaluation_use_case.execute()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nâœ… í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("=" * 50)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print("-" * 30)
        
        result_dict = result.to_dict()
        
        # ë©”ì¸ ë©”íŠ¸ë¦­ ì¶œë ¥
        print(f"ğŸ† ì¢…í•© ì ìˆ˜ (RAGAS Score): {result_dict.get('ragas_score', 0):.3f}")
        print(f"âœ… Faithfulness:           {result_dict.get('faithfulness', 0):.3f}")
        print(f"ğŸ¯ Answer Relevancy:        {result_dict.get('answer_relevancy', 0):.3f}")
        print(f"ğŸ”„ Context Recall:          {result_dict.get('context_recall', 0):.3f}")
        print(f"ğŸ“ Context Precision:       {result_dict.get('context_precision', 0):.3f}")
        
        # ë©”íƒ€ë°ì´í„° ì¶œë ¥
        if verbose and 'metadata' in result_dict:
            metadata = result_dict['metadata']
            print("\nğŸ“‹ í‰ê°€ ì •ë³´:")
            print(f"  í‰ê°€ ID: {metadata.get('evaluation_id', 'N/A')}")
            print(f"  ëª¨ë¸: {metadata.get('model', 'N/A')}")
            print(f"  ë°ì´í„°ì…‹ í¬ê¸°: {metadata.get('dataset_size', 'N/A')}")
            print(f"  í‰ê°€ ì‹œê°„: {metadata.get('timestamp', 'N/A')}")
        
        # íŒŒì¼ ì €ì¥
        if output_file:
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        if verbose:
            import traceback
            traceback.print_exc()
        return False


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
        from src.config import validate_settings
        validate_settings()
        
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "list-datasets":
        list_datasets()
        
    elif args.command == "list-prompts":
        list_prompts()
        
    elif args.command == "evaluate":
        success = evaluate_dataset(
            dataset_name=args.dataset,
            prompt_type=args.prompt_type,
            output_file=args.output,
            verbose=args.verbose
        )
        if not success:
            sys.exit(1)
    
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()