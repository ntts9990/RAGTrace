#!/usr/bin/env python3
"""
RAGTrace CLI ì¸í„°í˜ì´ìŠ¤

RAGAS í‰ê°€ë¥¼ ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” CLI ë„êµ¬
í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ ê¸°ëŠ¥ í¬í•¨
"""

import argparse
import sys
import json
from typing import Optional
from pathlib import Path

from src.config import (
    settings, 
    PROMPT_TYPE_HELP, 
    SUPPORTED_LLM_TYPES, 
    SUPPORTED_EMBEDDING_TYPES
)
from src.container.main_container import container
from src.domain.prompts import PromptType
from src.utils.paths import get_available_datasets, get_evaluation_data_path
from src.infrastructure.data_import.importers import ImporterFactory
from src.infrastructure.data_import.validators import ImportDataValidator


def create_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="RAGTrace - RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€ ì‹¤í–‰
  python cli.py evaluate evaluation_data.json

  # LLMê³¼ ì„ë² ë”© ëª¨ë¸ ì„ íƒ
  python cli.py evaluate evaluation_data.json --llm gemini --embedding hcx
  
  # í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
  python cli.py evaluate evaluation_data.json --prompt-type korean_tech
  
  # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°
  python cli.py list-datasets
  
  # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì •ë³´ ë³´ê¸°
  python cli.py list-prompts
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    
    # evaluate ì„œë¸Œì»¤ë§¨ë“œ
    eval_parser = subparsers.add_parser("evaluate", help="RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹¤í–‰")
    eval_parser.add_argument(
        "dataset",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (í™•ì¥ì ì œì™¸)"
    )
    eval_parser.add_argument(
        "--llm",
        choices=SUPPORTED_LLM_TYPES,
        default=settings.DEFAULT_LLM,
        help=f"í‰ê°€ì— ì‚¬ìš©í•  LLM (ê¸°ë³¸ê°’: {settings.DEFAULT_LLM})"
    )
    eval_parser.add_argument(
        "--embedding",
        choices=SUPPORTED_EMBEDDING_TYPES,
        default=None,
        help=f"í‰ê°€ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸ê°’: {settings.DEFAULT_EMBEDDING}). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )
    eval_parser.add_argument(
        "--prompt-type", 
        choices=[pt.value for pt in PromptType],
        default=None,
        help="ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì…"
    )
    eval_parser.add_argument(
        "--output",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    eval_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    # list-datasets ì„œë¸Œì»¤ë§¨ë“œ
    list_parser = subparsers.add_parser("list-datasets", help="ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°")
    
    # list-prompts ì„œë¸Œì»¤ë§¨ë“œ
    prompts_parser = subparsers.add_parser("list-prompts", help="ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ë³´ê¸°")
    
    # import-data ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    import_parser = subparsers.add_parser("import-data", help="Excel/CSV íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
    import_parser.add_argument(
        "input_file",
        help="ë³€í™˜í•  Excel(.xlsx, .xls) ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ"
    )
    import_parser.add_argument(
        "--output", "-o",
        help="ë³€í™˜ëœ JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥íŒŒì¼ëª….json)"
    )
    import_parser.add_argument(
        "--validate", "-v",
        action="store_true",
        help="ë³€í™˜ëœ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰"
    )
    import_parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 50)"
    )
    
    return parser


def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥"""
    print("ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
    print("-" * 40)
    
    datasets = get_available_datasets()
    if not datasets:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   data/ ë””ë ‰í† ë¦¬ì— JSON í˜•ì‹ì˜ í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    for i, dataset in enumerate(datasets, 1):
        print(f"{i}. {dataset}")
    
    print(f"\nì´ {len(datasets)}ê°œì˜ ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤.")


def list_prompts():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ëª©ë¡ ì¶œë ¥"""
    print("ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì…:")
    print("-" * 50)
    
    for prompt_type in PromptType:
        description = PROMPT_TYPE_HELP.get(prompt_type, "ì„¤ëª… ì—†ìŒ")
        status = "âœ… ê¸°ë³¸ê°’" if prompt_type == PromptType.DEFAULT else "ğŸ”§ ì»¤ìŠ¤í…€"
        print(f"{status} {prompt_type.value:20} : {description}")
    
    print(f"\ní˜„ì¬ ê¸°ë³¸ ì„¤ì •: {settings.get_prompt_type().value}")
    
    if settings.is_custom_prompt_enabled():
        print("ğŸ’¡ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ --prompt-type ì˜µì…˜ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def import_data(input_file: str, output_file: Optional[str] = None, 
               validate: bool = False, batch_size: int = 50):
    """Excel/CSV íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    try:
        # Import ì–´ëŒ‘í„° ìƒì„±
        importer = ImporterFactory.create_importer(input_file)
        
        # íŒŒì¼ í˜•ì‹ ê²€ì¦
        if not importer.validate_format(input_file):
            print(f"âŒ íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}")
            print(f"ì§€ì›ë˜ëŠ” í˜•ì‹: {ImporterFactory.get_supported_formats()}")
            return False
        
        print(f"ğŸ“‚ íŒŒì¼ ë³€í™˜ ì‹œì‘: {input_file}")
        
        # ë°ì´í„° Import
        evaluation_data_list = importer.import_data(input_file)
        
        if not evaluation_data_list:
            print("âŒ ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… {len(evaluation_data_list)}ê°œ í•­ëª© ë³€í™˜ ì™„ë£Œ")
        
        # ê²€ì¦ ìˆ˜í–‰ (ì˜µì…˜)
        if validate:
            validator = ImportDataValidator()
            validation_result = validator.validate_data_list(evaluation_data_list)
            
            print("\n" + validator.get_validation_summary(validation_result))
            
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´ ì¶œë ¥
            if not validation_result.is_valid:
                print("\nğŸ“‹ ì˜¤ë¥˜ ìƒì„¸:")
                for error in validation_result.errors[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    print(f"   {error}")
                
                if len(validation_result.errors) > 10:
                    print(f"   ... ì™¸ {len(validation_result.errors) - 10}ê°œ ì˜¤ë¥˜")
                
                if validation_result.warnings:
                    print("\nâš ï¸ ê²½ê³  ìƒì„¸:")
                    for warning in validation_result.warnings[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        print(f"   {warning}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì •
        if not output_file:
            input_path = Path(input_file)
            output_file = str(input_path.with_suffix('.json'))
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        from dataclasses import asdict
        output_data = [asdict(data) for data in evaluation_data_list]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë³€í™˜ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ ì¶œë ¥
        if len(evaluation_data_list) > batch_size:
            estimated_batches = (len(evaluation_data_list) + batch_size - 1) // batch_size
            print(f"\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´:")
            print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
            print(f"   ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {estimated_batches}")
            print(f"   ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return False


def evaluate_dataset(dataset_name: str, llm: str, embedding: Optional[str] = None, 
                    prompt_type: Optional[str] = None, output_file: Optional[str] = None, 
                    verbose: bool = False):
    """ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰"""
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ (ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©)
    embedding_choice = embedding or settings.DEFAULT_EMBEDDING
    
    # LLM ë° ì„ë² ë”© ì–´ëŒ‘í„° ì„ íƒ
    try:
        # íŒ©í† ë¦¬ë¥¼ í†µí•´ UseCase ìƒì„±
        from src.container.factories.evaluation_use_case_factory import EvaluationRequest
        
        request = EvaluationRequest(
            llm_type=llm,
            embedding_type=embedding_choice,
            prompt_type=PromptType(prompt_type) if prompt_type else settings.get_prompt_type()
        )
        
        evaluation_use_case, _, _ = container.create_evaluation_use_case(request)
        
        if llm in ["hcx"] and not settings.CLOVA_STUDIO_API_KEY:
            print(f"âŒ '{llm}' LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False
        if embedding_choice in ["hcx"] and not settings.CLOVA_STUDIO_API_KEY:
            print(f"âŒ '{embedding_choice}' ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False

    except Exception as e:
        print(f"âŒ '{llm}' LLM ë˜ëŠ” '{embedding_choice}' ì„ë² ë”© ì–´ëŒ‘í„° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    print(f"ğŸ¤– LLM: {llm}")
    print(f"ğŸŒ Embedding: {embedding_choice}")

    # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ì •
    prompt_type_enum = PromptType(prompt_type) if prompt_type else settings.get_prompt_type()
    print(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type_enum.value.upper()}")

    try:
        data_path = get_evaluation_data_path(dataset_name)
        if not data_path:
            print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            available_datasets = get_available_datasets()
            if available_datasets:
                print("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
                for ds in available_datasets:
                    print(f"  - {ds}")
            return False
            
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")

        result = evaluation_use_case.execute(
            dataset_name=dataset_name,
        )
        
        if not result:
            print("âŒ í‰ê°€ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        print(f"ragas_score  : {result.ragas_score:.4f}")
        print(f"answer_relevancy: {result.answer_relevancy:.4f}")
        print(f"faithfulness   : {result.faithfulness:.4f}")
        print(f"context_recall : {result.context_recall:.4f}")
        print(f"context_precision: {result.context_precision:.4f}")
        print("="*50)

        if output_file:
            import json
            from dataclasses import asdict
            
            # ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(result), f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ê²°ê³¼ê°€ {output_file} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âœ… í‰ê°€ ì™„ë£Œ.")
        
        return True

    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        if verbose:
            import traceback
            traceback.print_exc()
        return False


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
        from src.config import validate_settings
        validate_settings()
        
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "list-datasets":
        list_datasets()
        
    elif args.command == "list-prompts":
        list_prompts()
        
    elif args.command == "evaluate":
        success = evaluate_dataset(
            dataset_name=args.dataset,
            llm=args.llm,
            embedding=args.embedding,
            prompt_type=args.prompt_type,
            output_file=args.output,
            verbose=args.verbose
        )
        if not success:
            sys.exit(1)
    
    elif args.command == "import-data":
        success = import_data(
            input_file=args.input_file,
            output_file=args.output,
            validate=args.validate,
            batch_size=args.batch_size
        )
        if not success:
            sys.exit(1)
    
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()