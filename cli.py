#!/usr/bin/env python3
"""
RAGTrace CLI ì¸í„°í˜ì´ìŠ¤

RAGAS í‰ê°€ë¥¼ ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” CLI ë„êµ¬
í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ ê¸°ëŠ¥ í¬í•¨
"""

import argparse
import sys
import json
import statistics
from typing import Optional
from pathlib import Path
from datetime import datetime

from src.config import (
    settings, 
    PROMPT_TYPE_HELP, 
    SUPPORTED_LLM_TYPES, 
    SUPPORTED_EMBEDDING_TYPES
)
from src.container import container
from src.domain.prompts import PromptType
from src.utils.paths import get_available_datasets, get_evaluation_data_path
from src.infrastructure.data_import.importers import ImporterFactory
from src.infrastructure.data_import.validators import ImportDataValidator


def create_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="RAGTrace - RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (HCX-005 + BGE-M3, ìë™ ê²°ê³¼ ì €ì¥)
  python cli.py quick-eval evaluation_data
  
  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€ ì‹¤í–‰
  python cli.py evaluate evaluation_data.json

  # LLMê³¼ ì„ë² ë”© ëª¨ë¸ ì„ íƒ
  python cli.py evaluate evaluation_data.json --llm gemini --embedding hcx
  
  # í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
  python cli.py evaluate evaluation_data.json --prompt-type korean_tech
  
  # í‰ê°€ ê²°ê³¼ ê³ ê¸‰ í†µê³„ ë¶„ì„
  python cli.py analyze-results results.json --analysis-type all
  
  # ì—¬ëŸ¬ í‰ê°€ ê²°ê³¼ ë¹„êµ ë¶„ì„
  python cli.py compare-results model1.json model2.json --labels "Model A" "Model B"
  
  # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°
  python cli.py list-datasets
  
  # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì •ë³´ ë³´ê¸°
  python cli.py list-prompts
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    
    # evaluate ì„œë¸Œì»¤ë§¨ë“œ
    eval_parser = subparsers.add_parser("evaluate", help="RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹¤í–‰")
    eval_parser.add_argument(
        "dataset",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (í™•ì¥ì ì œì™¸)"
    )
    eval_parser.add_argument(
        "--llm",
        choices=SUPPORTED_LLM_TYPES,
        default=settings.DEFAULT_LLM,
        help=f"í‰ê°€ì— ì‚¬ìš©í•  LLM (ê¸°ë³¸ê°’: {settings.DEFAULT_LLM})"
    )
    eval_parser.add_argument(
        "--embedding",
        choices=SUPPORTED_EMBEDDING_TYPES,
        default=None,
        help=f"í‰ê°€ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸ê°’: {settings.DEFAULT_EMBEDDING}). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )
    eval_parser.add_argument(
        "--prompt-type", 
        choices=[pt.value for pt in PromptType],
        default=None,
        help="ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì…"
    )
    eval_parser.add_argument(
        "--output",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    eval_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    # list-datasets ì„œë¸Œì»¤ë§¨ë“œ
    list_parser = subparsers.add_parser("list-datasets", help="ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°")
    
    # list-prompts ì„œë¸Œì»¤ë§¨ë“œ
    prompts_parser = subparsers.add_parser("list-prompts", help="ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ë³´ê¸°")
    
    # import-data ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    import_parser = subparsers.add_parser("import-data", help="Excel/CSV íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
    import_parser.add_argument(
        "input_file",
        help="ë³€í™˜í•  Excel(.xlsx, .xls) ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ"
    )
    import_parser.add_argument(
        "--output", "-o",
        help="ë³€í™˜ëœ JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥íŒŒì¼ëª….json)"
    )
    import_parser.add_argument(
        "--validate", "-v",
        action="store_true",
        help="ë³€í™˜ëœ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰"
    )
    import_parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 50)"
    )
    
    # export-results ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    export_parser = subparsers.add_parser("export-results", help="í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°")
    export_parser.add_argument(
        "result_file",
        help="ë‚´ë³´ë‚¼ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ"
    )
    export_parser.add_argument(
        "--format",
        choices=["csv", "summary", "report", "all"],
        default="all",
        help="ë‚´ë³´ë‚¼ í˜•ì‹ (ê¸°ë³¸ê°’: all)"
    )
    export_parser.add_argument(
        "--output-dir", "-o",
        default="exports",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: exports)"
    )
    
    # list-checkpoints ì„œë¸Œì»¤ë§¨ë“œ
    checkpoint_list_parser = subparsers.add_parser("list-checkpoints", help="ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ë³´ê¸°")
    
    # resume-evaluation ì„œë¸Œì»¤ë§¨ë“œ
    resume_parser = subparsers.add_parser("resume-evaluation", help="ì¤‘ë‹¨ëœ í‰ê°€ ì¬ê°œ")
    resume_parser.add_argument(
        "session_id",
        help="ì¬ê°œí•  í‰ê°€ ì„¸ì…˜ ID"
    )
    
    # cleanup-checkpoints ì„œë¸Œì»¤ë§¨ë“œ
    cleanup_parser = subparsers.add_parser("cleanup-checkpoints", help="ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬")
    cleanup_parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="ë³´ì¡´í•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)"
    )
    
    # quick-eval ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    quick_parser = subparsers.add_parser("quick-eval", help="ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (HCX-005 + BGE-M3, ìë™ ê²°ê³¼ ì €ì¥)")
    quick_parser.add_argument(
        "dataset",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (í™•ì¥ì ì œì™¸)"
    )
    quick_parser.add_argument(
        "--output-dir",
        default="quick_results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: quick_results)"
    )
    quick_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    # analyze-results ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    analyze_parser = subparsers.add_parser("analyze-results", help="í‰ê°€ ê²°ê³¼ ê³ ê¸‰ í†µê³„ ë¶„ì„")
    analyze_parser.add_argument(
        "result_file",
        help="ë¶„ì„í•  í‰ê°€ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ"
    )
    analyze_parser.add_argument(
        "--analysis-type",
        choices=["basic", "eda", "advanced", "all"],
        default="all", 
        help="ë¶„ì„ ìœ í˜• (ê¸°ë³¸ê°’: all)"
    )
    analyze_parser.add_argument(
        "--output-dir",
        default="analysis_results",
        help="ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: analysis_results)"
    )
    analyze_parser.add_argument(
        "--save-plots",
        action="store_true",
        help="ì°¨íŠ¸ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥"
    )
    
    # compare-results ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    compare_parser = subparsers.add_parser("compare-results", help="ì—¬ëŸ¬ í‰ê°€ ê²°ê³¼ ë¹„êµ ë¶„ì„")
    compare_parser.add_argument(
        "result_files",
        nargs="+",
        help="ë¹„êµí•  í‰ê°€ ê²°ê³¼ JSON íŒŒì¼ë“¤"
    )
    compare_parser.add_argument(
        "--output-dir",
        default="comparison_results",
        help="ë¹„êµ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: comparison_results)"
    )
    compare_parser.add_argument(
        "--labels",
        nargs="*",
        help="ê° ê²°ê³¼ íŒŒì¼ì˜ ë¼ë²¨ (ë¯¸ì§€ì • ì‹œ íŒŒì¼ëª… ì‚¬ìš©)"
    )
    
    return parser


def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥"""
    print("ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
    print("-" * 60)
    
    datasets = get_available_datasets()
    if not datasets:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   data/ ë””ë ‰í† ë¦¬ì— í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        print("   ì§€ì› í˜•ì‹: JSON, CSV, Excel (.xlsx, .xls)")
        return
    
    # íŒŒì¼ í˜•ì‹ë³„ë¡œ ê·¸ë£¹í™”
    json_files = [d for d in datasets if d.endswith('.json')]
    csv_files = [d for d in datasets if d.endswith('.csv')]
    excel_files = [d for d in datasets if d.endswith(('.xlsx', '.xls'))]
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
    file_num = 1
    
    if json_files:
        print("\nğŸ“„ JSON íŒŒì¼:")
        for dataset in json_files:
            print(f"  {file_num}. {dataset}")
            file_num += 1
    
    if csv_files:
        print("\nğŸ“Š CSV íŒŒì¼:")
        for dataset in csv_files:
            print(f"  {file_num}. {dataset} (ë³€í™˜ í•„ìš”)")
            file_num += 1
    
    if excel_files:
        print("\nğŸ“ˆ Excel íŒŒì¼:")
        for dataset in excel_files:
            print(f"  {file_num}. {dataset} (ë³€í™˜ í•„ìš”)")
            file_num += 1
    
    print(f"\nì´ {len(datasets)}ê°œì˜ ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤.")
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("  - JSON: python cli.py evaluate <filename>")
    print("  - CSV/Excel: python cli.py import-data <filename> --output converted.json")


def list_prompts():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ëª©ë¡ ì¶œë ¥"""
    print("ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì…:")
    print("-" * 50)
    
    for prompt_type in PromptType:
        description = PROMPT_TYPE_HELP.get(prompt_type, "ì„¤ëª… ì—†ìŒ")
        status = "âœ… ê¸°ë³¸ê°’" if prompt_type == PromptType.DEFAULT else "ğŸ”§ ì»¤ìŠ¤í…€"
        print(f"{status} {prompt_type.value:20} : {description}")
    
    print(f"\ní˜„ì¬ ê¸°ë³¸ ì„¤ì •: {settings.get_prompt_type().value}")
    
    if settings.is_custom_prompt_enabled():
        print("ğŸ’¡ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ --prompt-type ì˜µì…˜ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def import_data(input_file: str, output_file: Optional[str] = None, 
               validate: bool = False, batch_size: int = 50):
    """Excel/CSV íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    try:
        # Import ì–´ëŒ‘í„° ìƒì„±
        importer = ImporterFactory.create_importer(input_file)
        
        # íŒŒì¼ í˜•ì‹ ê²€ì¦
        if not importer.validate_format(input_file):
            print(f"âŒ íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}")
            print(f"ì§€ì›ë˜ëŠ” í˜•ì‹: {ImporterFactory.get_supported_formats()}")
            return False
        
        print(f"ğŸ“‚ íŒŒì¼ ë³€í™˜ ì‹œì‘: {input_file}")
        
        # ë°ì´í„° Import
        evaluation_data_list = importer.import_data(input_file)
        
        if not evaluation_data_list:
            print("âŒ ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… {len(evaluation_data_list)}ê°œ í•­ëª© ë³€í™˜ ì™„ë£Œ")
        
        # ê²€ì¦ ìˆ˜í–‰ (ì˜µì…˜)
        if validate:
            validator = ImportDataValidator()
            validation_result = validator.validate_data_list(evaluation_data_list)
            
            print("\n" + validator.get_validation_summary(validation_result))
            
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´ ì¶œë ¥
            if not validation_result.is_valid:
                print("\nğŸ“‹ ì˜¤ë¥˜ ìƒì„¸:")
                for error in validation_result.errors[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    print(f"   {error}")
                
                if len(validation_result.errors) > 10:
                    print(f"   ... ì™¸ {len(validation_result.errors) - 10}ê°œ ì˜¤ë¥˜")
                
                if validation_result.warnings:
                    print("\nâš ï¸ ê²½ê³  ìƒì„¸:")
                    for warning in validation_result.warnings[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        print(f"   {warning}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì •
        if not output_file:
            input_path = Path(input_file)
            output_file = str(input_path.with_suffix('.json'))
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        from dataclasses import asdict
        output_data = [asdict(data) for data in evaluation_data_list]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë³€í™˜ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ ì¶œë ¥
        if len(evaluation_data_list) > batch_size:
            estimated_batches = (len(evaluation_data_list) + batch_size - 1) // batch_size
            print(f"\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´:")
            print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
            print(f"   ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {estimated_batches}")
            print(f"   ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return False


def evaluate_dataset(dataset_name: str, llm: str, embedding: Optional[str] = None, 
                    prompt_type: Optional[str] = None, output_file: Optional[str] = None, 
                    verbose: bool = False):
    """ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰"""
    
    # CSV/Excel íŒŒì¼ì¸ ê²½ìš° ìë™ ë³€í™˜
    if dataset_name.endswith(('.csv', '.xlsx', '.xls')):
        print(f"\nğŸ“‚ CSV/Excel íŒŒì¼ ê°ì§€ - JSONìœ¼ë¡œ ìë™ ë³€í™˜ ì¤‘...")
        
        # ë°ì´í„° ê²½ë¡œ í™•ì¸ ë° ì²˜ë¦¬
        from src.utils.paths import DATA_DIR
        dataset_path = Path(dataset_name)
        if not dataset_path.is_absolute():
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° data/ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            dataset_path = DATA_DIR / dataset_name
            if not dataset_path.exists():
                # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œë„ ì°¾ê¸°
                dataset_path = Path(dataset_name)
        
        if not dataset_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
            return False
        
        # ë³€í™˜ëœ íŒŒì¼ëª… ìƒì„±
        base_name = dataset_path.stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        converted_filename = f"{base_name}_converted_{timestamp}.json"
        converted_path = DATA_DIR / converted_filename
        
        # import_data í•¨ìˆ˜ í˜¸ì¶œ
        import_success = import_data(
            input_file=str(dataset_path),
            output_file=str(converted_path),
            validate=True,
            batch_size=50
        )
        
        if not import_success:
            print("âŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨")
            return False
        
        dataset_name = str(converted_path)
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {converted_path}")
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ (ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©)
    embedding_choice = embedding or settings.DEFAULT_EMBEDDING
    
    # LLM ë° ì„ë² ë”© ì–´ëŒ‘í„° ì„ íƒ
    try:
        # íŒ©í† ë¦¬ë¥¼ í†µí•´ UseCase ìƒì„±
        from src.container.factories.evaluation_use_case_factory import EvaluationRequest
        
        request = EvaluationRequest(
            llm_type=llm,
            embedding_type=embedding_choice,
            prompt_type=PromptType(prompt_type) if prompt_type else settings.get_prompt_type()
        )
        
        evaluation_use_case, _, _ = container.create_evaluation_use_case(request)
        
        if llm in ["hcx"] and not settings.CLOVA_STUDIO_API_KEY:
            print(f"âŒ '{llm}' LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False
        if embedding_choice in ["hcx"] and not settings.CLOVA_STUDIO_API_KEY:
            print(f"âŒ '{embedding_choice}' ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False

    except Exception as e:
        print(f"âŒ '{llm}' LLM ë˜ëŠ” '{embedding_choice}' ì„ë² ë”© ì–´ëŒ‘í„° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    print(f"ğŸ¤– LLM: {llm}")
    print(f"ğŸŒ Embedding: {embedding_choice}")

    # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ì •
    prompt_type_enum = PromptType(prompt_type) if prompt_type else settings.get_prompt_type()
    print(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type_enum.value.upper()}")

    try:
        data_path = get_evaluation_data_path(dataset_name)
        if not data_path:
            print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            available_datasets = get_available_datasets()
            if available_datasets:
                print("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
                for ds in available_datasets:
                    print(f"  - {ds}")
            return False
            
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")

        result = evaluation_use_case.execute(
            dataset_name=dataset_name,
        )
        
        if not result:
            print("âŒ í‰ê°€ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        print(f"ragas_score  : {result.ragas_score:.4f}")
        print(f"answer_relevancy: {result.answer_relevancy:.4f}")
        print(f"faithfulness   : {result.faithfulness:.4f}")
        print(f"context_recall : {result.context_recall:.4f}")
        print(f"context_precision: {result.context_precision:.4f}")
        
        # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶œë ¥
        if hasattr(result, 'answer_correctness') and result.answer_correctness is not None:
            print(f"answer_correctness: {result.answer_correctness:.4f}")
        
        print("="*50)

        # ìë™ ë³´ê³ ì„œ ìƒì„±
        from src.application.services.result_exporter import ResultExporter
        from dataclasses import asdict
        
        result_dict = asdict(result)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        import uuid
        evaluation_id = str(uuid.uuid4())[:8]
        result_dict["metadata"] = {
            "evaluation_id": evaluation_id,
            "timestamp": result_dict.get("timestamp", ""),
            "model": f"{llm.upper()}",
            "embedding_model": f"{embedding_choice.upper()}",
            "dataset": dataset_name,
            "prompt_type": prompt_type_enum.value,
        }
        
        print("\nğŸ“¤ ìë™ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path("exports")
        output_dir.mkdir(exist_ok=True)
        
        exporter = ResultExporter(output_dir=str(output_dir))
        
        try:
            # ì „ì²´ íŒ¨í‚¤ì§€ ìƒì„± (CSV + ìš”ì•½ + ë³´ê³ ì„œ)
            files = exporter.export_full_package(result_dict, f"eval_{evaluation_id}")
            
            print(f"âœ… ìë™ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            print(f"ğŸ“„ ìƒì„±ëœ íŒŒì¼:")
            for file_type, file_path in files.items():
                file_name = Path(file_path).name
                print(f"  - {file_name}")
                
        except Exception as e:
            print(f"âš ï¸ ìë™ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            print("í‰ê°€ëŠ” ì„±ê³µí–ˆì§€ë§Œ ë³´ê³ ì„œ ìƒì„±ì— ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.")

        if output_file:
            # ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì§€ì • íŒŒì¼ì—ë„ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ê²°ê³¼ê°€ {output_file} íŒŒì¼ì—ë„ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("âœ… í‰ê°€ ì™„ë£Œ.")
        
        return True

    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        if verbose:
            import traceback
            traceback.print_exc()
        return False


def export_results(args):
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    from src.application.services.result_exporter import ResultExporter
    
    print(f"ğŸ“¤ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹œì‘: {args.result_file}")
    
    result_path = Path(args.result_file)
    if not result_path.exists():
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.result_file}")
        return False
    
    try:
        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        with open(result_path, 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        print(f"âœ… ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        
        # ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        exporter = ResultExporter(output_dir=args.output_dir)
        
        files_created = []
        
        if args.format in ["csv", "all"]:
            csv_file = exporter.export_to_csv(result)
            files_created.append(("ìƒì„¸ CSV", csv_file))
            print(f"ğŸ“Š ìƒì„¸ CSV ìƒì„±: {csv_file}")
        
        if args.format in ["summary", "all"]:
            summary_file = exporter.export_summary_csv(result)
            files_created.append(("ìš”ì•½ CSV", summary_file))
            print(f"ğŸ“ˆ ìš”ì•½ CSV ìƒì„±: {summary_file}")
        
        if args.format in ["report", "all"]:
            report_file = exporter.generate_analysis_report(result)
            files_created.append(("ë¶„ì„ ë³´ê³ ì„œ", report_file))
            print(f"ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±: {report_file}")
        
        if args.format == "all":
            print(f"\nâœ… ì „ì²´ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ!")
            print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
            print("ğŸ“„ ìƒì„±ëœ íŒŒì¼:")
            for file_type, file_path in files_created:
                print(f"   - {file_type}: {Path(file_path).name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False


def list_checkpoints():
    """ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¶œë ¥"""
    from src.application.services.evaluation_checkpoint import EvaluationCheckpoint
    
    checkpoint_manager = EvaluationCheckpoint()
    sessions = checkpoint_manager.list_sessions()
    
    if not sessions:
        print("ğŸ“ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ’¾ ì €ì¥ëœ í‰ê°€ ì„¸ì…˜:")
    print("-" * 100)
    print(f"{'ì„¸ì…˜ ID':<25} {'ë°ì´í„°ì…‹':<20} {'ìƒíƒœ':<10} {'ì§„í–‰ë¥ ':<8} {'ì‹œì‘ ì‹œê°„':<20}")
    print("-" * 100)
    
    for session in sessions:
        session_id = session['session_id'][:24] + "..." if len(session['session_id']) > 24 else session['session_id']
        dataset_name = session['dataset_name'][:19] + "..." if len(session['dataset_name']) > 19 else session['dataset_name']
        status = session['status']
        progress = f"{session['progress']:.1f}%"
        start_time = session['start_time'][:19] if session['start_time'] else 'N/A'
        
        print(f"{session_id:<25} {dataset_name:<20} {status:<10} {progress:<8} {start_time:<20}")
    
    print(f"\nì´ {len(sessions)}ê°œì˜ ì„¸ì…˜ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")


def resume_evaluation(args):
    """ì¤‘ë‹¨ëœ í‰ê°€ ì¬ê°œ"""
    from src.application.services.evaluation_checkpoint import EvaluationCheckpoint, BatchEvaluationManager
    from src.container.factories.evaluation_use_case_factory import EvaluationRequest
    from src.utils.paths import get_evaluation_data_path
    from datasets import Dataset
    
    print(f"ğŸ”„ í‰ê°€ ì¬ê°œ ì‹œë„: {args.session_id}")
    
    checkpoint_manager = EvaluationCheckpoint()
    checkpoint = checkpoint_manager.resume_session(args.session_id)
    
    if not checkpoint:
        return False
    
    if checkpoint.get('status') == 'completed':
        print("âœ… ì´ë¯¸ ì™„ë£Œëœ í‰ê°€ì…ë‹ˆë‹¤.")
        
        # ì™„ë£Œëœ ê²°ê³¼ íŒŒì¼ í™•ì¸
        result_file = checkpoint_manager.checkpoint_dir / f"{args.session_id}.result.json"
        if result_file.exists():
            print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {result_file}")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                print("\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
                print(f"- RAGAS Score: {result.get('ragas_score', 0):.3f}")
                print(f"- ë°ì´í„°ì…‹ í¬ê¸°: {result.get('metadata', {}).get('dataset_size', 0)}ê°œ")
                print(f"- ì„±ê³µë¥ : {result.get('metadata', {}).get('success_rate', 0):.1f}%")
                
            except Exception as e:
                print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        return True
    
    if checkpoint.get('status') == 'failed':
        print("âŒ ì‹¤íŒ¨í•œ í‰ê°€ ì„¸ì…˜ì…ë‹ˆë‹¤.")
        partial_result = checkpoint.get('partial_result')
        if partial_result:
            print("ğŸ“Š ë¶€ë¶„ ê²°ê³¼ ì¶œë ¥:")
            print(f"- ë¶€ë¶„ RAGAS Score: {partial_result.get('ragas_score', 0):.3f}")
            print(f"- ì™„ë£Œëœ í•­ëª©: {len(partial_result.get('individual_scores', []))}ê°œ")
        return False
    
    # ì‹¤ì œ ì¬ê°œ êµ¬í˜„
    try:
        print("ğŸ”„ ì¤‘ë‹¨ëœ í‰ê°€ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤...")
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„¤ì • ë³µì›
        config = checkpoint.get('config', {})
        dataset_name = checkpoint.get('dataset_name')
        completed_items = checkpoint.get('completed_items', 0)
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")
        print(f"âœ… ì´ë¯¸ ì™„ë£Œ: {completed_items}ê°œ")
        print(f"ğŸ”„ ì¬ê°œ ìœ„ì¹˜: {completed_items + 1}ë²ˆì§¸ í•­ëª©ë¶€í„°")
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
        data_path = get_evaluation_data_path(dataset_name)
        if not data_path:
            print(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
            return False
        
        # ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¯¸ì™„ë£Œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        remaining_data = data[completed_items:]  # ë¯¸ì™„ë£Œ ë¶€ë¶„ë§Œ
        
        if not remaining_data:
            print("âœ… ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        
        print(f"ğŸ“‹ ë‚¨ì€ í•­ëª©: {len(remaining_data)}ê°œ")
        
        # í‰ê°€ ì„¤ì • ë³µì›
        llm_type = config.get('llm_type', 'gemini')
        embedding_type = config.get('embedding_type', 'gemini')
        prompt_type_str = config.get('prompt_type', 'default')
        
        # PromptType enumìœ¼ë¡œ ë³€í™˜
        from src.domain.prompts import PromptType
        if isinstance(prompt_type_str, str):
            try:
                prompt_type = PromptType(prompt_type_str)
            except ValueError:
                prompt_type = PromptType.DEFAULT
        else:
            prompt_type = prompt_type_str
        
        print(f"ğŸ¤– LLM: {llm_type}")
        print(f"ğŸŒ Embedding: {embedding_type}")
        print(f"ğŸ¯ Prompt Type: {prompt_type.value}")
        
        # í‰ê°€ ì–´ëŒ‘í„° ìƒì„±
        request = EvaluationRequest(
            llm_type=llm_type,
            embedding_type=embedding_type,
            prompt_type=prompt_type
        )
        
        evaluation_use_case, llm_adapter, embedding_adapter = container.create_evaluation_use_case(request)
        
        # ë‚¨ì€ ë°ì´í„°ë¡œ Dataset ìƒì„±
        remaining_dataset = Dataset.from_list(remaining_data)
        
        # RAGAS ì–´ëŒ‘í„° ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì¬ê°œ
        from src.infrastructure.evaluation.ragas_adapter_legacy import RagasEvalAdapter
        
        ragas_adapter = RagasEvalAdapter(
            llm=llm_adapter,
            embeddings=embedding_adapter,
            prompt_type=prompt_type
        )
        
        # ë°°ì¹˜ í‰ê°€ ê´€ë¦¬ì ìƒì„±
        batch_size = config.get('batch_size', 10)
        batch_manager = BatchEvaluationManager(checkpoint_manager, batch_size)
        
        # ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ë‚˜ë¨¸ì§€ í‰ê°€ ì‹¤í–‰
        def batch_eval_func(batch_dataset):
            return ragas_adapter._run_evaluation_with_timeout(batch_dataset)
        
        # ê¸°ì¡´ ê²°ê³¼ì™€ ìƒˆ ê²°ê³¼ í•©ì¹˜ê¸°
        print("ğŸ”„ ë‚˜ë¨¸ì§€ í‰ê°€ ì‹¤í–‰ ì¤‘...")
        remaining_result = batch_manager.evaluate_with_checkpoints(
            remaining_dataset, 
            batch_eval_func, 
            config
        )
        
        # ê¸°ì¡´ ê°œë³„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        existing_results = checkpoint.get('individual_results', [])
        new_results = remaining_result.get('individual_scores', [])
        
        # ì „ì²´ ê²°ê³¼ í•©ì¹˜ê¸°
        all_individual_results = existing_results + new_results
        
        # ìµœì¢… ê²°ê³¼ ì¬ê³„ì‚°
        final_result = batch_manager._compile_final_result(
            all_individual_results, 
            config, 
            remaining_result.get('metadata', {}).get('error_count', 0)
        )
        
        # ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
        checkpoint_manager.complete_session(final_result)
        
        print("\nâœ… í‰ê°€ ì¬ê°œ ì™„ë£Œ!")
        print("ğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"- RAGAS Score: {final_result.get('ragas_score', 0):.3f}")
        print(f"- ì „ì²´ í•­ëª©: {len(all_individual_results)}ê°œ")
        print(f"- ì„±ê³µë¥ : {final_result.get('metadata', {}).get('success_rate', 0):.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¬ê°œ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False


def cleanup_checkpoints(args):
    """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
    from src.application.services.evaluation_checkpoint import EvaluationCheckpoint
    
    print(f"ğŸ§¹ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì‹œì‘ ({args.days}ì¼ ì´ìƒëœ íŒŒì¼)")
    
    checkpoint_manager = EvaluationCheckpoint()
    checkpoint_manager.cleanup_old_sessions(args.days)
    
    print("âœ… ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")
    return True


def quick_eval(args):
    """ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (HCX-005 + BGE-M3 + ìë™ ê²°ê³¼ ì €ì¥)"""
    import os
    from datetime import datetime
    from pathlib import Path
    
    print("ğŸš€ RAGTrace ê°„ë‹¨ í‰ê°€ ì‹œì‘")
    print("=" * 50)
    print("ğŸ¤– LLM: HCX-005 (Naver)")
    print("ğŸŒ Embedding: BGE-M3 (Local)")
    print("ğŸ“ ë°ì´í„°ì…‹:", args.dataset)
    print("=" * 50)
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # CSV/Excel íŒŒì¼ì¸ ê²½ìš° ìë™ ë³€í™˜
    dataset_to_use = args.dataset
    if args.dataset.endswith(('.csv', '.xlsx', '.xls')):
        print(f"\nğŸ“‚ CSV/Excel íŒŒì¼ ê°ì§€ - JSONìœ¼ë¡œ ìë™ ë³€í™˜ ì¤‘...")
        
        # ë°ì´í„° ê²½ë¡œ í™•ì¸ ë° ì²˜ë¦¬
        from src.utils.paths import DATA_DIR
        dataset_path = Path(args.dataset)
        if not dataset_path.is_absolute():
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° data/ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            dataset_path = DATA_DIR / args.dataset
            if not dataset_path.exists():
                # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œë„ ì°¾ê¸°
                dataset_path = Path(args.dataset)
        
        if not dataset_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dataset}")
            return False
        
        # ë³€í™˜ëœ íŒŒì¼ëª… ìƒì„±
        base_name = dataset_path.stem
        converted_filename = f"{base_name}_converted.json"
        converted_path = output_dir / converted_filename
        
        # import_data í•¨ìˆ˜ í˜¸ì¶œ
        import_success = import_data(
            input_file=str(dataset_path),
            output_file=str(converted_path),
            validate=True,
            batch_size=50
        )
        
        if not import_success:
            print("âŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨")
            return False
        
        dataset_to_use = str(converted_path)
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {converted_path}")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_dataset_name = Path(dataset_to_use).stem
    result_filename = f"{base_dataset_name}_{timestamp}.json"
    result_path = output_dir / result_filename
    
    try:
        # 1. í‰ê°€ ì‹¤í–‰
        print("\nğŸ“Š 1ë‹¨ê³„: í‰ê°€ ì‹¤í–‰ ì¤‘...")
        success = evaluate_dataset(
            dataset_name=dataset_to_use,
            llm="hcx",
            embedding="bge_m3",
            prompt_type=None,
            output_file=str(result_path),
            verbose=args.verbose
        )
        
        if not success:
            print("âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨")
            return False
        
        # 2. ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        print(f"\nğŸ“¤ 2ë‹¨ê³„: ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì¤‘...")
        
        # export_results í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ê¸° ìœ„í•œ args ê°ì²´ ìƒì„±
        class ExportArgs:
            def __init__(self, result_file, output_dir):
                self.result_file = result_file
                self.format = "all"
                self.output_dir = output_dir
        
        export_args = ExportArgs(str(result_path), str(output_dir))
        export_success = export_results(export_args)
        
        if not export_success:
            print("âš ï¸ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨ (í‰ê°€ëŠ” ì™„ë£Œ)")
        
        # 3. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 50)
        print("âœ… ê°„ë‹¨ í‰ê°€ ì™„ë£Œ!")
        print("=" * 50)
        print(f"ğŸ“„ í‰ê°€ ê²°ê³¼ íŒŒì¼: {result_path}")
        
        if export_success:
            print(f"ğŸ“ ë¶„ì„ ë³´ê³ ì„œ ë””ë ‰í† ë¦¬: {output_dir}")
            print("ğŸ“‹ ìƒì„±ëœ íŒŒì¼:")
            print("   - ìƒì„¸ CSV: ê°œë³„ í•­ëª©ë³„ ì ìˆ˜")
            print("   - ìš”ì•½ CSV: ë©”íŠ¸ë¦­ë³„ ê¸°ì´ˆ í†µê³„")
            print("   - ë¶„ì„ ë³´ê³ ì„œ: ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ (Markdown)")
        
        print(f"\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print(f"   uv run python cli.py export-results {result_path} --format all --output-dir {output_dir}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ê°„ë‹¨ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
        from src.config import validate_settings
        validate_settings()
        
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "list-datasets":
        list_datasets()
        
    elif args.command == "list-prompts":
        list_prompts()
        
    elif args.command == "evaluate":
        success = evaluate_dataset(
            dataset_name=args.dataset,
            llm=args.llm,
            embedding=args.embedding,
            prompt_type=args.prompt_type,
            output_file=args.output,
            verbose=args.verbose
        )
        if not success:
            sys.exit(1)
            
    elif args.command == "export-results":
        success = export_results(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "import-data":
        success = import_data(
            input_file=args.input_file,
            output_file=args.output,
            validate=args.validate,
            batch_size=args.batch_size
        )
        if not success:
            sys.exit(1)
    
    elif args.command == "list-checkpoints":
        list_checkpoints()
    
    elif args.command == "resume-evaluation":
        success = resume_evaluation(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "cleanup-checkpoints":
        success = cleanup_checkpoints(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "quick-eval":
        success = quick_eval(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "analyze-results":
        success = analyze_results(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "compare-results":
        success = compare_results(args)
        if not success:
            sys.exit(1)
    
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")
        parser.print_help()
        sys.exit(1)


def analyze_results(args) -> bool:
    """í‰ê°€ ê²°ê³¼ ê³ ê¸‰ í†µê³„ ë¶„ì„"""
    try:
        result_file = Path(args.result_file)
        if not result_file.exists():
            print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_file}")
            return False
        
        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        with open(result_file, 'r', encoding='utf-8') as f:
            result_data = json.load(f)
        
        print(f"ğŸ“Š ê²°ê³¼ ë¶„ì„ ì‹œì‘: {result_file.name}")
        print(f"ğŸ¯ ë¶„ì„ ìœ í˜•: {args.analysis_type}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(args.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ê°œë³„ ì ìˆ˜ ì¶”ì¶œ
        individual_scores = result_data.get("individual_scores", [])
        if not individual_scores:
            print("âš ï¸ ê°œë³„ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í†µê³„ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.")
            
        # ë¶„ì„ ìœ í˜•ì— ë”°ë¥¸ ì²˜ë¦¬
        if args.analysis_type in ["basic", "all"]:
            print("\nğŸ“ˆ ê¸°ì´ˆ í†µê³„ ë¶„ì„")
            basic_stats = perform_basic_analysis(result_data, individual_scores)
            save_basic_analysis(basic_stats, output_dir)
            
        if args.analysis_type in ["eda", "all"]:
            print("\nğŸ” íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)")
            eda_results = perform_eda_analysis(individual_scores)
            save_eda_analysis(eda_results, output_dir)
            
        if args.analysis_type in ["advanced", "all"]:
            print("\nğŸ§® ê³ ê¸‰ í†µê³„ ë¶„ì„")
            advanced_stats = perform_advanced_analysis(individual_scores)
            save_advanced_analysis(advanced_stats, output_dir)
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def compare_results(args) -> bool:
    """ì—¬ëŸ¬ í‰ê°€ ê²°ê³¼ ë¹„êµ ë¶„ì„"""
    try:
        result_files = [Path(f) for f in args.result_files]
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        for result_file in result_files:
            if not result_file.exists():
                print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_file}")
                return False
        
        print(f"ğŸ“Š {len(result_files)}ê°œ ê²°ê³¼ íŒŒì¼ ë¹„êµ ë¶„ì„")
        
        # ë¼ë²¨ ì„¤ì •
        if args.labels and len(args.labels) == len(result_files):
            labels = args.labels
        else:
            labels = [f.stem for f in result_files]
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(args.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ë°ì´í„° ë¡œë“œ
        results_data = []
        for i, result_file in enumerate(result_files):
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                data['label'] = labels[i]
                results_data.append(data)
        
        # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
        comparison_stats = perform_comparison_analysis(results_data)
        save_comparison_analysis(comparison_stats, output_dir, labels)
        
        print(f"\nâœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        print(f"âŒ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def perform_basic_analysis(result_data: dict, individual_scores: list) -> dict:
    """ê¸°ì´ˆ í†µê³„ ë¶„ì„ ìˆ˜í–‰"""
    print("  ğŸ“‹ ë©”íŠ¸ë¦­ë³„ ê¸°ì´ˆ í†µê³„ ê³„ì‚° ì¤‘...")
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness']
    basic_stats = {}
    
    for metric in metrics:
        if metric in result_data:
            # ì „ì²´ í‰ê· ê°’
            overall_mean = result_data[metric]
            
            # ê°œë³„ ì ìˆ˜ì—ì„œ í†µê³„ ê³„ì‚° (None ê°’ ì œì™¸)
            if individual_scores:
                scores = [item.get(metric) for item in individual_scores if item.get(metric) is not None]
                if scores:
                    basic_stats[metric] = {
                        'mean': statistics.mean(scores),
                        'median': statistics.median(scores),
                        'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0.0,
                        'min': min(scores),
                        'max': max(scores),
                        'count': len(scores),
                        'success_rate': len(scores) / len(individual_scores) * 100
                    }
                else:
                    basic_stats[metric] = {'error': 'No valid scores'}
            else:
                basic_stats[metric] = {'mean': overall_mean, 'count': 1}
    
    # ì „ì²´ ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
    if 'ragas_score' in result_data:
        ragas_score = result_data['ragas_score']
        if ragas_score >= 0.8:
            grade = "A (ìš°ìˆ˜)"
        elif ragas_score >= 0.6:
            grade = "B (ë³´í†µ)"
        elif ragas_score >= 0.4:
            grade = "C (ê°œì„  í•„ìš”)"
        else:
            grade = "D (í¬ê²Œ ê°œì„  í•„ìš”)"
        
        basic_stats['overall'] = {
            'ragas_score': ragas_score,
            'grade': grade
        }
    
    return basic_stats


def perform_eda_analysis(individual_scores: list) -> dict:
    """íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"""
    if not individual_scores:
        return {'error': 'No individual scores available'}
    
    print("  ğŸ” ìƒê´€ê´€ê³„ ë° ë¶„í¬ ë¶„ì„ ì¤‘...")
    
    import pandas as pd
    import numpy as np
    
    # DataFrame ìƒì„± (None ê°’ ì œì™¸)
    df = pd.DataFrame(individual_scores)
    
    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
    numeric_df = df.select_dtypes(include=[np.number])
    correlation_matrix = numeric_df.corr()
    
    # ë¶„í¬ íŠ¹ì„± ë¶„ì„
    distribution_stats = {}
    for column in numeric_df.columns:
        data = numeric_df[column].dropna()
        if len(data) > 0:
            distribution_stats[column] = {
                'skewness': float(data.skew()),
                'kurtosis': float(data.kurtosis()),
                'quartiles': {
                    'q1': float(data.quantile(0.25)),
                    'q2': float(data.quantile(0.5)),
                    'q3': float(data.quantile(0.75)),
                    'iqr': float(data.quantile(0.75) - data.quantile(0.25))
                }
            }
    
    return {
        'correlation_matrix': correlation_matrix.to_dict(),
        'distribution_stats': distribution_stats,
        'data_completeness': {
            'total_samples': len(individual_scores),
            'missing_data': df.isnull().sum().to_dict()
        }
    }


def perform_advanced_analysis(individual_scores: list) -> dict:
    """ê³ ê¸‰ í†µê³„ ë¶„ì„ ìˆ˜í–‰"""
    if not individual_scores or len(individual_scores) < 3:
        return {'error': 'Insufficient data for advanced analysis (need at least 3 samples)'}
    
    print("  ğŸ§® ì •ê·œì„± ê²€ì • ë° ì‹ ë¢°êµ¬ê°„ ê³„ì‚° ì¤‘...")
    
    import pandas as pd
    import numpy as np
    from scipy import stats
    
    df = pd.DataFrame(individual_scores)
    advanced_stats = {}
    
    for column in df.select_dtypes(include=[np.number]).columns:
        data = df[column].dropna()
        if len(data) >= 3:
            # ì •ê·œì„± ê²€ì •
            shapiro_stat, shapiro_p = stats.shapiro(data)
            
            # ì‹ ë¢°êµ¬ê°„ (95%)
            mean_val = np.mean(data)
            sem = stats.sem(data)
            ci_95 = stats.t.interval(0.95, df=len(data)-1, loc=mean_val, scale=sem)
            
            advanced_stats[column] = {
                'normality_test': {
                    'shapiro_wilk_statistic': float(shapiro_stat),
                    'shapiro_wilk_p_value': float(shapiro_p),
                    'is_normal': shapiro_p > 0.05
                },
                'confidence_intervals': {
                    'mean': float(mean_val),
                    'ci_95_lower': float(ci_95[0]),
                    'ci_95_upper': float(ci_95[1]),
                    'margin_of_error': float(ci_95[1] - mean_val)
                }
            }
    
    return advanced_stats


def perform_comparison_analysis(results_data: list) -> dict:
    """ì—¬ëŸ¬ ê²°ê³¼ ë¹„êµ ë¶„ì„"""
    print("  ğŸ“Š ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµ ì¤‘...")
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness', 'ragas_score']
    comparison_stats = {}
    
    for metric in metrics:
        metric_values = []
        labels = []
        
        for result in results_data:
            if metric in result:
                metric_values.append(result[metric])
                labels.append(result['label'])
        
        if metric_values:
            comparison_stats[metric] = {
                'values': dict(zip(labels, metric_values)),
                'best_model': labels[metric_values.index(max(metric_values))],
                'worst_model': labels[metric_values.index(min(metric_values))],
                'range': max(metric_values) - min(metric_values),
                'mean': statistics.mean(metric_values),
                'std_dev': statistics.stdev(metric_values) if len(metric_values) > 1 else 0.0
            }
    
    return comparison_stats


def save_basic_analysis(stats: dict, output_dir: Path):
    """ê¸°ì´ˆ í†µê³„ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    with open(output_dir / 'basic_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # í…ìŠ¤íŠ¸ ìš”ì•½ ì €ì¥
    with open(output_dir / 'basic_analysis_summary.txt', 'w', encoding='utf-8') as f:
        f.write("ğŸ“ˆ ê¸°ì´ˆ í†µê³„ ë¶„ì„ ìš”ì•½\n")
        f.write("=" * 50 + "\n\n")
        
        for metric, data in stats.items():
            if metric == 'overall':
                f.write(f"ğŸ¯ ì „ì²´ ì„±ëŠ¥: {data.get('grade', 'N/A')} (ì ìˆ˜: {data.get('ragas_score', 0):.3f})\n\n")
            elif 'error' not in data:
                f.write(f"{metric}:\n")
                f.write(f"  í‰ê· : {data.get('mean', 0):.3f}\n")
                f.write(f"  ì¤‘ì•™ê°’: {data.get('median', 0):.3f}\n")
                f.write(f"  í‘œì¤€í¸ì°¨: {data.get('std_dev', 0):.3f}\n")
                f.write(f"  ë²”ìœ„: {data.get('min', 0):.3f} ~ {data.get('max', 0):.3f}\n")
                if 'success_rate' in data:
                    f.write(f"  ì„±ê³µë¥ : {data['success_rate']:.1f}%\n")
                f.write("\n")
    
    print(f"  âœ… ê¸°ì´ˆ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_dir}/basic_analysis.json")


def save_eda_analysis(results: dict, output_dir: Path):
    """EDA ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    with open(output_dir / 'eda_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ… EDA ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_dir}/eda_analysis.json")


def save_advanced_analysis(stats: dict, output_dir: Path):
    """ê³ ê¸‰ í†µê³„ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    with open(output_dir / 'advanced_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ… ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_dir}/advanced_analysis.json")


def save_comparison_analysis(stats: dict, output_dir: Path, labels: list):
    """ë¹„êµ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    with open(output_dir / 'comparison_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # ë¹„êµ ìš”ì•½ ì €ì¥
    with open(output_dir / 'comparison_summary.txt', 'w', encoding='utf-8') as f:
        f.write("ğŸ“Š ëª¨ë¸ ë¹„êµ ë¶„ì„ ìš”ì•½\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"ë¹„êµ ëŒ€ìƒ: {', '.join(labels)}\n\n")
        
        for metric, data in stats.items():
            if 'values' in data:
                f.write(f"{metric}:\n")
                f.write(f"  ìµœê³  ì„±ëŠ¥: {data['best_model']} ({data['values'][data['best_model']]:.3f})\n")
                f.write(f"  ìµœì € ì„±ëŠ¥: {data['worst_model']} ({data['values'][data['worst_model']]:.3f})\n")
                f.write(f"  ì„±ëŠ¥ ì°¨ì´: {data['range']:.3f}\n")
                f.write(f"  í‰ê· : {data['mean']:.3f}\n\n")
    
    print(f"  âœ… ë¹„êµ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_dir}/comparison_analysis.json")


if __name__ == "__main__":
    main()