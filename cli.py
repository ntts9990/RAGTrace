#!/usr/bin/env python3
"""
RAGTrace CLI ì¸í„°í˜ì´ìŠ¤

RAGAS í‰ê°€ë¥¼ ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” CLI ë„êµ¬
í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ ê¸°ëŠ¥ í¬í•¨
"""

import argparse
import sys
import json
from typing import Optional
from pathlib import Path

from src.config import (
    settings, 
    PROMPT_TYPE_HELP, 
    SUPPORTED_LLM_TYPES, 
    SUPPORTED_EMBEDDING_TYPES
)
from src.container import container
from src.domain.prompts import PromptType
from src.utils.paths import get_available_datasets, get_evaluation_data_path
from src.infrastructure.data_import.importers import ImporterFactory
from src.infrastructure.data_import.validators import ImportDataValidator


def create_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="RAGTrace - RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (HCX-005 + BGE-M3, ìë™ ê²°ê³¼ ì €ì¥)
  python cli.py quick-eval evaluation_data
  
  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€ ì‹¤í–‰
  python cli.py evaluate evaluation_data.json

  # LLMê³¼ ì„ë² ë”© ëª¨ë¸ ì„ íƒ
  python cli.py evaluate evaluation_data.json --llm gemini --embedding hcx
  
  # í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
  python cli.py evaluate evaluation_data.json --prompt-type korean_tech
  
  # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°
  python cli.py list-datasets
  
  # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì •ë³´ ë³´ê¸°
  python cli.py list-prompts
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    
    # evaluate ì„œë¸Œì»¤ë§¨ë“œ
    eval_parser = subparsers.add_parser("evaluate", help="RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹¤í–‰")
    eval_parser.add_argument(
        "dataset",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (í™•ì¥ì ì œì™¸)"
    )
    eval_parser.add_argument(
        "--llm",
        choices=SUPPORTED_LLM_TYPES,
        default=settings.DEFAULT_LLM,
        help=f"í‰ê°€ì— ì‚¬ìš©í•  LLM (ê¸°ë³¸ê°’: {settings.DEFAULT_LLM})"
    )
    eval_parser.add_argument(
        "--embedding",
        choices=SUPPORTED_EMBEDDING_TYPES,
        default=None,
        help=f"í‰ê°€ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸ê°’: {settings.DEFAULT_EMBEDDING}). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )
    eval_parser.add_argument(
        "--prompt-type", 
        choices=[pt.value for pt in PromptType],
        default=None,
        help="ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì…"
    )
    eval_parser.add_argument(
        "--output",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    eval_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    # list-datasets ì„œë¸Œì»¤ë§¨ë“œ
    list_parser = subparsers.add_parser("list-datasets", help="ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë³´ê¸°")
    
    # list-prompts ì„œë¸Œì»¤ë§¨ë“œ
    prompts_parser = subparsers.add_parser("list-prompts", help="ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ë³´ê¸°")
    
    # import-data ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    import_parser = subparsers.add_parser("import-data", help="Excel/CSV íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
    import_parser.add_argument(
        "input_file",
        help="ë³€í™˜í•  Excel(.xlsx, .xls) ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ"
    )
    import_parser.add_argument(
        "--output", "-o",
        help="ë³€í™˜ëœ JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥íŒŒì¼ëª….json)"
    )
    import_parser.add_argument(
        "--validate", "-v",
        action="store_true",
        help="ë³€í™˜ëœ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰"
    )
    import_parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 50)"
    )
    
    # export-results ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    export_parser = subparsers.add_parser("export-results", help="í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°")
    export_parser.add_argument(
        "result_file",
        help="ë‚´ë³´ë‚¼ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ"
    )
    export_parser.add_argument(
        "--format",
        choices=["csv", "summary", "report", "all"],
        default="all",
        help="ë‚´ë³´ë‚¼ í˜•ì‹ (ê¸°ë³¸ê°’: all)"
    )
    export_parser.add_argument(
        "--output-dir", "-o",
        default="exports",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: exports)"
    )
    
    # list-checkpoints ì„œë¸Œì»¤ë§¨ë“œ
    checkpoint_list_parser = subparsers.add_parser("list-checkpoints", help="ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ë³´ê¸°")
    
    # resume-evaluation ì„œë¸Œì»¤ë§¨ë“œ
    resume_parser = subparsers.add_parser("resume-evaluation", help="ì¤‘ë‹¨ëœ í‰ê°€ ì¬ê°œ")
    resume_parser.add_argument(
        "session_id",
        help="ì¬ê°œí•  í‰ê°€ ì„¸ì…˜ ID"
    )
    
    # cleanup-checkpoints ì„œë¸Œì»¤ë§¨ë“œ
    cleanup_parser = subparsers.add_parser("cleanup-checkpoints", help="ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬")
    cleanup_parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="ë³´ì¡´í•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)"
    )
    
    # quick-eval ì„œë¸Œì»¤ë§¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
    quick_parser = subparsers.add_parser("quick-eval", help="ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (HCX-005 + BGE-M3, ìë™ ê²°ê³¼ ì €ì¥)")
    quick_parser.add_argument(
        "dataset",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (í™•ì¥ì ì œì™¸)"
    )
    quick_parser.add_argument(
        "--output-dir",
        default="quick_results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: quick_results)"
    )
    quick_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    return parser


def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥"""
    print("ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
    print("-" * 60)
    
    datasets = get_available_datasets()
    if not datasets:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   data/ ë””ë ‰í† ë¦¬ì— í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        print("   ì§€ì› í˜•ì‹: JSON, CSV, Excel (.xlsx, .xls)")
        return
    
    # íŒŒì¼ í˜•ì‹ë³„ë¡œ ê·¸ë£¹í™”
    json_files = [d for d in datasets if d.endswith('.json')]
    csv_files = [d for d in datasets if d.endswith('.csv')]
    excel_files = [d for d in datasets if d.endswith(('.xlsx', '.xls'))]
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
    file_num = 1
    
    if json_files:
        print("\nğŸ“„ JSON íŒŒì¼:")
        for dataset in json_files:
            print(f"  {file_num}. {dataset}")
            file_num += 1
    
    if csv_files:
        print("\nğŸ“Š CSV íŒŒì¼:")
        for dataset in csv_files:
            print(f"  {file_num}. {dataset} (ë³€í™˜ í•„ìš”)")
            file_num += 1
    
    if excel_files:
        print("\nğŸ“ˆ Excel íŒŒì¼:")
        for dataset in excel_files:
            print(f"  {file_num}. {dataset} (ë³€í™˜ í•„ìš”)")
            file_num += 1
    
    print(f"\nì´ {len(datasets)}ê°œì˜ ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤.")
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("  - JSON: python cli.py evaluate <filename>")
    print("  - CSV/Excel: python cli.py import-data <filename> --output converted.json")


def list_prompts():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… ëª©ë¡ ì¶œë ¥"""
    print("ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì…:")
    print("-" * 50)
    
    for prompt_type in PromptType:
        description = PROMPT_TYPE_HELP.get(prompt_type, "ì„¤ëª… ì—†ìŒ")
        status = "âœ… ê¸°ë³¸ê°’" if prompt_type == PromptType.DEFAULT else "ğŸ”§ ì»¤ìŠ¤í…€"
        print(f"{status} {prompt_type.value:20} : {description}")
    
    print(f"\ní˜„ì¬ ê¸°ë³¸ ì„¤ì •: {settings.get_prompt_type().value}")
    
    if settings.is_custom_prompt_enabled():
        print("ğŸ’¡ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ --prompt-type ì˜µì…˜ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def import_data(input_file: str, output_file: Optional[str] = None, 
               validate: bool = False, batch_size: int = 50):
    """Excel/CSV íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    try:
        # Import ì–´ëŒ‘í„° ìƒì„±
        importer = ImporterFactory.create_importer(input_file)
        
        # íŒŒì¼ í˜•ì‹ ê²€ì¦
        if not importer.validate_format(input_file):
            print(f"âŒ íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}")
            print(f"ì§€ì›ë˜ëŠ” í˜•ì‹: {ImporterFactory.get_supported_formats()}")
            return False
        
        print(f"ğŸ“‚ íŒŒì¼ ë³€í™˜ ì‹œì‘: {input_file}")
        
        # ë°ì´í„° Import
        evaluation_data_list = importer.import_data(input_file)
        
        if not evaluation_data_list:
            print("âŒ ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… {len(evaluation_data_list)}ê°œ í•­ëª© ë³€í™˜ ì™„ë£Œ")
        
        # ê²€ì¦ ìˆ˜í–‰ (ì˜µì…˜)
        if validate:
            validator = ImportDataValidator()
            validation_result = validator.validate_data_list(evaluation_data_list)
            
            print("\n" + validator.get_validation_summary(validation_result))
            
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´ ì¶œë ¥
            if not validation_result.is_valid:
                print("\nğŸ“‹ ì˜¤ë¥˜ ìƒì„¸:")
                for error in validation_result.errors[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    print(f"   {error}")
                
                if len(validation_result.errors) > 10:
                    print(f"   ... ì™¸ {len(validation_result.errors) - 10}ê°œ ì˜¤ë¥˜")
                
                if validation_result.warnings:
                    print("\nâš ï¸ ê²½ê³  ìƒì„¸:")
                    for warning in validation_result.warnings[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        print(f"   {warning}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì •
        if not output_file:
            input_path = Path(input_file)
            output_file = str(input_path.with_suffix('.json'))
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        from dataclasses import asdict
        output_data = [asdict(data) for data in evaluation_data_list]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë³€í™˜ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ ì¶œë ¥
        if len(evaluation_data_list) > batch_size:
            estimated_batches = (len(evaluation_data_list) + batch_size - 1) // batch_size
            print(f"\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´:")
            print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
            print(f"   ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {estimated_batches}")
            print(f"   ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return False


def evaluate_dataset(dataset_name: str, llm: str, embedding: Optional[str] = None, 
                    prompt_type: Optional[str] = None, output_file: Optional[str] = None, 
                    verbose: bool = False):
    """ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰"""
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ (ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©)
    embedding_choice = embedding or settings.DEFAULT_EMBEDDING
    
    # LLM ë° ì„ë² ë”© ì–´ëŒ‘í„° ì„ íƒ
    try:
        # íŒ©í† ë¦¬ë¥¼ í†µí•´ UseCase ìƒì„±
        from src.container.factories.evaluation_use_case_factory import EvaluationRequest
        
        request = EvaluationRequest(
            llm_type=llm,
            embedding_type=embedding_choice,
            prompt_type=PromptType(prompt_type) if prompt_type else settings.get_prompt_type()
        )
        
        evaluation_use_case, _, _ = container.create_evaluation_use_case(request)
        
        if llm in ["hcx"] and not settings.CLOVA_STUDIO_API_KEY:
            print(f"âŒ '{llm}' LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False
        if embedding_choice in ["hcx"] and not settings.CLOVA_STUDIO_API_KEY:
            print(f"âŒ '{embedding_choice}' ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False

    except Exception as e:
        print(f"âŒ '{llm}' LLM ë˜ëŠ” '{embedding_choice}' ì„ë² ë”© ì–´ëŒ‘í„° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    print(f"ğŸ¤– LLM: {llm}")
    print(f"ğŸŒ Embedding: {embedding_choice}")

    # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ì •
    prompt_type_enum = PromptType(prompt_type) if prompt_type else settings.get_prompt_type()
    print(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type_enum.value.upper()}")

    try:
        data_path = get_evaluation_data_path(dataset_name)
        if not data_path:
            print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            available_datasets = get_available_datasets()
            if available_datasets:
                print("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
                for ds in available_datasets:
                    print(f"  - {ds}")
            return False
            
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")

        result = evaluation_use_case.execute(
            dataset_name=dataset_name,
        )
        
        if not result:
            print("âŒ í‰ê°€ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        print(f"ragas_score  : {result.ragas_score:.4f}")
        print(f"answer_relevancy: {result.answer_relevancy:.4f}")
        print(f"faithfulness   : {result.faithfulness:.4f}")
        print(f"context_recall : {result.context_recall:.4f}")
        print(f"context_precision: {result.context_precision:.4f}")
        
        # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶œë ¥
        if hasattr(result, 'answer_correctness') and result.answer_correctness is not None:
            print(f"answer_correctness: {result.answer_correctness:.4f}")
        
        print("="*50)

        # ìë™ ë³´ê³ ì„œ ìƒì„±
        from src.application.services.result_exporter import ResultExporter
        from dataclasses import asdict
        
        result_dict = asdict(result)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        import uuid
        evaluation_id = str(uuid.uuid4())[:8]
        result_dict["metadata"] = {
            "evaluation_id": evaluation_id,
            "timestamp": result_dict.get("timestamp", ""),
            "model": f"{llm.upper()}",
            "embedding_model": f"{embedding_choice.upper()}",
            "dataset": dataset_name,
            "prompt_type": prompt_type_enum.value,
        }
        
        print("\nğŸ“¤ ìë™ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path("exports")
        output_dir.mkdir(exist_ok=True)
        
        exporter = ResultExporter(output_dir=str(output_dir))
        
        try:
            # ì „ì²´ íŒ¨í‚¤ì§€ ìƒì„± (CSV + ìš”ì•½ + ë³´ê³ ì„œ)
            files = exporter.export_full_package(result_dict, f"eval_{evaluation_id}")
            
            print(f"âœ… ìë™ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            print(f"ğŸ“„ ìƒì„±ëœ íŒŒì¼:")
            for file_type, file_path in files.items():
                file_name = Path(file_path).name
                print(f"  - {file_name}")
                
        except Exception as e:
            print(f"âš ï¸ ìë™ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            print("í‰ê°€ëŠ” ì„±ê³µí–ˆì§€ë§Œ ë³´ê³ ì„œ ìƒì„±ì— ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.")

        if output_file:
            # ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì§€ì • íŒŒì¼ì—ë„ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ê²°ê³¼ê°€ {output_file} íŒŒì¼ì—ë„ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("âœ… í‰ê°€ ì™„ë£Œ.")
        
        return True

    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        if verbose:
            import traceback
            traceback.print_exc()
        return False


def export_results(args):
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    from src.application.services.result_exporter import ResultExporter
    
    print(f"ğŸ“¤ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹œì‘: {args.result_file}")
    
    result_path = Path(args.result_file)
    if not result_path.exists():
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.result_file}")
        return False
    
    try:
        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        with open(result_path, 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        print(f"âœ… ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        
        # ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        exporter = ResultExporter(output_dir=args.output_dir)
        
        files_created = []
        
        if args.format in ["csv", "all"]:
            csv_file = exporter.export_to_csv(result)
            files_created.append(("ìƒì„¸ CSV", csv_file))
            print(f"ğŸ“Š ìƒì„¸ CSV ìƒì„±: {csv_file}")
        
        if args.format in ["summary", "all"]:
            summary_file = exporter.export_summary_csv(result)
            files_created.append(("ìš”ì•½ CSV", summary_file))
            print(f"ğŸ“ˆ ìš”ì•½ CSV ìƒì„±: {summary_file}")
        
        if args.format in ["report", "all"]:
            report_file = exporter.generate_analysis_report(result)
            files_created.append(("ë¶„ì„ ë³´ê³ ì„œ", report_file))
            print(f"ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±: {report_file}")
        
        if args.format == "all":
            print(f"\nâœ… ì „ì²´ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ!")
            print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
            print("ğŸ“„ ìƒì„±ëœ íŒŒì¼:")
            for file_type, file_path in files_created:
                print(f"   - {file_type}: {Path(file_path).name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False


def list_checkpoints():
    """ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¶œë ¥"""
    from src.application.services.evaluation_checkpoint import EvaluationCheckpoint
    
    checkpoint_manager = EvaluationCheckpoint()
    sessions = checkpoint_manager.list_sessions()
    
    if not sessions:
        print("ğŸ“ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ’¾ ì €ì¥ëœ í‰ê°€ ì„¸ì…˜:")
    print("-" * 100)
    print(f"{'ì„¸ì…˜ ID':<25} {'ë°ì´í„°ì…‹':<20} {'ìƒíƒœ':<10} {'ì§„í–‰ë¥ ':<8} {'ì‹œì‘ ì‹œê°„':<20}")
    print("-" * 100)
    
    for session in sessions:
        session_id = session['session_id'][:24] + "..." if len(session['session_id']) > 24 else session['session_id']
        dataset_name = session['dataset_name'][:19] + "..." if len(session['dataset_name']) > 19 else session['dataset_name']
        status = session['status']
        progress = f"{session['progress']:.1f}%"
        start_time = session['start_time'][:19] if session['start_time'] else 'N/A'
        
        print(f"{session_id:<25} {dataset_name:<20} {status:<10} {progress:<8} {start_time:<20}")
    
    print(f"\nì´ {len(sessions)}ê°œì˜ ì„¸ì…˜ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")


def resume_evaluation(args):
    """ì¤‘ë‹¨ëœ í‰ê°€ ì¬ê°œ"""
    from src.application.services.evaluation_checkpoint import EvaluationCheckpoint, BatchEvaluationManager
    from src.container.factories.evaluation_use_case_factory import EvaluationRequest
    from src.utils.paths import get_evaluation_data_path
    from datasets import Dataset
    
    print(f"ğŸ”„ í‰ê°€ ì¬ê°œ ì‹œë„: {args.session_id}")
    
    checkpoint_manager = EvaluationCheckpoint()
    checkpoint = checkpoint_manager.resume_session(args.session_id)
    
    if not checkpoint:
        return False
    
    if checkpoint.get('status') == 'completed':
        print("âœ… ì´ë¯¸ ì™„ë£Œëœ í‰ê°€ì…ë‹ˆë‹¤.")
        
        # ì™„ë£Œëœ ê²°ê³¼ íŒŒì¼ í™•ì¸
        result_file = checkpoint_manager.checkpoint_dir / f"{args.session_id}.result.json"
        if result_file.exists():
            print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {result_file}")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                print("\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
                print(f"- RAGAS Score: {result.get('ragas_score', 0):.3f}")
                print(f"- ë°ì´í„°ì…‹ í¬ê¸°: {result.get('metadata', {}).get('dataset_size', 0)}ê°œ")
                print(f"- ì„±ê³µë¥ : {result.get('metadata', {}).get('success_rate', 0):.1f}%")
                
            except Exception as e:
                print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        return True
    
    if checkpoint.get('status') == 'failed':
        print("âŒ ì‹¤íŒ¨í•œ í‰ê°€ ì„¸ì…˜ì…ë‹ˆë‹¤.")
        partial_result = checkpoint.get('partial_result')
        if partial_result:
            print("ğŸ“Š ë¶€ë¶„ ê²°ê³¼ ì¶œë ¥:")
            print(f"- ë¶€ë¶„ RAGAS Score: {partial_result.get('ragas_score', 0):.3f}")
            print(f"- ì™„ë£Œëœ í•­ëª©: {len(partial_result.get('individual_scores', []))}ê°œ")
        return False
    
    # ì‹¤ì œ ì¬ê°œ êµ¬í˜„
    try:
        print("ğŸ”„ ì¤‘ë‹¨ëœ í‰ê°€ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤...")
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„¤ì • ë³µì›
        config = checkpoint.get('config', {})
        dataset_name = checkpoint.get('dataset_name')
        completed_items = checkpoint.get('completed_items', 0)
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")
        print(f"âœ… ì´ë¯¸ ì™„ë£Œ: {completed_items}ê°œ")
        print(f"ğŸ”„ ì¬ê°œ ìœ„ì¹˜: {completed_items + 1}ë²ˆì§¸ í•­ëª©ë¶€í„°")
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
        data_path = get_evaluation_data_path(dataset_name)
        if not data_path:
            print(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
            return False
        
        # ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¯¸ì™„ë£Œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        remaining_data = data[completed_items:]  # ë¯¸ì™„ë£Œ ë¶€ë¶„ë§Œ
        
        if not remaining_data:
            print("âœ… ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        
        print(f"ğŸ“‹ ë‚¨ì€ í•­ëª©: {len(remaining_data)}ê°œ")
        
        # í‰ê°€ ì„¤ì • ë³µì›
        llm_type = config.get('llm_type', 'gemini')
        embedding_type = config.get('embedding_type', 'gemini')
        prompt_type_str = config.get('prompt_type', 'default')
        
        # PromptType enumìœ¼ë¡œ ë³€í™˜
        from src.domain.prompts import PromptType
        if isinstance(prompt_type_str, str):
            try:
                prompt_type = PromptType(prompt_type_str)
            except ValueError:
                prompt_type = PromptType.DEFAULT
        else:
            prompt_type = prompt_type_str
        
        print(f"ğŸ¤– LLM: {llm_type}")
        print(f"ğŸŒ Embedding: {embedding_type}")
        print(f"ğŸ¯ Prompt Type: {prompt_type.value}")
        
        # í‰ê°€ ì–´ëŒ‘í„° ìƒì„±
        request = EvaluationRequest(
            llm_type=llm_type,
            embedding_type=embedding_type,
            prompt_type=prompt_type
        )
        
        evaluation_use_case, llm_adapter, embedding_adapter = container.create_evaluation_use_case(request)
        
        # ë‚¨ì€ ë°ì´í„°ë¡œ Dataset ìƒì„±
        remaining_dataset = Dataset.from_list(remaining_data)
        
        # RAGAS ì–´ëŒ‘í„° ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì¬ê°œ
        from src.infrastructure.evaluation.ragas_adapter_legacy import RagasEvalAdapter
        
        ragas_adapter = RagasEvalAdapter(
            llm=llm_adapter,
            embeddings=embedding_adapter,
            prompt_type=prompt_type
        )
        
        # ë°°ì¹˜ í‰ê°€ ê´€ë¦¬ì ìƒì„±
        batch_size = config.get('batch_size', 10)
        batch_manager = BatchEvaluationManager(checkpoint_manager, batch_size)
        
        # ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ë‚˜ë¨¸ì§€ í‰ê°€ ì‹¤í–‰
        def batch_eval_func(batch_dataset):
            return ragas_adapter._run_evaluation_with_timeout(batch_dataset)
        
        # ê¸°ì¡´ ê²°ê³¼ì™€ ìƒˆ ê²°ê³¼ í•©ì¹˜ê¸°
        print("ğŸ”„ ë‚˜ë¨¸ì§€ í‰ê°€ ì‹¤í–‰ ì¤‘...")
        remaining_result = batch_manager.evaluate_with_checkpoints(
            remaining_dataset, 
            batch_eval_func, 
            config
        )
        
        # ê¸°ì¡´ ê°œë³„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        existing_results = checkpoint.get('individual_results', [])
        new_results = remaining_result.get('individual_scores', [])
        
        # ì „ì²´ ê²°ê³¼ í•©ì¹˜ê¸°
        all_individual_results = existing_results + new_results
        
        # ìµœì¢… ê²°ê³¼ ì¬ê³„ì‚°
        final_result = batch_manager._compile_final_result(
            all_individual_results, 
            config, 
            remaining_result.get('metadata', {}).get('error_count', 0)
        )
        
        # ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
        checkpoint_manager.complete_session(final_result)
        
        print("\nâœ… í‰ê°€ ì¬ê°œ ì™„ë£Œ!")
        print("ğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"- RAGAS Score: {final_result.get('ragas_score', 0):.3f}")
        print(f"- ì „ì²´ í•­ëª©: {len(all_individual_results)}ê°œ")
        print(f"- ì„±ê³µë¥ : {final_result.get('metadata', {}).get('success_rate', 0):.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¬ê°œ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False


def cleanup_checkpoints(args):
    """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
    from src.application.services.evaluation_checkpoint import EvaluationCheckpoint
    
    print(f"ğŸ§¹ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì‹œì‘ ({args.days}ì¼ ì´ìƒëœ íŒŒì¼)")
    
    checkpoint_manager = EvaluationCheckpoint()
    checkpoint_manager.cleanup_old_sessions(args.days)
    
    print("âœ… ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")
    return True


def quick_eval(args):
    """ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (HCX-005 + BGE-M3 + ìë™ ê²°ê³¼ ì €ì¥)"""
    import os
    from datetime import datetime
    from pathlib import Path
    
    print("ğŸš€ RAGTrace ê°„ë‹¨ í‰ê°€ ì‹œì‘")
    print("=" * 50)
    print("ğŸ¤– LLM: HCX-005 (Naver)")
    print("ğŸŒ Embedding: BGE-M3 (Local)")
    print("ğŸ“ ë°ì´í„°ì…‹:", args.dataset)
    print("=" * 50)
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_filename = f"{args.dataset}_{timestamp}.json"
    result_path = output_dir / result_filename
    
    try:
        # 1. í‰ê°€ ì‹¤í–‰
        print("\nğŸ“Š 1ë‹¨ê³„: í‰ê°€ ì‹¤í–‰ ì¤‘...")
        success = evaluate_dataset(
            dataset_name=args.dataset,
            llm="hcx",
            embedding="bge_m3",
            prompt_type=None,
            output_file=str(result_path),
            verbose=args.verbose
        )
        
        if not success:
            print("âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨")
            return False
        
        # 2. ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        print(f"\nğŸ“¤ 2ë‹¨ê³„: ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì¤‘...")
        
        # export_results í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ê¸° ìœ„í•œ args ê°ì²´ ìƒì„±
        class ExportArgs:
            def __init__(self, result_file, output_dir):
                self.result_file = result_file
                self.format = "all"
                self.output_dir = output_dir
        
        export_args = ExportArgs(str(result_path), str(output_dir))
        export_success = export_results(export_args)
        
        if not export_success:
            print("âš ï¸ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨ (í‰ê°€ëŠ” ì™„ë£Œ)")
        
        # 3. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 50)
        print("âœ… ê°„ë‹¨ í‰ê°€ ì™„ë£Œ!")
        print("=" * 50)
        print(f"ğŸ“„ í‰ê°€ ê²°ê³¼ íŒŒì¼: {result_path}")
        
        if export_success:
            print(f"ğŸ“ ë¶„ì„ ë³´ê³ ì„œ ë””ë ‰í† ë¦¬: {output_dir}")
            print("ğŸ“‹ ìƒì„±ëœ íŒŒì¼:")
            print("   - ìƒì„¸ CSV: ê°œë³„ í•­ëª©ë³„ ì ìˆ˜")
            print("   - ìš”ì•½ CSV: ë©”íŠ¸ë¦­ë³„ ê¸°ì´ˆ í†µê³„")
            print("   - ë¶„ì„ ë³´ê³ ì„œ: ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ (Markdown)")
        
        print(f"\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print(f"   uv run python cli.py export-results {result_path} --format all --output-dir {output_dir}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ê°„ë‹¨ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
        from src.config import validate_settings
        validate_settings()
        
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "list-datasets":
        list_datasets()
        
    elif args.command == "list-prompts":
        list_prompts()
        
    elif args.command == "evaluate":
        success = evaluate_dataset(
            dataset_name=args.dataset,
            llm=args.llm,
            embedding=args.embedding,
            prompt_type=args.prompt_type,
            output_file=args.output,
            verbose=args.verbose
        )
        if not success:
            sys.exit(1)
            
    elif args.command == "export-results":
        success = export_results(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "import-data":
        success = import_data(
            input_file=args.input_file,
            output_file=args.output,
            validate=args.validate,
            batch_size=args.batch_size
        )
        if not success:
            sys.exit(1)
    
    elif args.command == "list-checkpoints":
        list_checkpoints()
    
    elif args.command == "resume-evaluation":
        success = resume_evaluation(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "cleanup-checkpoints":
        success = cleanup_checkpoints(args)
        if not success:
            sys.exit(1)
    
    elif args.command == "quick-eval":
        success = quick_eval(args)
        if not success:
            sys.exit(1)
    
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()