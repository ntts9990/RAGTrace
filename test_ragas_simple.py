#!/usr/bin/env python3
"""
RAGAS ë¬¸ì œ ì§„ë‹¨ìš© ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
"""

import os
from datasets import Dataset
from langchain_google_genai import GoogleGenerativeAI
from ragas import evaluate
from ragas.metrics import faithfulness

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if not os.getenv("GEMINI_API_KEY"):
    print("âŒ GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì˜ì–´)
test_data = {
    "question": ["What is the capital of France?"],
    "contexts": [["Paris is the capital and largest city of France."]],
    "answer": ["The capital of France is Paris."],
    "ground_truth": ["Paris is the capital of France."]
}

print("ğŸ§ª RAGAS ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì‹œì‘")
print("=" * 50)

try:
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = Dataset.from_dict(test_data)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(dataset)}ê°œ í•­ëª©")
    
    # LLM ì„¤ì •
    llm = GoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.1
    )
    print(f"âœ… LLM ì„¤ì • ì™„ë£Œ: {llm.model}")
    
    # ê°„ë‹¨í•œ LLM í…ŒìŠ¤íŠ¸
    test_response = llm.invoke("Hello, how are you?")
    print(f"âœ… LLM ì‘ë‹µ í…ŒìŠ¤íŠ¸: {test_response[:50]}...")
    
    # Faithfulnessë§Œ í…ŒìŠ¤íŠ¸
    print("\nğŸ” Faithfulness ë©”íŠ¸ë¦­ë§Œ í…ŒìŠ¤íŠ¸...")
    result = evaluate(
        dataset=dataset,
        metrics=[faithfulness],
        llm=llm,
        raise_exceptions=True  # ì˜¤ë¥˜ ë°œìƒì‹œ ë°”ë¡œ í™•ì¸
    )
    
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"ê²°ê³¼ íƒ€ì…: {type(result)}")
    
    if hasattr(result, "_scores_dict"):
        print(f"ì ìˆ˜: {result._scores_dict}")
    else:
        print(f"ê²°ê³¼ ì†ì„±: {dir(result)}")
        
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()