"""
Refactored Ragas Adapter using Strategy Pattern

Strategy íŒ¨í„´ì„ ì ìš©í•œ ë¦¬íŒ©í† ë§ëœ Ragas ì–´ëŒ‘í„°ì…ë‹ˆë‹¤.
"""

from typing import Any, Optional
import datetime
import uuid

import pandas as pd
from datasets import Dataset
from langchain_core.embeddings import Embeddings

from src.domain.prompts import PromptType
from src.infrastructure.evaluation.parsing_strategies import ResultParser
from src.infrastructure.evaluation.strategies import EvaluationContext


class RagasEvalAdapterV2:
    """Strategy íŒ¨í„´ì„ ì‚¬ìš©í•œ ë¦¬íŒ©í† ë§ëœ Ragas í‰ê°€ ì–´ëŒ‘í„°"""

    def __init__(
        self,
        llm: Any,  # LLM ì–´ëŒ‘í„° (GeminiAdapter ë˜ëŠ” HcxAdapter)
        embeddings: Embeddings,
        prompt_type: Optional[PromptType] = None,
    ):
        # LLM ì–´ëŒ‘í„°ì—ì„œ ì‹¤ì œ LangChain í˜¸í™˜ LLM ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        if hasattr(llm, 'get_llm'):
            self.llm = llm.get_llm()
        else:
            self.llm = llm
        
        self.embeddings = embeddings
        self.prompt_type = prompt_type or PromptType.DEFAULT
        
        # Strategy Context ì´ˆê¸°í™”
        self.evaluation_context = EvaluationContext(
            llm=self.llm,
            embeddings=self.embeddings,
            prompt_type=self.prompt_type
        )
        
        # ê²°ê³¼ íŒŒì„œ ì´ˆê¸°í™”
        self.result_parser = ResultParser()

    def evaluate(self, dataset: Dataset) -> dict[str, float]:
        """
        ì£¼ì–´ì§„ ë°ì´í„°ì…‹ê³¼ LLM, Embeddingì„ ì‚¬ìš©í•˜ì—¬ Ragas í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        try:
            # Strategy Contextë¥¼ í†µí•œ í‰ê°€ ì‹¤í–‰
            raw_result = self.evaluation_context.run_evaluation_with_timeout(dataset)
            
            # ê²°ê³¼ íŒŒì‹± ë° ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            result_dict = self._parse_result(raw_result, dataset)
            return self._create_final_report(result_dict, dataset)
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print("âš ï¸  í´ë°±: ìƒ˜í”Œ ê²°ê³¼ ë°˜í™˜")
            try:
                raw_result = self.evaluation_context._create_dummy_result(dataset)
                result_dict = self._parse_result(raw_result, dataset)
                return self._create_final_report(result_dict, dataset)
            except Exception as fallback_error:
                print(f"âŒ ìƒ˜í”Œ ê²°ê³¼ ìƒì„±ë„ ì‹¤íŒ¨: {str(fallback_error)}")
                return self._create_error_result()

    def _parse_result(self, result, dataset: Dataset) -> dict:
        """ê²°ê³¼ íŒŒì‹± - ì „ëµ íŒ¨í„´ì„ í†µí•œ ì•ˆì •ì ì¸ íŒŒì‹±"""
        try:
            metrics = self.evaluation_context.get_metrics()
            return self.result_parser.parse_result(result, dataset, metrics)
        except Exception as e:
            print(f"âŒ ëª¨ë“  íŒŒì‹± ì „ëµ ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ê²°ê³¼ ë°˜í™˜
            metrics = self.evaluation_context.get_metrics()
            result_dict = {metric.name: 0.0 for metric in metrics}
            result_dict["individual_scores"] = [
                {metric.name: 0.0 for metric in metrics} 
                for _ in range(len(dataset))
            ]
            return result_dict

    def _create_final_report(self, result_dict: dict, dataset: Dataset) -> dict:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        # ragas_score ê³„ì‚°
        metric_values = [v for k, v in result_dict.items() if k != "individual_scores" and v > 0]
        result_dict["ragas_score"] = sum(metric_values) / len(metric_values) if metric_values else 0.0
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result_dict["metadata"] = {
            "evaluation_id": str(uuid.uuid4())[:8],
            "timestamp": datetime.datetime.now().isoformat(),
            "model": str(self.llm.model),
            "temperature": getattr(self.llm, "temperature", 0.0),
            "dataset_size": len(dataset),
            "strategy": self.evaluation_context.primary_strategy.get_strategy_name(),
        }
        
        print(f"âœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: RAGAS Score = {result_dict['ragas_score']:.4f}")
        return result_dict

    def _create_error_result(self) -> dict:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        metrics = self.evaluation_context.get_metrics()
        error_result = {metric.name: 0.0 for metric in metrics}
        error_result["ragas_score"] = 0.0
        error_result["individual_scores"] = []
        return error_result