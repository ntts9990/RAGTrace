import time
from typing import Any, Optional

import pandas as pd
from datasets import Dataset

# Remove problematic import, use simpler approach
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pydantic import Field
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    context_precision,
    context_recall,
    faithfulness,
)

from src.domain.prompts import PromptType
from src.infrastructure.evaluation.custom_prompts import CustomPromptFactory


class RateLimitedEmbeddings(GoogleGenerativeAIEmbeddings):
    """Rate limitingì´ ì ìš©ëœ ì„ë² ë”© ë˜í¼"""

    requests_per_minute: int | None = Field(default=10, exclude=True)
    min_request_interval: float | None = Field(default=6.0, exclude=True)
    last_request_time: float | None = Field(default=0.0, exclude=True)

    def __init__(self, *args, requests_per_minute: int = 10, **kwargs):
        super().__init__(*args, **kwargs)
        self.requests_per_minute = requests_per_minute
        self.min_request_interval = 60.0 / requests_per_minute
        self.last_request_time = 0

    def _rate_limit(self):
        """ìš”ì²­ ê°„ ìµœì†Œ ì‹œê°„ ê°„ê²©ì„ ë³´ì¥"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        if time_since_last_request < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last_request
            time.sleep(sleep_time)
        self.last_request_time = time.time()

    def embed_documents(self, texts):
        """ë¬¸ì„œ ì„ë² ë”© ì‹œ rate limiting ì ìš©"""
        self._rate_limit()
        return super().embed_documents(texts)

    def embed_query(self, text):
        """ì¿¼ë¦¬ ì„ë² ë”© ì‹œ rate limiting ì ìš©"""
        self._rate_limit()
        return super().embed_query(text)


class RagasEvalAdapter:
    """Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ì„ ë‹´ë‹¹í•˜ëŠ” ì–´ëŒ‘í„°"""

    def __init__(
        self,
        embedding_model_name: str,
        api_key: str,
        embedding_requests_per_minute: int,
        prompt_type: Optional[PromptType] = None,
    ):
        self.embedding_model_name = embedding_model_name
        self.api_key = api_key
        self.embedding_requests_per_minute = embedding_requests_per_minute
        self.prompt_type = prompt_type or PromptType.DEFAULT

        # í”„ë¡¬í”„íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ë©”íŠ¸ë¦­ ì„¤ì •
        if self.prompt_type == PromptType.DEFAULT:
            # ê¸°ë³¸ RAGAS ë©”íŠ¸ë¦­ ì‚¬ìš©
            self.metrics = [
                faithfulness,
                answer_relevancy,
                context_recall,
                context_precision,
            ]
            print("ê¸°ë³¸ RAGAS í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì˜ì–´)")
        else:
            # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‚¬ìš©
            custom_metrics = CustomPromptFactory.create_custom_metrics(self.prompt_type)
            self.metrics = [
                custom_metrics['faithfulness'],
                custom_metrics['answer_relevancy'],
                custom_metrics['context_recall'],
                custom_metrics['context_precision'],
            ]
            prompt_description = CustomPromptFactory.get_prompt_type_description(self.prompt_type)
            print(f"ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {prompt_description}")

        # í‰ê°€ ê¸°ì¤€ ì„¤ëª…
        print("í‰ê°€ ê¸°ì¤€:")
        print("- Faithfulness: ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ì„± (ë¬¸ë§¥ ì¼ì¹˜ë„)")
        print("- Answer Relevancy: ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì—°ê´€ì„±")
        print("- Context Recall: ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„")
        print("- Context Precision: ê²€ìƒ‰ëœ ë¬¸ë§¥ì˜ ì •í™•ì„±")

    def _combine_individual_results(self, individual_results: dict, dataset: Dataset):
        """ê°œë³„ ë©”íŠ¸ë¦­ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ ê²°ê³¼ë¡œ í•©ì¹˜ê¸°"""
        print("ğŸ”„ ê°œë³„ ê²°ê³¼ë“¤ì„ í•©ì¹˜ëŠ” ì¤‘...")
        
        class CombinedResult:
            def __init__(self):
                self._scores_dict = {}
                self.dataset = dataset
        
        combined = CombinedResult()
        
        for metric_name, result in individual_results.items():
            if result and hasattr(result, "_scores_dict") and result._scores_dict:
                if metric_name in result._scores_dict:
                    combined._scores_dict[metric_name] = result._scores_dict[metric_name]
                    print(f"   âœ… {metric_name} ê²°ê³¼ ë³‘í•© ì™„ë£Œ")
                else:
                    print(f"   âš ï¸  {metric_name} ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print(f"   âŒ {metric_name} ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
        
        return combined

    def _create_dummy_result(self, dataset: Dataset):
        """í‰ê°€ ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ê²°ê³¼ ìƒì„±"""
        print("âš ï¸  ë”ë¯¸ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ëª¨ë“  í‰ê°€ê°€ ì‹¤íŒ¨í•¨)")
        
        class DummyResult:
            def __init__(self, dataset_size):
                self._scores_dict = {
                    'faithfulness': [0.5] * dataset_size,
                    'answer_relevancy': [0.5] * dataset_size,
                    'context_recall': [0.5] * dataset_size,
                    'context_precision': [0.5] * dataset_size,
                }
                self.dataset = dataset
        
        return DummyResult(len(dataset))

    def evaluate(self, dataset: Dataset, llm: Any) -> dict[str, float]:
        """
        ì£¼ì–´ì§„ ë°ì´í„°ì…‹ê³¼ LLMì„ ì‚¬ìš©í•˜ì—¬ Ragas í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        :param dataset: í‰ê°€í•  ë°ì´í„°ì…‹ (Hugging Face Dataset ê°ì²´)
        :param llm: í‰ê°€ì— ì‚¬ìš©í•  LLM ê°ì²´ (LangChain ì—°ë™)
        :return: í‰ê°€ ì§€í‘œë³„ ì ìˆ˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # 1. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            embeddings = self._initialize_embeddings()
            
            # 2. í‰ê°€ ì‹¤í–‰
            raw_result = self._run_evaluation(dataset, llm, embeddings)
            
            # 3. ê²°ê³¼ íŒŒì‹±
            result_dict = self._parse_result(raw_result, dataset)
            
            # 4. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            return self._create_final_report(result_dict, dataset, llm)
            
        except Exception as e:
            print(f"RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._create_error_result()

    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            embeddings = RateLimitedEmbeddings(
                model=self.embedding_model_name,
                google_api_key=self.api_key,
                requests_per_minute=self.embedding_requests_per_minute,
            )
            print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return embeddings
        except Exception as e:
            print(f"âš ï¸  ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            from langchain_google_genai import GoogleGenerativeAIEmbeddings
            embeddings = GoogleGenerativeAIEmbeddings(
                model=self.embedding_model_name,
                google_api_key=self.api_key,
            )
            print("âœ… ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ë¡œ fallback ì™„ë£Œ")
            return embeddings

    def _run_evaluation(self, dataset: Dataset, llm: Any, embeddings):
        """í‰ê°€ ì‹¤í–‰"""
        import datetime
        import uuid

        current_time = datetime.datetime.now()
        evaluation_id = str(uuid.uuid4())[:8]
        
        print("\n=== í•œêµ­ì–´ ì½˜í…íŠ¸ RAGAS í‰ê°€ ì‹œì‘ ===")
        print(f"ğŸ” í‰ê°€ ID: {evaluation_id}")
        print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ QA ìŒ")
        print(f"ğŸ¤– LLM ëª¨ë¸: {llm.model}")
        print("í‰ê°€ ì§„í–‰ ì¤‘...")

        try:
            print(f"ğŸš€ RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘... (ë©”íŠ¸ë¦­: {len(self.metrics)}ê°œ)")
            result = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                llm=llm,
                embeddings=embeddings,
                raise_exceptions=False,
            )
            print("âœ… RAGAS evaluate í•¨ìˆ˜ ì™„ë£Œ")
            return result
            
        except Exception as eval_error:
            print(f"âŒ RAGAS evaluate í•¨ìˆ˜ì—ì„œ ì˜¤ë¥˜: {eval_error}")
            return self._fallback_evaluation(dataset, llm, embeddings)

    def _fallback_evaluation(self, dataset: Dataset, llm: Any, embeddings):
        """í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¬ì‹œë„"""
        print("ğŸ”„ ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¬ì‹œë„...")
        from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
        basic_metrics = [faithfulness, answer_relevancy, context_recall, context_precision]
        
        try:
            result = evaluate(
                dataset=dataset,
                metrics=basic_metrics,
                llm=llm,
                embeddings=embeddings,
                raise_exceptions=False,
            )
            print("âœ… ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€ ì„±ê³µ")
            return result
        except Exception as basic_error:
            print(f"âŒ ê¸°ë³¸ ë©”íŠ¸ë¦­ë„ ì‹¤íŒ¨: {basic_error}")
            return self._create_dummy_result(dataset)

    def _parse_result(self, result, dataset: Dataset) -> dict:
        """ê²°ê³¼ íŒŒì‹± - DataFrame ë³€í™˜ì„ í†µí•œ ì•ˆì •ì ì¸ íŒŒì‹±"""
        try:
            # ragas ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ì•ˆì •ì ì¸ ë°©ë²•)
            if hasattr(result, 'to_pandas'):
                df = result.to_pandas()
                result_dict = {}
                individual_scores = []
                
                # DataFrameì—ì„œ ë©”íŠ¸ë¦­ ê°’ ì¶”ì¶œ
                for metric in self.metrics:
                    metric_name = metric.name
                    if metric_name in df.columns:
                        metric_values = df[metric_name].fillna(0.0)  # NaNì„ 0.0ìœ¼ë¡œ ëŒ€ì²´
                        result_dict[metric_name] = float(metric_values.mean())
                        print(f"âœ… {metric_name} í‰ê· : {result_dict[metric_name]:.4f}")
                    else:
                        result_dict[metric_name] = 0.0
                        print(f"âš ï¸  {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ê°œë³„ ì ìˆ˜ ì¶”ì¶œ
                for idx in range(len(dataset)):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        if metric_name in df.columns and idx < len(df):
                            score_value = df.iloc[idx][metric_name]
                            qa_scores[metric_name] = float(score_value) if pd.notna(score_value) else 0.0
                        else:
                            qa_scores[metric_name] = 0.0
                    individual_scores.append(qa_scores)
                
                result_dict["individual_scores"] = individual_scores
                return result_dict
                
            else:
                # Fallback: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                return self._parse_result_legacy(result, dataset)
                
        except Exception as e:
            print(f"DataFrame íŒŒì‹± ì‹¤íŒ¨: {e}, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback")
            return self._parse_result_legacy(result, dataset)

    def _parse_result_legacy(self, result, dataset: Dataset) -> dict:
        """ê¸°ì¡´ ê²°ê³¼ íŒŒì‹± ë°©ì‹ (legacy)"""
        result_dict = {}
        individual_scores = []

        # _scores_dictì—ì„œ ê²°ê³¼ ì¶”ì¶œ
        if hasattr(result, "_scores_dict") and result._scores_dict:
            scores_dict = result._scores_dict
            
            # ê°œë³„ QA ì ìˆ˜ ì¶”ì¶œ
            num_samples = len(dataset)
            for i in range(num_samples):
                qa_scores = {}
                for metric in self.metrics:
                    metric_name = metric.name
                    if metric_name in scores_dict:
                        scores = scores_dict[metric_name]
                        if isinstance(scores, list) and i < len(scores):
                            score_value = scores[i]
                            qa_scores[metric_name] = float(score_value) if score_value == score_value else 0.0
                        else:
                            qa_scores[metric_name] = float(scores) if scores == scores else 0.0
                    else:
                        qa_scores[metric_name] = 0.0
                individual_scores.append(qa_scores)

            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ í‰ê· ê°’ ê³„ì‚°
            for metric in self.metrics:
                metric_name = metric.name
                if metric_name in scores_dict:
                    scores = scores_dict[metric_name]
                    if isinstance(scores, list) and scores:
                        valid_scores = [float(s) for s in scores if s == s]  # NaN ì œì™¸
                        result_dict[metric_name] = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
                    else:
                        result_dict[metric_name] = float(scores) if scores == scores else 0.0
                else:
                    result_dict[metric_name] = 0.0
        else:
            # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ
            for metric in self.metrics:
                metric_name = metric.name
                if hasattr(result, metric_name):
                    value = getattr(result, metric_name)
                    result_dict[metric_name] = float(value) if value is not None else 0.0
                else:
                    result_dict[metric_name] = 0.0
            
            # ê°œë³„ ì ìˆ˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
            for i in range(len(dataset)):
                qa_scores = {metric.name: result_dict.get(metric.name, 0.0) for metric in self.metrics}
                individual_scores.append(qa_scores)

        result_dict["individual_scores"] = individual_scores
        return result_dict

    def _create_final_report(self, result_dict: dict, dataset: Dataset, llm: Any) -> dict:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        import datetime
        import uuid
        
        # ragas_score ê³„ì‚°
        metric_values = [v for k, v in result_dict.items() if k != "individual_scores" and v > 0]
        result_dict["ragas_score"] = sum(metric_values) / len(metric_values) if metric_values else 0.0
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result_dict["metadata"] = {
            "evaluation_id": str(uuid.uuid4())[:8],
            "timestamp": datetime.datetime.now().isoformat(),
            "model": str(llm.model),
            "temperature": getattr(llm, "temperature", 0.0),
            "dataset_size": len(dataset),
        }
        
        print(f"âœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: RAGAS Score = {result_dict['ragas_score']:.4f}")
        return result_dict

    def _create_error_result(self) -> dict:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        error_result = {metric.name: 0.0 for metric in self.metrics}
        error_result["ragas_score"] = 0.0
        error_result["individual_scores"] = []
        return error_result
