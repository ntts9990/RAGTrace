import time
from typing import Any, Optional

from datasets import Dataset

# Remove problematic import, use simpler approach
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pydantic import Field
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    context_precision,
    context_recall,
    faithfulness,
)

from src.domain.prompts import PromptType
from src.infrastructure.evaluation.custom_prompts import CustomPromptFactory


class RateLimitedEmbeddings(GoogleGenerativeAIEmbeddings):
    """Rate limitingì´ ì ìš©ëœ ì„ë² ë”© ë˜í¼"""

    requests_per_minute: int | None = Field(default=10, exclude=True)
    min_request_interval: float | None = Field(default=6.0, exclude=True)
    last_request_time: float | None = Field(default=0.0, exclude=True)

    def __init__(self, *args, requests_per_minute: int = 10, **kwargs):
        super().__init__(*args, **kwargs)
        self.requests_per_minute = requests_per_minute
        self.min_request_interval = 60.0 / requests_per_minute
        self.last_request_time = 0

    def _rate_limit(self):
        """ìš”ì²­ ê°„ ìµœì†Œ ì‹œê°„ ê°„ê²©ì„ ë³´ì¥"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        if time_since_last_request < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last_request
            time.sleep(sleep_time)
        self.last_request_time = time.time()

    def embed_documents(self, texts):
        """ë¬¸ì„œ ì„ë² ë”© ì‹œ rate limiting ì ìš©"""
        self._rate_limit()
        return super().embed_documents(texts)

    def embed_query(self, text):
        """ì¿¼ë¦¬ ì„ë² ë”© ì‹œ rate limiting ì ìš©"""
        self._rate_limit()
        return super().embed_query(text)


class RagasEvalAdapter:
    """Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ì„ ë‹´ë‹¹í•˜ëŠ” ì–´ëŒ‘í„°"""

    def __init__(
        self,
        embedding_model_name: str,
        api_key: str,
        embedding_requests_per_minute: int,
        prompt_type: Optional[PromptType] = None,
    ):
        self.embedding_model_name = embedding_model_name
        self.api_key = api_key
        self.embedding_requests_per_minute = embedding_requests_per_minute
        self.prompt_type = prompt_type or PromptType.DEFAULT

        # í”„ë¡¬í”„íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ë©”íŠ¸ë¦­ ì„¤ì •
        if self.prompt_type == PromptType.DEFAULT:
            # ê¸°ë³¸ RAGAS ë©”íŠ¸ë¦­ ì‚¬ìš©
            self.metrics = [
                faithfulness,
                answer_relevancy,
                context_recall,
                context_precision,
            ]
            print("ê¸°ë³¸ RAGAS í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì˜ì–´)")
        else:
            # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‚¬ìš©
            custom_metrics = CustomPromptFactory.create_custom_metrics(self.prompt_type)
            self.metrics = [
                custom_metrics['faithfulness'],
                custom_metrics['answer_relevancy'],
                custom_metrics['context_recall'],
                custom_metrics['context_precision'],
            ]
            prompt_description = CustomPromptFactory.get_prompt_type_description(self.prompt_type)
            print(f"ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {prompt_description}")

        # í‰ê°€ ê¸°ì¤€ ì„¤ëª…
        print("í‰ê°€ ê¸°ì¤€:")
        print("- Faithfulness: ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ì„± (ë¬¸ë§¥ ì¼ì¹˜ë„)")
        print("- Answer Relevancy: ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì—°ê´€ì„±")
        print("- Context Recall: ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„")
        print("- Context Precision: ê²€ìƒ‰ëœ ë¬¸ë§¥ì˜ ì •í™•ì„±")

    def _combine_individual_results(self, individual_results: dict, dataset: Dataset):
        """ê°œë³„ ë©”íŠ¸ë¦­ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ ê²°ê³¼ë¡œ í•©ì¹˜ê¸°"""
        print("ğŸ”„ ê°œë³„ ê²°ê³¼ë“¤ì„ í•©ì¹˜ëŠ” ì¤‘...")
        
        class CombinedResult:
            def __init__(self):
                self._scores_dict = {}
                self.dataset = dataset
        
        combined = CombinedResult()
        
        for metric_name, result in individual_results.items():
            if result and hasattr(result, "_scores_dict") and result._scores_dict:
                if metric_name in result._scores_dict:
                    combined._scores_dict[metric_name] = result._scores_dict[metric_name]
                    print(f"   âœ… {metric_name} ê²°ê³¼ ë³‘í•© ì™„ë£Œ")
                else:
                    print(f"   âš ï¸  {metric_name} ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print(f"   âŒ {metric_name} ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
        
        return combined

    def _create_dummy_result(self, dataset: Dataset):
        """í‰ê°€ ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ê²°ê³¼ ìƒì„±"""
        print("âš ï¸  ë”ë¯¸ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ëª¨ë“  í‰ê°€ê°€ ì‹¤íŒ¨í•¨)")
        
        class DummyResult:
            def __init__(self, dataset_size):
                self._scores_dict = {
                    'faithfulness': [0.5] * dataset_size,
                    'answer_relevancy': [0.5] * dataset_size,
                    'context_recall': [0.5] * dataset_size,
                    'context_precision': [0.5] * dataset_size,
                }
                self.dataset = dataset
        
        return DummyResult(len(dataset))

    def evaluate(self, dataset: Dataset, llm: Any) -> dict[str, float]:
        """
        ì£¼ì–´ì§„ ë°ì´í„°ì…‹ê³¼ LLMì„ ì‚¬ìš©í•˜ì—¬ Ragas í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        :param dataset: í‰ê°€í•  ë°ì´í„°ì…‹ (Hugging Face Dataset ê°ì²´)
        :param llm: í‰ê°€ì— ì‚¬ìš©í•  LLM ê°ì²´ (LangChain ì—°ë™)
        :return: í‰ê°€ ì§€í‘œë³„ ì ìˆ˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # Rate limitingì´ ì ìš©ëœ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
            try:
                embeddings = RateLimitedEmbeddings(
                    model=self.embedding_model_name,
                    google_api_key=self.api_key,
                    requests_per_minute=self.embedding_requests_per_minute,
                )
                print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # Fallback: ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©
                from langchain_google_genai import GoogleGenerativeAIEmbeddings
                embeddings = GoogleGenerativeAIEmbeddings(
                    model=self.embedding_model_name,
                    google_api_key=self.api_key,
                )
                print("âœ… ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ë¡œ fallback ì™„ë£Œ")

            print("\n=== í•œêµ­ì–´ ì½˜í…íŠ¸ RAGAS í‰ê°€ ì‹œì‘ ===")
            print("\ud55cêµ­ì–´ ë¬¸ì„œì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ê³ ë ¤í•œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:")
            print("- í•¨ì¶•ì  í‘œí˜„ê³¼ ê°„ì ‘ì  ì˜ë¯¸ ì „ë‹¬")
            print("- ì¡´ëŒ“ë§ê³¼ ê²¸ì†ì–´ ì‚¬ìš©")
            print("- í•œêµ­ ë¬¸í™”ì  ë¬¸ë§¥ ë°˜ì˜")
            print("- í•œìì–´ì™€ ìˆœìš°ë¦¬ë§ì˜ ë‰˜ì•™ìŠ¤")
            print("\nAPI ì†ë„ ì œí•œìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...\n")

            # Ragas í‰ê°€ ì‹¤í–‰
            # raise_exceptions=Falseë¡œ ì„¤ì •í•˜ì—¬ ì¼ë¶€ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
            # í‰ê°€ë§ˆë‹¤ ì•½ê°„ì˜ ë³€ë™ì„±ì„ ìœ„í•´ temperature ë“± ì„¤ì •

            import datetime
            import uuid

            current_time = datetime.datetime.now()
            evaluation_id = str(uuid.uuid4())[:8]
            print(f"ğŸ” í‰ê°€ ID: {evaluation_id}")
            print(f"ğŸ“… í‰ê°€ ì‹œì‘ ì‹œê°„: {current_time}")
            print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ QA ìŒ")
            print(f"ğŸ¤– LLM ëª¨ë¸: {llm.model}")
            print(f"ğŸŒ¡ï¸  Temperature: {getattr(llm, 'temperature', 'N/A')}")
            print("í‰ê°€ ì§„í–‰ ì¤‘...")

            # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
            print(f"ğŸ”§ ì‚¬ìš© ë©”íŠ¸ë¦­: {[m.name for m in self.metrics]}")
            print(f"ğŸ”§ LLM íƒ€ì…: {type(llm)}")
            print(f"ğŸ”§ ì„ë² ë”© íƒ€ì…: {type(embeddings)}")
            
            # ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (ë” ì•ˆì •ì ì¸ ì ‘ê·¼)
            try:
                print(f"ğŸš€ RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘... (ë©”íŠ¸ë¦­: {len(self.metrics)}ê°œ)")
                result = evaluate(
                    dataset=dataset,
                    metrics=self.metrics,
                    llm=llm,
                    embeddings=embeddings,
                    raise_exceptions=False,  # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
                )
                print("âœ… RAGAS evaluate í•¨ìˆ˜ ì™„ë£Œ")
            except Exception as eval_error:
                print(f"âŒ RAGAS evaluate í•¨ìˆ˜ì—ì„œ ì˜¤ë¥˜: {eval_error}")
                print(f"ì˜¤ë¥˜ íƒ€ì…: {type(eval_error)}")
                import traceback
                traceback.print_exc()
                
                # Fallback: ê¸°ë³¸ ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©
                print("ğŸ”„ ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¬ì‹œë„...")
                from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
                basic_metrics = [faithfulness, answer_relevancy, context_recall, context_precision]
                
                try:
                    result = evaluate(
                        dataset=dataset,
                        metrics=basic_metrics,
                        llm=llm,
                        embeddings=embeddings,
                        raise_exceptions=False,
                    )
                    print("âœ… ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€ ì„±ê³µ")
                except Exception as basic_error:
                    print(f"âŒ ê¸°ë³¸ ë©”íŠ¸ë¦­ë„ ì‹¤íŒ¨: {basic_error}")
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ë”ë¯¸ ê²°ê³¼ ë°˜í™˜
                    result = self._create_dummy_result(dataset)

            # EvaluationResult ê°ì²´ë¥¼ ìˆœìˆ˜í•œ dictë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            result_dict = {}
            individual_scores = []

            # result ê°ì²´ íƒ€ì… í™•ì¸
            print(f"í‰ê°€ ê²°ê³¼ íƒ€ì…: {type(result)}")

            # _scores_dictì—ì„œ ê²°ê³¼ ì¶”ì¶œ
            if hasattr(result, "_scores_dict") and result._scores_dict:
                scores_dict = result._scores_dict
                print(f"_scores_dict ë°œê²¬: {scores_dict}")

                # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
                for metric_name, scores in scores_dict.items():
                    print(f"ğŸ” {metric_name} ì ìˆ˜ í™•ì¸:")
                    print(f"   íƒ€ì…: {type(scores)}")
                    print(f"   ê°’: {scores}")
                    
                    if isinstance(scores, list):
                        nan_count = sum(1 for s in scores if s != s)  # NaN ì²´í¬
                        print(f"   ë¦¬ìŠ¤íŠ¸ ê¸¸ì´: {len(scores)}, NaN ê°œìˆ˜: {nan_count}")
                        if nan_count > 0:
                            print(f"   âš ï¸  NaN ê°’ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    elif scores != scores:  # NaN ì²´í¬
                        print(f"   âš ï¸  ì´ ì ìˆ˜ëŠ” NaNì…ë‹ˆë‹¤!")

                # ê°œë³„ QA ì ìˆ˜ ì¶”ì¶œ
                num_samples = len(dataset)
                for i in range(num_samples):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        if metric_name in scores_dict:
                            scores = scores_dict[metric_name]
                            if isinstance(scores, list) and i < len(scores):
                                score_value = scores[i]
                                # NaN ì²´í¬ ë° ì²˜ë¦¬
                                if score_value != score_value:  # NaN ì²´í¬
                                    print(f"âš ï¸  {metric_name}[{i}]ì—ì„œ NaN ê°ì§€, 0.0ìœ¼ë¡œ ëŒ€ì²´")
                                    qa_scores[metric_name] = 0.0
                                else:
                                    qa_scores[metric_name] = float(score_value)
                            else:
                                score_value = scores if scores is not None else 0.0
                                if score_value != score_value:  # NaN ì²´í¬
                                    print(f"âš ï¸  {metric_name}(ë‹¨ì¼ê°’)ì—ì„œ NaN ê°ì§€, 0.0ìœ¼ë¡œ ëŒ€ì²´")
                                    qa_scores[metric_name] = 0.0
                                else:
                                    qa_scores[metric_name] = float(score_value)
                        else:
                            qa_scores[metric_name] = 0.0
                    individual_scores.append(qa_scores)

                # ê° ë©”íŠ¸ë¦­ë³„ë¡œ í‰ê· ê°’ ê³„ì‚°
                for metric in self.metrics:
                    metric_name = metric.name
                    if metric_name in scores_dict:
                        scores = scores_dict[metric_name]
                        if isinstance(scores, list) and scores:
                            # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ í‰ê·  ê³„ì‚°
                            valid_scores = [float(s) for s in scores if s == s]  # NaN ì œì™¸
                            if valid_scores:
                                avg_score = sum(valid_scores) / len(valid_scores)
                                result_dict[metric_name] = avg_score
                                print(f"âœ… {metric_name} í‰ê· : {avg_score:.4f} (ìœ íš¨ê°’ {len(valid_scores)}/{len(scores)})")
                            else:
                                print(f"âš ï¸  {metric_name}: ëª¨ë“  ê°’ì´ NaNì´ë¯€ë¡œ 0.0ìœ¼ë¡œ ì„¤ì •")
                                result_dict[metric_name] = 0.0
                        else:
                            score_value = scores if scores is not None else 0.0
                            if score_value != score_value:  # NaN ì²´í¬
                                print(f"âš ï¸  {metric_name}(ë‹¨ì¼ê°’) NaN ê°ì§€, 0.0ìœ¼ë¡œ ì„¤ì •")
                                result_dict[metric_name] = 0.0
                            else:
                                result_dict[metric_name] = float(score_value)
                    else:
                        print(f"ê²½ê³ : {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result_dict[metric_name] = 0.0

            # _repr_dictì—ì„œ ëŒ€ì²´ê°’ í™•ì¸
            elif hasattr(result, "_repr_dict") and result._repr_dict:
                repr_dict = result._repr_dict
                print(f"_repr_dict ë°œê²¬: {repr_dict}")

                for metric in self.metrics:
                    metric_name = metric.name
                    if metric_name in repr_dict:
                        result_dict[metric_name] = float(repr_dict[metric_name])
                    else:
                        print(f"ê²½ê³ : {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result_dict[metric_name] = 0.0

                # ê°œë³„ ì ìˆ˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
                for i in range(len(dataset)):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        qa_scores[metric_name] = result_dict.get(metric_name, 0.0)
                    individual_scores.append(qa_scores)

            # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ ì‹œë„
            else:
                for metric in self.metrics:
                    metric_name = metric.name
                    if hasattr(result, metric_name):
                        value = getattr(result, metric_name)
                        result_dict[metric_name] = (
                            float(value) if value is not None else 0.0
                        )
                    else:
                        print(f"ê²½ê³ : {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result_dict[metric_name] = 0.0

                # ê°œë³„ ì ìˆ˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
                for i in range(len(dataset)):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        qa_scores[metric_name] = result_dict.get(metric_name, 0.0)
                    individual_scores.append(qa_scores)

            # ragas_score ê³„ì‚°
            if result_dict:
                values = [v for v in result_dict.values() if v > 0]
                result_dict["ragas_score"] = (
                    sum(values) / len(values) if values else 0.0
                )
            else:
                result_dict["ragas_score"] = 0.0

            # ê°œë³„ ì ìˆ˜ì™€ ë©”íƒ€ë°ì´í„° í¬í•¨
            result_dict["individual_scores"] = individual_scores
            result_dict["metadata"] = {
                "evaluation_id": evaluation_id,
                "timestamp": current_time.isoformat(),
                "model": str(llm.model),
                "temperature": getattr(llm, "temperature", 0.0),
                "dataset_size": len(dataset),
            }

            end_time = datetime.datetime.now()
            duration = (end_time - current_time).total_seconds()
            print(f"\\nâœ… í‰ê°€ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            print(f"ğŸ” í‰ê°€ ID: {evaluation_id}")
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {result_dict}")
            print(f"ğŸ‘¥ ê°œë³„ ì ìˆ˜ ê°œìˆ˜: {len(individual_scores)}")
            return result_dict

        except Exception as e:
            print(f"RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback

            traceback.print_exc()
            # ë””ë²„ê¹…ì„ ìœ„í•´ ë¹ˆ ê²°ê³¼ ëŒ€ì‹  ì˜¤ë¥˜ ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ ë°˜í™˜
            error_result = {metric.name: 0.0 for metric in self.metrics}
            error_result["ragas_score"] = 0.0
            error_result["individual_scores"] = []
            return error_result
