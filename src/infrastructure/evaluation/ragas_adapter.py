import time
from typing import Any, Dict, Optional

from datasets import Dataset

# Remove problematic import, use simpler approach
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pydantic import Field
from ragas import evaluate
from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness

import config


class RateLimitedEmbeddings(GoogleGenerativeAIEmbeddings):
    """Rate limitingì´ ì ìš©ëœ ì„ë² ë”© ë˜í¼"""

    requests_per_minute: Optional[int] = Field(default=10, exclude=True)
    min_request_interval: Optional[float] = Field(default=6.0, exclude=True)
    last_request_time: Optional[float] = Field(default=0.0, exclude=True)

    def __init__(self, *args, requests_per_minute: int = 10, **kwargs):
        super().__init__(*args, **kwargs)
        self.requests_per_minute = requests_per_minute
        self.min_request_interval = 60.0 / requests_per_minute
        self.last_request_time = 0

    def _rate_limit(self):
        """ìš”ì²­ ê°„ ìµœì†Œ ì‹œê°„ ê°„ê²©ì„ ë³´ì¥"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        if time_since_last_request < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last_request
            time.sleep(sleep_time)
        self.last_request_time = time.time()

    def embed_documents(self, texts):
        """ë¬¸ì„œ ì„ë² ë”© ì‹œ rate limiting ì ìš©"""
        self._rate_limit()
        return super().embed_documents(texts)

    def embed_query(self, text):
        """ì¿¼ë¦¬ ì„ë² ë”© ì‹œ rate limiting ì ìš©"""
        self._rate_limit()
        return super().embed_query(text)


class RagasEvalAdapter:
    """Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ì„ ë‹´ë‹¹í•˜ëŠ” ì–´ëŒ‘í„°"""

    def __init__(self):
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì‚¬ìš© (í˜„ì¬ë¡œì„œëŠ” ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ë‹¤êµ­ì–´ë¥¼ ì§€ì›í•˜ë¯€ë¡œ)
        # ì¶”í›„ í•„ìš”ì‹œ í•œêµ­ì–´ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì¶”ê°€ ê°€ëŠ¥
        self.metrics = [
            faithfulness,
            answer_relevancy,
            context_recall,
            context_precision,
        ]

        # í•œêµ­ì–´ í‰ê°€ë¥¼ ìœ„í•œ ì„¤ì • ë©”ì‹œì§€ ì¶”ê°€
        print("í•œêµ­ì–´ ì½˜í…íŠ¸ í‰ê°€ì— ìµœì í™”ëœ RAGAS ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print("í‰ê°€ ê¸°ì¤€:")
        print("- Faithfulness: ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ì„± (ë¬¸ë§¥ ì¼ì¹˜ë„)")
        print("- Answer Relevancy: ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì—°ê´€ì„±")
        print("- Context Recall: ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„")
        print("- Context Precision: ê²€ìƒ‰ëœ ë¬¸ë§¥ì˜ ì •í™•ì„±")

    def evaluate(self, dataset: Dataset, llm: Any) -> Dict[str, float]:
        """
        ì£¼ì–´ì§„ ë°ì´í„°ì…‹ê³¼ LLMì„ ì‚¬ìš©í•˜ì—¬ Ragas í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        :param dataset: í‰ê°€í•  ë°ì´í„°ì…‹ (Hugging Face Dataset ê°ì²´)
        :param llm: í‰ê°€ì— ì‚¬ìš©í•  LLM ê°ì²´ (LangChain ì—°ë™)
        :return: í‰ê°€ ì§€í‘œë³„ ì ìˆ˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # Rate limitingì´ ì ìš©ëœ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
            embeddings = RateLimitedEmbeddings(
                model=config.GEMINI_EMBEDDING_MODEL,  # configì—ì„œ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
                google_api_key=config.GEMINI_API_KEY,
                requests_per_minute=config.EMBEDDING_REQUESTS_PER_MINUTE,  # configì—ì„œ RPM ì„¤ì •
            )

            print("\n=== í•œêµ­ì–´ ì½˜í…íŠ¸ RAGAS í‰ê°€ ì‹œì‘ ===")
            print("\ud55cêµ­ì–´ ë¬¸ì„œì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ê³ ë ¤í•œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:")
            print("- í•¨ì¶•ì  í‘œí˜„ê³¼ ê°„ì ‘ì  ì˜ë¯¸ ì „ë‹¬")
            print("- ì¡´ëŒ“ë§ê³¼ ê²¸ì†ì–´ ì‚¬ìš©")
            print("- í•œêµ­ ë¬¸í™”ì  ë¬¸ë§¥ ë°˜ì˜")
            print("- í•œìì–´ì™€ ìˆœìš°ë¦¬ë§ì˜ ë‰˜ì•™ìŠ¤")
            print("\nAPI ì†ë„ ì œí•œìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...\n")

            # Ragas í‰ê°€ ì‹¤í–‰
            # raise_exceptions=Falseë¡œ ì„¤ì •í•˜ì—¬ ì¼ë¶€ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
            # í‰ê°€ë§ˆë‹¤ ì•½ê°„ì˜ ë³€ë™ì„±ì„ ìœ„í•´ temperature ë“± ì„¤ì •

            import datetime
            import uuid

            current_time = datetime.datetime.now()
            evaluation_id = str(uuid.uuid4())[:8]
            print(f"ğŸ” í‰ê°€ ID: {evaluation_id}")
            print(f"ğŸ“… í‰ê°€ ì‹œì‘ ì‹œê°„: {current_time}")
            print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ QA ìŒ")
            print(f"ğŸ¤– LLM ëª¨ë¸: {llm.model}")
            print(f"ğŸŒ¡ï¸  Temperature: {getattr(llm, 'temperature', 'N/A')}")
            print("í‰ê°€ ì§„í–‰ ì¤‘...")

            result = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                llm=llm,
                embeddings=embeddings,
                raise_exceptions=False,
            )

            # EvaluationResult ê°ì²´ë¥¼ ìˆœìˆ˜í•œ dictë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            result_dict = {}
            individual_scores = []

            # result ê°ì²´ íƒ€ì… í™•ì¸
            print(f"í‰ê°€ ê²°ê³¼ íƒ€ì…: {type(result)}")

            # _scores_dictì—ì„œ ê²°ê³¼ ì¶”ì¶œ
            if hasattr(result, "_scores_dict") and result._scores_dict:
                scores_dict = result._scores_dict
                print(f"_scores_dict ë°œê²¬: {scores_dict}")

                # ê°œë³„ QA ì ìˆ˜ ì¶”ì¶œ
                num_samples = len(dataset)
                for i in range(num_samples):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        if metric_name in scores_dict:
                            scores = scores_dict[metric_name]
                            if isinstance(scores, list) and i < len(scores):
                                qa_scores[metric_name] = float(scores[i])
                            else:
                                qa_scores[metric_name] = float(scores) if scores else 0.0
                        else:
                            qa_scores[metric_name] = 0.0
                    individual_scores.append(qa_scores)

                # ê° ë©”íŠ¸ë¦­ë³„ë¡œ í‰ê· ê°’ ê³„ì‚°
                for metric in self.metrics:
                    metric_name = metric.name
                    if metric_name in scores_dict:
                        scores = scores_dict[metric_name]
                        if isinstance(scores, list) and scores:
                            # numpy float64ë¥¼ ì¼ë°˜ floatë¡œ ë³€í™˜
                            avg_score = sum(float(s) for s in scores) / len(scores)
                            result_dict[metric_name] = avg_score
                        else:
                            result_dict[metric_name] = float(scores) if scores else 0.0
                    else:
                        print(f"ê²½ê³ : {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result_dict[metric_name] = 0.0

            # _repr_dictì—ì„œ ëŒ€ì²´ê°’ í™•ì¸
            elif hasattr(result, "_repr_dict") and result._repr_dict:
                repr_dict = result._repr_dict
                print(f"_repr_dict ë°œê²¬: {repr_dict}")

                for metric in self.metrics:
                    metric_name = metric.name
                    if metric_name in repr_dict:
                        result_dict[metric_name] = float(repr_dict[metric_name])
                    else:
                        print(f"ê²½ê³ : {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result_dict[metric_name] = 0.0

                # ê°œë³„ ì ìˆ˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
                for i in range(len(dataset)):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        qa_scores[metric_name] = result_dict.get(metric_name, 0.0)
                    individual_scores.append(qa_scores)

            # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ ì‹œë„
            else:
                for metric in self.metrics:
                    metric_name = metric.name
                    if hasattr(result, metric_name):
                        value = getattr(result, metric_name)
                        result_dict[metric_name] = float(value) if value is not None else 0.0
                    else:
                        print(f"ê²½ê³ : {metric_name} ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result_dict[metric_name] = 0.0

                # ê°œë³„ ì ìˆ˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
                for i in range(len(dataset)):
                    qa_scores = {}
                    for metric in self.metrics:
                        metric_name = metric.name
                        qa_scores[metric_name] = result_dict.get(metric_name, 0.0)
                    individual_scores.append(qa_scores)

            # ragas_score ê³„ì‚°
            if result_dict:
                values = [v for v in result_dict.values() if v > 0]
                result_dict["ragas_score"] = sum(values) / len(values) if values else 0.0
            else:
                result_dict["ragas_score"] = 0.0

            # ê°œë³„ ì ìˆ˜ì™€ ë©”íƒ€ë°ì´í„° í¬í•¨
            result_dict["individual_scores"] = individual_scores
            result_dict["metadata"] = {
                "evaluation_id": evaluation_id,
                "timestamp": current_time.isoformat(),
                "model": str(llm.model),
                "temperature": getattr(llm, "temperature", 0.0),
                "dataset_size": len(dataset),
            }

            end_time = datetime.datetime.now()
            duration = (end_time - current_time).total_seconds()
            print(f"\\nâœ… í‰ê°€ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            print(f"ğŸ” í‰ê°€ ID: {evaluation_id}")
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {result_dict}")
            print(f"ğŸ‘¥ ê°œë³„ ì ìˆ˜ ê°œìˆ˜: {len(individual_scores)}")
            return result_dict

        except Exception as e:
            print(f"RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback

            traceback.print_exc()
            # ë””ë²„ê¹…ì„ ìœ„í•´ ë¹ˆ ê²°ê³¼ ëŒ€ì‹  ì˜¤ë¥˜ ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ ë°˜í™˜
            error_result = {metric.name: 0.0 for metric in self.metrics}
            error_result["ragas_score"] = 0.0
            error_result["individual_scores"] = []
            return error_result
