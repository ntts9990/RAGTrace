"""
Custom Prompt Evaluation Strategy

ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì „ëµì…ë‹ˆë‹¤.
"""

from typing import Any, List
from datasets import Dataset
from ragas import evaluate
from ragas.run_config import RunConfig

from src.domain.prompts import PromptType
from src.infrastructure.evaluation.custom_prompts import CustomPromptFactory
from .base_strategy import EvaluationStrategy


class CustomPromptEvaluationStrategy(EvaluationStrategy):
    """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì „ëµ"""
    
    def __init__(self, llm, embeddings, prompt_type: PromptType):
        super().__init__(llm, embeddings)
        self.prompt_type = prompt_type
        # HCX ì‚¬ìš© ì‹œ ìˆœì°¨ ì²˜ë¦¬ë¡œ API í•œë„ ë°©ì§€
        max_workers = 1
        timeout = 600  # HCX ì‚¬ìš© ì‹œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
        
        # LLMì´ HCXê°€ ì•„ë‹Œ ê²½ìš° ë” ë§ì€ ì›Œì»¤ ì‚¬ìš© ê°€ëŠ¥
        if hasattr(llm, 'model') and 'HCX' not in str(llm.model):
            max_workers = 4
            timeout = 300
        
        self.run_config = RunConfig(
            timeout=timeout,
            max_retries=5,  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            max_workers=max_workers,
            max_wait=120,   # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            log_tenacity=True
        )
        
        # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìƒì„±
        self.custom_metrics = CustomPromptFactory.create_custom_metrics(self.prompt_type)
    
    def get_metrics(self) -> List[Any]:
        """ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ë°˜í™˜ (Context Precisionê³¼ Answer CorrectnessëŠ” ê¸°ë³¸ ë©”íŠ¸ë¦­ ì‚¬ìš©)"""
        from ragas.metrics import context_precision, answer_correctness
        
        return [
            self.custom_metrics['faithfulness'],
            self.custom_metrics['answer_relevancy'],
            self.custom_metrics['context_recall'],
            context_precision,  # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            answer_correctness,  # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì‚¬ìš© (ground_truth ë¹„êµ í•„ìš”)
        ]
    
    def run_evaluation(self, dataset: Dataset) -> Any:
        """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰"""
        prompt_description = CustomPromptFactory.get_prompt_type_description(self.prompt_type)
        print(f"ğŸš€ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰ ì¤‘: {prompt_description}")
        print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ QA ìŒ")
        print(f"ğŸ¤– LLM ëª¨ë¸: {self.llm.model}")
        
        # ì„ë² ë”© ëª¨ë¸ ì •ë³´ ì¶œë ¥
        embedding_info = f"ğŸŒ ì„ë² ë”© ëª¨ë¸: {type(self.embeddings).__name__}"
        if hasattr(self.embeddings, 'model_name'):
            embedding_info += f" ({self.embeddings.model_name})"
        elif hasattr(self.embeddings, 'device'):
            embedding_info += f" (ë””ë°”ì´ìŠ¤: {self.embeddings.device})"
        print(embedding_info)
        
        return evaluate(
            dataset=dataset,
            metrics=self.get_metrics(),
            llm=self.llm,
            embeddings=self.embeddings,
            run_config=self.run_config,
            raise_exceptions=False,
        )
    
    def get_strategy_name(self) -> str:
        """ì „ëµ ì´ë¦„ ë°˜í™˜"""
        prompt_description = CustomPromptFactory.get_prompt_type_description(self.prompt_type)
        return f"ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í‰ê°€ ({prompt_description})"