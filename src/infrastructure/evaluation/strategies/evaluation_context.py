"""
Evaluation Context

í‰ê°€ ì „ëµê³¼ ê´€ë ¨ ê°ì²´ë¥¼ ë‹´ëŠ” ë°ì´í„° ì»¨í…Œì´ë„ˆì…ë‹ˆë‹¤.
"""

from typing import Optional

from src.domain.prompts import PromptType
from .base_strategy import EvaluationStrategy
from .standard_evaluation_strategy import StandardEvaluationStrategy
from .custom_prompt_evaluation_strategy import CustomPromptEvaluationStrategy
from .fallback_evaluation_strategy import FallbackEvaluationStrategy
from .hcx_evaluation_strategy import HcxEvaluationStrategy


class EvaluationContext:
    """í‰ê°€ ì „ëµê³¼ ê´€ë ¨ ê°ì²´ë¥¼ ë‹´ëŠ” ë°ì´í„° ì»¨í…Œì´ë„ˆ"""
    
    def __init__(self, llm, embeddings, prompt_type: Optional[PromptType] = None):
        self.llm = llm
        self.embeddings = embeddings
        self.prompt_type = prompt_type or PromptType.DEFAULT
        
        # ì „ëµ ì„ íƒ
        self.primary_strategy: EvaluationStrategy = self._create_primary_strategy()
        self.fallback_strategy: EvaluationStrategy = FallbackEvaluationStrategy(llm, embeddings)
    
    def _create_primary_strategy(self) -> EvaluationStrategy:
        """ì£¼ìš” ì „ëµ ìƒì„±"""
        # HCX ëª¨ë¸ì¸ ê²½ìš° ì „ìš© ì „ëµ ì‚¬ìš©
        if hasattr(self.llm, 'model') and 'HCX' in str(self.llm.model):
            print("ğŸ”§ HCX ëª¨ë¸ ê°ì§€ - HCX ì „ìš© í‰ê°€ ì „ëµ ì‚¬ìš©")
            return HcxEvaluationStrategy(self.llm, self.embeddings)
        
        # ê¸°ë³¸ ì „ëµ ì„ íƒ
        if self.prompt_type == PromptType.DEFAULT:
            return StandardEvaluationStrategy(self.llm, self.embeddings)
        else:
            return CustomPromptEvaluationStrategy(self.llm, self.embeddings, self.prompt_type)
    
    def get_metrics(self):
        """í˜„ì¬ ì „ëµì˜ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.primary_strategy.get_metrics()