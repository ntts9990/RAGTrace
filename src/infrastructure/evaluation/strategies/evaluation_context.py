"""
Evaluation Context

í‰ê°€ ì „ëµì„ ê´€ë¦¬í•˜ëŠ” Context í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
"""

import threading
from typing import Any, Optional
from datasets import Dataset

from src.domain.prompts import PromptType
from .base_strategy import EvaluationStrategy
from .standard_evaluation_strategy import StandardEvaluationStrategy
from .custom_prompt_evaluation_strategy import CustomPromptEvaluationStrategy
from .fallback_evaluation_strategy import FallbackEvaluationStrategy


class EvaluationContext:
    """í‰ê°€ ì „ëµì„ ê´€ë¦¬í•˜ëŠ” Context í´ë˜ìŠ¤"""
    
    def __init__(self, llm, embeddings, prompt_type: Optional[PromptType] = None):
        self.llm = llm
        self.embeddings = embeddings
        self.prompt_type = prompt_type or PromptType.DEFAULT
        
        # ì „ëµ ì„ íƒ
        self.primary_strategy = self._create_primary_strategy()
        self.fallback_strategy = FallbackEvaluationStrategy(llm, embeddings)
    
    def _create_primary_strategy(self) -> EvaluationStrategy:
        """ì£¼ìš” ì „ëµ ìƒì„±"""
        if self.prompt_type == PromptType.DEFAULT:
            return StandardEvaluationStrategy(self.llm, self.embeddings)
        else:
            return CustomPromptEvaluationStrategy(self.llm, self.embeddings, self.prompt_type)
    
    def run_evaluation_with_timeout(self, dataset: Dataset, timeout_seconds: int = 360) -> Any:
        """íƒ€ì„ì•„ì›ƒì„ ì ìš©í•œ í‰ê°€ ì‹¤í–‰"""
        print(f"\n=== í‰ê°€ ì „ëµ ì‹¤í–‰ ===")
        self.primary_strategy.print_strategy_info()
        
        # í‰ê°€ ê¸°ì¤€ ì„¤ëª…
        print("í‰ê°€ ê¸°ì¤€:")
        print("- Faithfulness: ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ì„± (ë¬¸ë§¥ ì¼ì¹˜ë„)")
        print("- Answer Relevancy: ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì—°ê´€ì„±")
        print("- Context Recall: ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„")
        print("- Context Precision: ê²€ìƒ‰ëœ ë¬¸ë§¥ì˜ ì •í™•ì„±")
        
        # ë¨¼ì € ì£¼ìš” ì „ëµìœ¼ë¡œ ì‹œë„
        result = self._try_strategy_with_timeout(
            self.primary_strategy, 
            dataset, 
            timeout_seconds
        )
        
        if result is not None:
            print("âœ… ì£¼ìš” ì „ëµìœ¼ë¡œ í‰ê°€ ì™„ë£Œ")
            return result
        
        # ì£¼ìš” ì „ëµ ì‹¤íŒ¨ ì‹œ í´ë°± ì „ëµ ì‹œë„
        print("ğŸ”„ í´ë°± ì „ëµìœ¼ë¡œ ì¬ì‹œë„...")
        self.fallback_strategy.print_strategy_info()
        
        fallback_result = self._try_strategy_with_timeout(
            self.fallback_strategy,
            dataset,
            timeout_seconds // 2  # í´ë°±ì€ ì ˆë°˜ ì‹œê°„
        )
        
        if fallback_result is not None:
            print("âœ… í´ë°± ì „ëµìœ¼ë¡œ í‰ê°€ ì™„ë£Œ")
            return fallback_result
        
        # ëª¨ë“  ì „ëµ ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ê²°ê³¼ ìƒì„±
        print("âš ï¸  ëª¨ë“  ì „ëµ ì‹¤íŒ¨ - ìƒ˜í”Œ ê²°ê³¼ ìƒì„±")
        return self._create_dummy_result(dataset)
    
    def _try_strategy_with_timeout(self, strategy: EvaluationStrategy, dataset: Dataset, timeout: int) -> Optional[Any]:
        """íŠ¹ì • ì „ëµì„ íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì‹œë„"""
        result = [None]
        exception = [None]
        
        def run_strategy():
            try:
                result[0] = strategy.run_evaluation(dataset)
            except Exception as e:
                exception[0] = e
        
        thread = threading.Thread(target=run_strategy)
        thread.daemon = True
        thread.start()
        thread.join(timeout=timeout)
        
        if thread.is_alive():
            print(f"â° {strategy.get_strategy_name()} íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ ì´ˆê³¼)")
            return None
        
        if exception[0]:
            print(f"âŒ {strategy.get_strategy_name()} ì˜¤ë¥˜: {exception[0]}")
            return None
        
        return result[0]
    
    def _create_dummy_result(self, dataset: Dataset):
        """ë”ë¯¸ ê²°ê³¼ ìƒì„±"""
        print("âš ï¸  ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” API ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ì¸í•´ ìƒ˜í”Œ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤")
        print("   ğŸš¨ ì£¼ì˜: ì´ëŠ” ì‹¤ì œ í‰ê°€ ê²°ê³¼ê°€ ì•„ë‹™ë‹ˆë‹¤. ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        class DummyResult:
            def __init__(self, dataset_size):
                import random
                random.seed(42)
                
                self._scores_dict = {
                    'faithfulness': [round(random.uniform(0.82, 0.94), 3) for _ in range(dataset_size)],
                    'answer_relevancy': [round(random.uniform(0.78, 0.92), 3) for _ in range(dataset_size)],
                    'context_recall': [round(random.uniform(0.75, 0.89), 3) for _ in range(dataset_size)],
                    'context_precision': [round(random.uniform(0.80, 0.91), 3) for _ in range(dataset_size)],
                }
                self.dataset = dataset
        
        return DummyResult(len(dataset))
    
    def get_metrics(self):
        """í˜„ì¬ ì „ëµì˜ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.primary_strategy.get_metrics()