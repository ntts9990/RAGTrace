"""
BGE-M3 Î°úÏª¨ ÏûÑÎ≤†Îî© Î™®Îç∏ Ïñ¥ÎåëÌÑ∞
sentence-transformersÎ•º ÏÇ¨Ïö©Ìïú Î°úÏª¨ ÏûÑÎ≤†Îî© Ï≤òÎ¶¨
CUDA GPU ÏûêÎèô Í∞êÏßÄ Î∞è ÏµúÏ†ÅÌôî ÏßÄÏõê
"""

import os
import time
import platform
from typing import List, Optional

from langchain_core.embeddings import Embeddings
from sentence_transformers import SentenceTransformer
from src.config import SUPPORTED_DEVICE_TYPES


class BgeM3EmbeddingAdapter(Embeddings):
    """BGE-M3 Î°úÏª¨ ÏûÑÎ≤†Îî© Î™®Îç∏ Ïñ¥ÎåëÌÑ∞ (GPU ÏûêÎèô Í∞êÏßÄ)"""
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        BGE-M3 Î™®Îç∏ÏùÑ Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.
        
        Args:
            model_path: Î°úÏª¨ Î™®Îç∏ Í≤ΩÎ°ú (NoneÏù¥Î©¥ ÏûêÎèô Îã§Ïö¥Î°úÎìú)
            device: Ïã§Ìñâ ÎîîÎ∞îÏù¥Ïä§ (NoneÏù¥Î©¥ ÏûêÎèô Í∞êÏßÄ, "cpu", "cuda", "mps")
        """
        self.model_path = model_path or "BAAI/bge-m3"
        self.device = self._detect_best_device(device)
        self.model = None
        self.device_info = {}
        
        # Î°úÏª¨ Î™®Îç∏ Í≤ΩÎ°ú ÌôïÏù∏
        if model_path and os.path.exists(model_path):
            print(f"‚úÖ Î°úÏª¨ BGE-M3 Î™®Îç∏ Í≤ΩÎ°ú ÌôïÏù∏: {model_path}")
        elif model_path:
            print(f"‚ö†Ô∏è ÏßÄÏ†ïÎêú Î™®Îç∏ Í≤ΩÎ°úÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {model_path}")
            print("üì• Hugging FaceÏóêÏÑú Î™®Îç∏ÏùÑ Îã§Ïö¥Î°úÎìúÌï©ÎãàÎã§...")
        else:
            print("üì• BGE-M3 Î™®Îç∏ÏùÑ ÏûêÎèô Îã§Ïö¥Î°úÎìúÌï©ÎãàÎã§...")
        
        # Î™®Îç∏ Î°úÎìú (ÏßÄÏó∞ Î°úÎî©)
        self._load_model()
    
    def _detect_best_device(self, preferred_device: Optional[str] = None) -> str:
        """ÏµúÏ†ÅÏùò ÎîîÎ∞îÏù¥Ïä§Î•º ÏûêÎèô Í∞êÏßÄÌï©ÎãàÎã§."""
        if preferred_device:
            # ÏÇ¨Ïö©ÏûêÍ∞Ä Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏßÄÏ†ïÌïú Í≤ΩÏö∞
            if preferred_device in SUPPORTED_DEVICE_TYPES:
                print(f"üéØ ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÎîîÎ∞îÏù¥Ïä§: {preferred_device}")
                return preferred_device
            else:
                print(f"‚ö†Ô∏è ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÎîîÎ∞îÏù¥Ïä§: {preferred_device} (ÏßÄÏõê: {SUPPORTED_DEVICE_TYPES}), ÏûêÎèô Í∞êÏßÄÎ°ú Ï†ÑÌôò")
        
        print("üîç ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ Ï§ë...")
        
        # CUDA ÏßÄÏõê ÌôïÏù∏
        try:
            import torch
            if torch.cuda.is_available():
                cuda_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(current_device)
                memory_total = torch.cuda.get_device_properties(current_device).total_memory
                memory_gb = memory_total / (1024**3)
                
                self.device_info = {
                    "type": "cuda",
                    "count": cuda_count,
                    "current": current_device,
                    "name": device_name,
                    "memory_gb": round(memory_gb, 1),
                    "compute_capability": torch.cuda.get_device_capability(current_device)
                }
                
                print(f"‚úÖ CUDA GPU Í∞êÏßÄÎê®:")
                print(f"   - GPU Í∞úÏàò: {cuda_count}")
                print(f"   - ÌòÑÏû¨ GPU: {device_name}")
                print(f"   - VRAM: {memory_gb:.1f}GB")
                print(f"   - Compute Capability: {self.device_info['compute_capability']}")
                
                return "cuda"
                
        except ImportError:
            print("‚ö†Ô∏è PyTorchÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        except Exception as e:
            print(f"‚ö†Ô∏è CUDA ÌôïÏù∏ Ï§ë Ïò§Î•ò: {e}")
        
        # MPS (Apple Silicon) ÏßÄÏõê ÌôïÏù∏
        try:
            import torch
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device_info = {
                    "type": "mps",
                    "platform": platform.platform(),
                    "processor": platform.processor()
                }
                
                print(f"‚úÖ Apple MPS (Metal) Í∞êÏßÄÎê®:")
                print(f"   - ÌîåÎû´Ìèº: {platform.platform()}")
                print(f"   - ÌîÑÎ°úÏÑ∏ÏÑú: {platform.processor()}")
                
                return "mps"
                
        except ImportError:
            pass
        except Exception as e:
            print(f"‚ö†Ô∏è MPS ÌôïÏù∏ Ï§ë Ïò§Î•ò: {e}")
        
        # CPU Ìè¥Î∞±
        self.device_info = {
            "type": "cpu",
            "cores": os.cpu_count(),
            "platform": platform.platform(),
            "processor": platform.processor()
        }
        
        print(f"üíª CPU ÏÇ¨Ïö©:")
        print(f"   - ÏΩîÏñ¥ Ïàò: {os.cpu_count()}")
        print(f"   - ÌîåÎû´Ìèº: {platform.platform()}")
        
        return "cpu"
    
    def _load_model(self):
        """Î™®Îç∏ÏùÑ Î°úÎìúÌïòÍ≥† GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ ÏµúÏ†ÅÌôîÌï©ÎãàÎã§."""
        try:
            print(f"üîÑ BGE-M3 Î™®Îç∏ Î°úÎî© Ï§ë... (ÎîîÎ∞îÏù¥Ïä§: {self.device})")
            start_time = time.time()
            
            # GPU Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
            model_kwargs = {}
            if self.device == "cuda":
                # CUDA ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
                model_kwargs.update({
                    "model_kwargs": {
                        "torch_dtype": "auto",  # ÏûêÎèô precision ÏÑ†ÌÉù
                        "device_map": "auto",   # ÏûêÎèô GPU Î∞∞Ïπò
                    }
                })
                
                # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        print("üóëÔ∏è GPU Î©îÎ™®Î¶¨ Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")
                except:
                    pass
            
            self.model = SentenceTransformer(self.model_path, device=self.device, **model_kwargs)
            
            load_time = time.time() - start_time
            print(f"‚úÖ BGE-M3 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å ({load_time:.2f}Ï¥à)")
            
            # Î™®Îç∏ Ï†ïÎ≥¥ Ï∂úÎ†•
            print(f"üìä Î™®Îç∏ Ï†ïÎ≥¥:")
            print(f"   - Î™®Îç∏Î™Ö: {self.model_path}")
            print(f"   - ÎîîÎ∞îÏù¥Ïä§: {self.device}")
            print(f"   - ÏµúÎåÄ ÏãúÌÄÄÏä§ Í∏∏Ïù¥: {self.model.max_seq_length}")
            
            # GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÌôïÏù∏
            if self.device == "cuda":
                self._print_gpu_memory_usage()
            
        except Exception as e:
            print(f"‚ùå BGE-M3 Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            print("üí° Ìï¥Í≤∞ Î∞©Î≤ï:")
            print("   1. sentence-transformers ÏÑ§Ïπò: pip install sentence-transformers")
            print("   2. ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ ÌôïÏù∏ (Î™®Îç∏ Îã§Ïö¥Î°úÎìúÏö©)")
            print("   3. ÎîîÏä§ÌÅ¨ Í≥µÍ∞Ñ ÌôïÏù∏ (Î™®Îç∏ ÌÅ¨Í∏∞: ~2GB)")
            if self.device == "cuda":
                print("   4. GPU Î©îÎ™®Î¶¨ Î∂ÄÏ°± Ïãú CPUÎ°ú Ìè¥Î∞±: device='cpu'")
            raise
    
    def _print_gpu_memory_usage(self):
        """GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ Ï∂úÎ†•Ìï©ÎãàÎã§."""
        try:
            import torch
            if torch.cuda.is_available():
                current_device = torch.cuda.current_device()
                memory_allocated = torch.cuda.memory_allocated(current_device) / (1024**3)
                memory_reserved = torch.cuda.memory_reserved(current_device) / (1024**3)
                memory_total = torch.cuda.get_device_properties(current_device).total_memory / (1024**3)
                
                print(f"üîã GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ:")
                print(f"   - Ìï†ÎãπÎê®: {memory_allocated:.2f}GB")
                print(f"   - ÏòàÏïΩÎê®: {memory_reserved:.2f}GB")
                print(f"   - Ï†ÑÏ≤¥: {memory_total:.1f}GB")
                print(f"   - ÏÇ¨Ïö©Î•†: {(memory_allocated/memory_total)*100:.1f}%")
                
        except Exception as e:
            print(f"‚ö†Ô∏è GPU Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ ÌôïÏù∏ Ïã§Ìå®: {e}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Î¨∏ÏÑúÎì§ÏùÑ ÏûÑÎ≤†Îî©Ìï©ÎãàÎã§. (GPU ÏµúÏ†ÅÌôî)"""
        if not self.model:
            raise RuntimeError("Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        try:
            print(f"üîÑ {len(texts)}Í∞ú Î¨∏ÏÑú ÏûÑÎ≤†Îî© Ï§ë...")
            start_time = time.time()
            
            # ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Î∞∞Ïπò ÌÅ¨Í∏∞ ÏµúÏ†ÅÌôî
            if self.device == "cuda":
                batch_size = 64  # GPUÎäî ÌÅ∞ Î∞∞Ïπò ÌÅ¨Í∏∞
                show_progress = len(texts) > 20
            elif self.device == "mps":
                batch_size = 32  # Apple Silicon ÏµúÏ†ÅÌôî
                show_progress = len(texts) > 15
            else:
                batch_size = 16  # CPUÎäî ÏûëÏùÄ Î∞∞Ïπò ÌÅ¨Í∏∞
                show_progress = len(texts) > 10
            
            # BGE-M3ÏúºÎ°ú ÏûÑÎ≤†Îî© ÏÉùÏÑ±
            embeddings = self.model.encode(
                texts,
                convert_to_tensor=False,
                show_progress_bar=show_progress,
                batch_size=batch_size,
                normalize_embeddings=True  # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ ÏµúÏ†ÅÌôî
            )
            
            embed_time = time.time() - start_time
            throughput = len(texts) / embed_time if embed_time > 0 else 0
            print(f"‚úÖ Î¨∏ÏÑú ÏûÑÎ≤†Îî© ÏôÑÎ£å ({embed_time:.2f}Ï¥à, {throughput:.1f} docs/sec)")
            
            # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (ÎåÄÏö©Îüâ Ï≤òÎ¶¨ ÌõÑ)
            if self.device == "cuda" and len(texts) > 100:
                try:
                    import torch
                    torch.cuda.empty_cache()
                except:
                    pass
            
            # numpy arrayÎ•º listÎ°ú Î≥ÄÌôò
            if hasattr(embeddings, 'tolist'):
                return embeddings.tolist()
            else:
                return [emb.tolist() if hasattr(emb, 'tolist') else emb for emb in embeddings]
                
        except Exception as e:
            print(f"‚ùå Î¨∏ÏÑú ÏûÑÎ≤†Îî© Ïã§Ìå®: {e}")
            if self.device == "cuda" and "out of memory" in str(e).lower():
                print("üí° GPU Î©îÎ™®Î¶¨ Î∂ÄÏ°± Ïãú Î∞∞Ïπò ÌÅ¨Í∏∞Î•º Ï§ÑÏù¥Í±∞ÎÇò CPUÎ°ú Ï†ÑÌôòÌïòÏÑ∏Ïöî.")
            # Ïã§Ìå®Ìïú Í≤ΩÏö∞ Îπà Î≤°ÌÑ∞Î°ú Ï≤òÎ¶¨ (BGE-M3Îäî 1024Ï∞®Ïõê)
            return [[0.0] * 1024 for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """Îã®Ïùº ÏøºÎ¶¨Î•º ÏûÑÎ≤†Îî©Ìï©ÎãàÎã§."""
        if not self.model:
            raise RuntimeError("Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        try:
            print(f"üîÑ ÏøºÎ¶¨ ÏûÑÎ≤†Îî© Ï§ë: {text[:50]}...")
            start_time = time.time()
            
            # BGE-M3ÏúºÎ°ú ÏûÑÎ≤†Îî© ÏÉùÏÑ±
            embedding = self.model.encode(text, convert_to_tensor=False)
            
            embed_time = time.time() - start_time
            print(f"‚úÖ ÏøºÎ¶¨ ÏûÑÎ≤†Îî© ÏôÑÎ£å ({embed_time:.3f}Ï¥à)")
            
            # numpy arrayÎ•º listÎ°ú Î≥ÄÌôò
            if hasattr(embedding, 'tolist'):
                return embedding.tolist()
            else:
                return embedding
                
        except Exception as e:
            print(f"‚ùå ÏøºÎ¶¨ ÏûÑÎ≤†Îî© Ïã§Ìå®: {e}")
            # Ïã§Ìå®Ìïú Í≤ΩÏö∞ Îπà Î≤°ÌÑ∞Î°ú Ï≤òÎ¶¨ (BGE-M3Îäî 1024Ï∞®Ïõê)
            return [0.0] * 1024
    
    def similarity(self, embeddings1: List[List[float]], embeddings2: List[List[float]]) -> List[List[float]]:
        """ÏûÑÎ≤†Îî© Í∞Ñ Ïú†ÏÇ¨ÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§."""
        if not self.model:
            raise RuntimeError("Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        try:
            import numpy as np
            
            # numpy arrayÎ°ú Î≥ÄÌôò
            emb1 = np.array(embeddings1)
            emb2 = np.array(embeddings2)
            
            # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            similarity_matrix = self.model.similarity(emb1, emb2)
            
            # numpy arrayÎ•º listÎ°ú Î≥ÄÌôò
            if hasattr(similarity_matrix, 'tolist'):
                return similarity_matrix.tolist()
            else:
                return similarity_matrix
                
        except Exception as e:
            print(f"‚ùå Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            # Ïã§Ìå®Ìïú Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í Î∞òÌôò
            return [[0.0] * len(embeddings2) for _ in embeddings1]
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """ÎπÑÎèôÍ∏∞ Î¨∏ÏÑú ÏûÑÎ≤†Îî© (ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú Ìè¥Î∞±)"""
        return self.embed_documents(texts)
    
    async def aembed_query(self, text: str) -> List[float]:
        """ÎπÑÎèôÍ∏∞ ÏøºÎ¶¨ ÏûÑÎ≤†Îî© (ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú Ìè¥Î∞±)"""
        return self.embed_query(text)
    
    def save_model_locally(self, save_path: str):
        """Î™®Îç∏ÏùÑ Î°úÏª¨Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§."""
        if not self.model:
            raise RuntimeError("Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        try:
            print(f"üíæ BGE-M3 Î™®Îç∏ÏùÑ Î°úÏª¨Ïóê Ï†ÄÏû• Ï§ë: {save_path}")
            self.model.save(save_path)
            print(f"‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: {save_path}")
            
        except Exception as e:
            print(f"‚ùå Î™®Îç∏ Ï†ÄÏû• Ïã§Ìå®: {e}")
            raise
    
    def get_model_info(self) -> dict:
        """Î™®Îç∏ Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§."""
        if not self.model:
            return {"status": "not_loaded"}
        
        info = {
            "status": "loaded",
            "model_path": self.model_path,
            "device": self.device,
            "max_seq_length": getattr(self.model, 'max_seq_length', 'unknown'),
            "embedding_dimension": 1024,  # BGE-M3 Í≥†Ï†ï Ï∞®Ïõê
            "model_type": "BGE-M3",
            "device_info": self.device_info
        }
        
        # GPU Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.device == "cuda":
            try:
                import torch
                if torch.cuda.is_available():
                    current_device = torch.cuda.current_device()
                    memory_allocated = torch.cuda.memory_allocated(current_device) / (1024**3)
                    memory_total = torch.cuda.get_device_properties(current_device).total_memory / (1024**3)
                    
                    info["gpu_memory"] = {
                        "allocated_gb": round(memory_allocated, 2),
                        "total_gb": round(memory_total, 1),
                        "usage_percent": round((memory_allocated/memory_total)*100, 1)
                    }
            except:
                pass
        
        return info
    
    def get_device_capabilities(self) -> dict:
        """ÌòÑÏû¨ ÎîîÎ∞îÏù¥Ïä§Ïùò ÏÑ±Îä• Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§."""
        capabilities = {
            "device": self.device,
            "auto_detected": True,
        }
        
        if self.device == "cuda":
            try:
                import torch
                if torch.cuda.is_available():
                    current_device = torch.cuda.current_device()
                    props = torch.cuda.get_device_properties(current_device)
                    
                    capabilities.update({
                        "gpu_name": props.name,
                        "compute_capability": f"{props.major}.{props.minor}",
                        "total_memory_gb": round(props.total_memory / (1024**3), 1),
                        "multiprocessor_count": props.multiprocessor_count,
                        "recommended_batch_size": 64,
                        "supports_mixed_precision": props.major >= 7,  # Volta Ïù¥ÏÉÅ
                    })
            except:
                pass
                
        elif self.device == "mps":
            capabilities.update({
                "platform": platform.platform(),
                "recommended_batch_size": 32,
                "supports_mixed_precision": True,
            })
            
        else:  # CPU
            capabilities.update({
                "cpu_cores": os.cpu_count(),
                "platform": platform.platform(),
                "recommended_batch_size": 16,
                "supports_mixed_precision": False,
            })
        
        return capabilities