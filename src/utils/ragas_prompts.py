"""
RAGAS ë©”íŠ¸ë¦­ì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹°

ì´ ëª¨ë“ˆì€ RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” í‰ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì™€ì„œ
Streamlit ì¸í„°í˜ì´ìŠ¤ì— í‘œì‹œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

from typing import Dict, Optional

from ragas.metrics import (
    answer_relevancy,
    context_precision,
    context_recall,
    faithfulness,
)


def get_ragas_prompt(metric_name: str) -> Optional[str]:
    """íŠ¹ì • ë©”íŠ¸ë¦­ì˜ ì‹¤ì œ í‰ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        metric_name: ë©”íŠ¸ë¦­ ì´ë¦„ ('faithfulness', 'answer_relevancy', 'context_recall', 'context_precision')
        
    Returns:
        í•´ë‹¹ ë©”íŠ¸ë¦­ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´, ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ None
    """
    metrics = {
        'faithfulness': faithfulness,
        'answer_relevancy': answer_relevancy,
        'context_recall': context_recall,
        'context_precision': context_precision,
    }
    
    metric = metrics.get(metric_name)
    if not metric:
        return None
    
    # RAGAS ë²„ì „ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì†ì„±ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    prompt_attributes = [
        'faithfulness_prompt',
        'question_generation', 
        'context_recall_prompt',
        'context_precision_prompt',
        'llm_prompt',
        '_evaluation_prompt',
        'prompt',
        'template',
    ]
    
    for attr in prompt_attributes:
        if hasattr(metric, attr):
            prompt_obj = getattr(metric, attr)
            if prompt_obj:
                # í”„ë¡¬í”„íŠ¸ ê°ì²´ì—ì„œ ì‹¤ì œ í…œí”Œë¦¿ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if hasattr(prompt_obj, 'template'):
                    return prompt_obj.template
                elif hasattr(prompt_obj, 'format_string'):
                    return prompt_obj.format_string
                elif isinstance(prompt_obj, str):
                    return prompt_obj
                else:
                    return str(prompt_obj)
    
    return None


def get_all_ragas_prompts() -> Dict[str, str]:
    """ëª¨ë“  RAGAS ë©”íŠ¸ë¦­ì˜ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Returns:
        ë©”íŠ¸ë¦­ëª…ì„ í‚¤ë¡œ í•˜ê³  í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    metric_names = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    prompts = {}
    
    for metric_name in metric_names:
        prompt = get_ragas_prompt(metric_name)
        if prompt:
            prompts[metric_name] = prompt
        else:
            prompts[metric_name] = f"í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({metric_name})"
    
    return prompts


def extract_prompt_from_metric_object(metric_obj) -> Optional[str]:
    """ë©”íŠ¸ë¦­ ê°ì²´ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    # ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ì†ì„±ë“¤ì„ ì²´í¬
    possible_attrs = [
        'faithfulness_prompt',
        'question_generation',
        'context_recall_prompt', 
        'context_precision_prompt',
        'llm_prompt',
        '_evaluation_prompt',
        'prompt',
        'template',
    ]
    
    for attr in possible_attrs:
        if hasattr(metric_obj, attr):
            prompt_obj = getattr(metric_obj, attr)
            if prompt_obj:
                # ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ê°ì²´ íƒ€ì… ì²˜ë¦¬
                if hasattr(prompt_obj, 'template'):
                    return prompt_obj.template
                elif hasattr(prompt_obj, 'format_string'):
                    return prompt_obj.format_string
                elif hasattr(prompt_obj, 'prompt'):
                    return prompt_obj.prompt
                elif isinstance(prompt_obj, str):
                    return prompt_obj
                else:
                    # ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ ì‹œë„
                    str_repr = str(prompt_obj)
                    if len(str_repr) > 50:  # ì˜ë¯¸ìˆëŠ” í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸
                        return str_repr
    
    return None


def format_prompt_for_display(prompt: str, metric_name: str) -> str:
    """í”„ë¡¬í”„íŠ¸ë¥¼ Streamlit í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    
    Args:
        prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        metric_name: ë©”íŠ¸ë¦­ ì´ë¦„
        
    Returns:
        í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    if not prompt:
        return f"âŒ {metric_name} í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # í”„ë¡¬í”„íŠ¸ ì •ë¦¬ ë° í¬ë§·íŒ…
    formatted = prompt.strip()
    
    # ë³€ìˆ˜ í”Œë ˆì´ìŠ¤í™€ë” ì„¤ëª… ì¶”ê°€
    variable_info = {
        'faithfulness': 'ë³€ìˆ˜: {question}, {answer}, {context}',
        'answer_relevancy': 'ë³€ìˆ˜: {question}, {answer}',
        'context_recall': 'ë³€ìˆ˜: {question}, {ground_truth}, {context}',
        'context_precision': 'ë³€ìˆ˜: {question}, {answer}, {context}',
    }
    
    if metric_name in variable_info:
        formatted += f"\n\nğŸ“ **{variable_info[metric_name]}**"
    
    return formatted


# ë©”íŠ¸ë¦­ë³„ í”„ë¡¬í”„íŠ¸ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
_prompt_cache = {}


def get_cached_ragas_prompt(metric_name: str) -> str:
    """ìºì‹œëœ RAGAS í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        metric_name: ë©”íŠ¸ë¦­ ì´ë¦„
        
    Returns:
        í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    if metric_name not in _prompt_cache:
        raw_prompt = get_ragas_prompt(metric_name)
        _prompt_cache[metric_name] = format_prompt_for_display(raw_prompt, metric_name)
    
    return _prompt_cache[metric_name]