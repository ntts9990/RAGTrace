"""
ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸ - ì™„ì „ ì¬ì‘ì„±
ê°œë³„ QA ìŒì˜ ìƒì„¸ í‰ê°€ ê²°ê³¼ ë¶„ì„
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
import sqlite3
from pathlib import Path
import os


def get_project_root():
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë°˜í™˜"""
    # í˜„ì¬ íŒŒì¼ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ê¹Œì§€ì˜ ê²½ë¡œ ê³„ì‚°
    current_file = Path(__file__).resolve()
    # src/presentation/web/components/detailed_analysis_new.pyì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ
    project_root = current_file.parent.parent.parent.parent
    return project_root


def load_evaluation_data():
    """í‰ê°€ìš© ë°ì´í„° ë¡œë“œ - ê²½ë¡œ ë¬¸ì œ í•´ê²°"""
    project_root = get_project_root()
    
    # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
    possible_paths = [
        project_root / "data" / "evaluation_data.json",
        project_root / "data" / "evaluation_data_variant1.json",
        Path.cwd() / "data" / "evaluation_data.json",
        Path.cwd() / "data" / "evaluation_data_variant1.json"
    ]
    
    for path in possible_paths:
        if path.exists():
            try:
                with open(path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {path.name}")
                return data
            except Exception as e:
                st.error(f"ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({path.name}): {e}")
                continue
    
    # ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ì‹œ ë””ë²„ê·¸ ì •ë³´
    st.error("âŒ í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.info("í™•ì¸ëœ ê²½ë¡œ:")
    for i, path in enumerate(possible_paths, 1):
        exists = "âœ…" if path.exists() else "âŒ"
        st.text(f"{i}. {exists} {path}")
    
    return None


def get_db_path():
    """ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
    return Path(__file__).parent.parent / "evaluations.db"


def load_latest_evaluation_results():
    """ìµœì‹  í‰ê°€ ê²°ê³¼ì™€ ê°œë³„ QA ì ìˆ˜ ë¡œë“œ"""
    try:
        db_path = get_db_path()
        if not db_path.exists():
            st.warning("ğŸ“Š í‰ê°€ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, []
        
        conn = sqlite3.connect(str(db_path))
        
        # ìµœì‹  í‰ê°€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        query = '''
            SELECT raw_data 
            FROM evaluations 
            ORDER BY timestamp DESC 
            LIMIT 1
        '''
        
        result = conn.execute(query).fetchone()
        conn.close()
        
        if result and result[0]:
            raw_data = json.loads(result[0])
            individual_scores = raw_data.get('individual_scores', [])
            st.success(f"âœ… í‰ê°€ ê²°ê³¼ ë¡œë“œ ì„±ê³µ: {len(individual_scores)}ê°œ QA ì ìˆ˜")
            return raw_data, individual_scores
        
        st.warning("ğŸ“Š í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None, []
        
    except Exception as e:
        st.error(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, []


def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ë©”ì¸ í™”ë©´ - ì „ë©´ ì¬ì‘ì„±"""
    st.header("ğŸ” ìƒì„¸ ë¶„ì„")
    
    # ë°ì´í„° ë¡œë“œ ìƒíƒœ í™•ì¸
    st.subheader("ğŸ“Š ë°ì´í„° ë¡œë“œ ìƒíƒœ")
    
    with st.expander("ë°ì´í„° ë¡œë“œ ì§„ë‹¨", expanded=False):
        # 1. í‰ê°€ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸
        st.markdown("#### 1. í‰ê°€ ë°ì´í„° íŒŒì¼ í™•ì¸")
        evaluation_data = load_evaluation_data()
        
        if evaluation_data:
            st.success(f"âœ… í‰ê°€ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(evaluation_data)}ê°œ QA ìŒ")
        else:
            st.error("âŒ í‰ê°€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # 2. í‰ê°€ ê²°ê³¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
        st.markdown("#### 2. í‰ê°€ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸")
        latest_results, individual_scores = load_latest_evaluation_results()
        
        if latest_results:
            st.success(f"âœ… í‰ê°€ ê²°ê³¼ ë¡œë“œ ì„±ê³µ")
            st.json({
                "ì „ì²´ ë©”íŠ¸ë¦­": {k: v for k, v in latest_results.items() if k in ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'ragas_score']},
                "ê°œë³„ ì ìˆ˜ ê°œìˆ˜": len(individual_scores)
            })
        else:
            st.error("âŒ í‰ê°€ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨")
    
    # ì‹¤ì œ ë°ì´í„°ê°€ ëª¨ë‘ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ë¶„ì„ ì§„í–‰
    if not evaluation_data:
        st.error("âŒ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € evaluation_data.json íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    if not latest_results:
        st.error("âŒ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ Overview í˜ì´ì§€ì—ì„œ 'ìƒˆ í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        return
    
    # ì„±ê³µì ìœ¼ë¡œ ë°ì´í„°ê°€ ë¡œë“œëœ ê²½ìš° ë¶„ì„ ì‹œì‘
    st.success("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š QA ê°œë³„ ë¶„ì„", "ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„í¬", "ğŸ¯ íŒ¨í„´ ë¶„ì„"])
    
    with tab1:
        show_qa_analysis_new(evaluation_data, individual_scores)
    
    with tab2:
        show_metric_distribution_new(evaluation_data, latest_results, individual_scores)
    
    with tab3:
        show_pattern_analysis_new(evaluation_data, latest_results, individual_scores)


def show_qa_analysis_new(evaluation_data, individual_scores):
    """ê°œë³„ QA ë¶„ì„ - ì¬ì‘ì„±"""
    st.subheader("ğŸ“‹ ì§ˆë¬¸-ë‹µë³€ ìŒë³„ ìƒì„¸ ë¶„ì„")
    
    # QA ìŒ ì„ íƒ
    qa_count = len(evaluation_data)
    st.info(f"ğŸ“Š ì´ {qa_count}ê°œì˜ QA ìŒì´ ìˆìŠµë‹ˆë‹¤.")
    
    qa_options = []
    for i, qa in enumerate(evaluation_data):
        question_preview = qa['question'][:50] + "..." if len(qa['question']) > 50 else qa['question']
        qa_options.append(f"Q{i+1}: {question_preview}")
    
    selected_qa_idx = st.selectbox(
        "ë¶„ì„í•  QA ì„ íƒ", 
        range(len(qa_options)), 
        format_func=lambda x: qa_options[x]
    )
    
    if selected_qa_idx is not None:
        qa_data = evaluation_data[selected_qa_idx]
        
        # í•´ë‹¹ QAì˜ ê°œë³„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ì•ˆì „í•˜ê²Œ)
        qa_scores = None
        if individual_scores and selected_qa_idx < len(individual_scores):
            qa_scores = individual_scores[selected_qa_idx]
        
        show_individual_qa_details_new(qa_data, selected_qa_idx + 1, qa_scores)


def show_individual_qa_details_new(qa_data, qa_number, qa_scores=None):
    """ê°œë³„ QA ìƒì„¸ ì •ë³´ í‘œì‹œ - ì¬ì‘ì„±"""
    st.markdown(f"### ğŸ“ QA {qa_number} ìƒì„¸ ë¶„ì„")
    
    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ¤” ì§ˆë¬¸")
        st.info(qa_data['question'])
        
        st.markdown("#### ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
        st.success(qa_data['answer'])
    
    with col2:
        st.markdown("#### ğŸ“š ì œê³µëœ ì»¨í…ìŠ¤íŠ¸")
        for i, context in enumerate(qa_data['contexts'], 1):
            with st.expander(f"ì»¨í…ìŠ¤íŠ¸ {i}"):
                st.text(context)
        
        st.markdown("#### âœ… ì •ë‹µ (Ground Truth)")
        st.info(qa_data['ground_truth'])
    
    # í‰ê°€ ì ìˆ˜ í‘œì‹œ
    st.markdown("#### ğŸ“Š ì´ QAì˜ í‰ê°€ ì ìˆ˜")
    
    if qa_scores:
        # ì‹¤ì œ í‰ê°€ ê²°ê³¼ ì‚¬ìš©
        scores = qa_scores
        st.success("âœ… ê°œë³„ í‰ê°€ ì ìˆ˜ ì‚¬ìš©")
    else:
        # í‰ê°€ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ì „ì²´ í‰ê·  ì‚¬ìš©
        st.warning("âš ï¸ ê°œë³„ í‰ê°€ ì ìˆ˜ê°€ ì—†ì–´ ì „ì²´ í‰ê·  ì ìˆ˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
        latest_results, _ = load_latest_evaluation_results()
        if latest_results:
            scores = {
                'faithfulness': latest_results.get('faithfulness', 0),
                'answer_relevancy': latest_results.get('answer_relevancy', 0),
                'context_recall': latest_results.get('context_recall', 0),
                'context_precision': latest_results.get('context_precision', 0)
            }
        else:
            st.error("âŒ í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    # ì ìˆ˜ ì¹´ë“œ í‘œì‹œ
    if scores:
        score_cols = st.columns(4)
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        
        for i, metric in enumerate(metrics):
            with score_cols[i]:
                score = scores.get(metric, 0)
                color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
                st.metric(
                    label=metric.replace('_', ' ').title(),
                    value=f"{score:.3f}"
                )
        
        # ì ìˆ˜ ì‹œê°í™”
        show_qa_score_chart_new(scores, qa_number)
        
        # í‰ê°€ ê·¼ê±°
        show_evaluation_reasoning_new(qa_data, qa_number, scores)


def show_qa_score_chart_new(scores, qa_number):
    """ê°œë³„ QA ì ìˆ˜ ì°¨íŠ¸ - ì¬ì‘ì„±"""
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ì‹œê°í™”")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°” ì°¨íŠ¸
        metrics = list(scores.keys())
        values = list(scores.values())
        
        fig = go.Figure(data=[
            go.Bar(
                x=metrics, 
                y=values, 
                marker_color=['green' if v >= 0.8 else 'orange' if v >= 0.6 else 'red' for v in values],
                text=[f"{v:.3f}" for v in values],
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ì ìˆ˜",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ë ˆì´ë” ì°¨íŠ¸
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values + [values[0]],  # ì°¨íŠ¸ë¥¼ ë‹«ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ê°’ ì¶”ê°€
            theta=metrics + [metrics[0]],
            fill='toself',
            name=f'QA {qa_number}',
            line_color='rgb(32, 201, 151)'
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ê· í˜•ë„",
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)


def show_evaluation_reasoning_new(qa_data, qa_number, scores):
    """í‰ê°€ ê·¼ê±° í‘œì‹œ - ì¬ì‘ì„±"""
    st.markdown("#### ğŸ§  í‰ê°€ ê·¼ê±°")
    
    # ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°
    question = qa_data['question']
    answer = qa_data['answer']
    contexts = qa_data['contexts']
    ground_truth = qa_data['ground_truth']
    
    # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„
    metrics_analysis = {
        'faithfulness': {
            'description': 'ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •',
            'score': scores.get('faithfulness', 0),
            'analysis': generate_faithfulness_analysis(answer, contexts, scores.get('faithfulness', 0))
        },
        'answer_relevancy': {
            'description': 'ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •',
            'score': scores.get('answer_relevancy', 0),
            'analysis': generate_relevancy_analysis(question, answer, scores.get('answer_relevancy', 0))
        },
        'context_recall': {
            'description': 'Ground truthì˜ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ ë°œê²¬ë˜ëŠ”ì§€ ì¸¡ì •',
            'score': scores.get('context_recall', 0),
            'analysis': generate_recall_analysis(ground_truth, contexts, scores.get('context_recall', 0))
        },
        'context_precision': {
            'description': 'ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •',
            'score': scores.get('context_precision', 0),
            'analysis': generate_precision_analysis(question, contexts, scores.get('context_precision', 0))
        }
    }
    
    for metric, info in metrics_analysis.items():
        with st.expander(f"ğŸ“ {metric.replace('_', ' ').title()} ë¶„ì„ (ì ìˆ˜: {info['score']:.3f})"):
            st.markdown(f"**ì„¤ëª…:** {info['description']}")
            st.markdown(f"**ë¶„ì„:** {info['analysis']}")


def generate_faithfulness_analysis(answer, contexts, score):
    """Faithfulness ë¶„ì„ ìƒì„±"""
    context_text = " ".join(contexts)
    if score >= 0.8:
        return f"ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì˜ ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€ì´ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"ë‹µë³€ì˜ ì¼ë¶€ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ë©ë‹ˆë‹¤. ì¼ë¶€ ë‚´ìš©ì€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ë’·ë°›ì¹¨ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶©ë¶„íˆ ë’·ë°›ì¹¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ê°€ í¬í•¨ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."


def generate_relevancy_analysis(question, answer, score):
    """Answer Relevancy ë¶„ì„ ìƒì„±"""
    if score >= 0.8:
        return f"ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì˜ íŒŒì•…í•˜ê³  ì ì ˆí•˜ê²Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì§€ë§Œ, ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆê±°ë‚˜ í•µì‹¬ì„ ì™„ì „íˆ ë‹¤ë£¨ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ì™€ ë§ì§€ ì•Šê±°ë‚˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë¶„ì„í•˜ì—¬ ë” ì§ì ‘ì ì¸ ë‹µë³€ì´ í•„ìš”í•©ë‹ˆë‹¤."


def generate_recall_analysis(ground_truth, contexts, score):
    """Context Recall ë¶„ì„ ìƒì„±"""
    if score >= 0.8:
        return f"Ground truthì˜ í•µì‹¬ ì •ë³´ê°€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ ë°œê²¬ë©ë‹ˆë‹¤. í•„ìš”í•œ ì •ë³´ ê²€ìƒ‰ì´ ì¶©ë¶„íˆ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"Ground truthì˜ ì¼ë¶€ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"Ground truthì˜ ì¤‘ìš”í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ë²”ìœ„ë‚˜ ì „ëµì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤."


def generate_precision_analysis(question, contexts, score):
    """Context Precision ë¶„ì„ ìƒì„±"""
    if score >= 0.8:
        return f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ì ê³  íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ìˆì§€ë§Œ, ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ê°€ ë§ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤."


def show_metric_distribution_new(evaluation_data, latest_results, individual_scores):
    """ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„ - ì¬ì‘ì„±"""
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„")
    
    if not individual_scores:
        st.warning("ğŸ“Š ê°œë³„ QA ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í‰ê°€ ê²°ê³¼ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
        
        # ì „ì²´ ê²°ê³¼ í‘œì‹œ
        if latest_results:
            metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
            col1, col2, col3, col4 = st.columns(4)
            
            for i, metric in enumerate(metrics):
                with [col1, col2, col3, col4][i]:
                    score = latest_results.get(metric, 0)
                    st.metric(
                        label=metric.replace('_', ' ').title(),
                        value=f"{score:.3f}"
                    )
        return
    
    # ê°œë³„ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ë¶„í¬ ë¶„ì„
    num_qa = len(evaluation_data)
    num_scores = len(individual_scores)
    
    st.info(f"ğŸ“Š QA ê°œìˆ˜: {num_qa}, ê°œë³„ ì ìˆ˜ ê°œìˆ˜: {num_scores}")
    
    # ì•ˆì „í•œ ê¸¸ì´ë¡œ ì¡°ì •
    safe_length = min(num_qa, num_scores)
    
    if safe_length == 0:
        st.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # DataFrame ìƒì„±
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    data = {'QA': [f'Q{i+1}' for i in range(safe_length)]}
    
    for metric in metrics:
        data[metric] = []
        for i in range(safe_length):
            if i < len(individual_scores) and individual_scores[i]:
                score = individual_scores[i].get(metric, 0)
            else:
                score = 0
            data[metric].append(score)
    
    df = pd.DataFrame(data)
    
    # íˆíŠ¸ë§µ
    st.markdown("#### ğŸ”¥ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ")
    
    heatmap_data = df[metrics].values
    
    fig = go.Figure(data=go.Heatmap(
        z=heatmap_data,
        x=[m.replace('_', ' ').title() for m in metrics],
        y=df['QA'],
        colorscale='RdYlGn',
        colorbar=dict(title="ì ìˆ˜")
    ))
    
    fig.update_layout(
        title="QAë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ",
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # ë¶„í¬ í†µê³„
    st.markdown("#### ğŸ“ˆ ë¶„í¬ í†µê³„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**í‰ê·  ì ìˆ˜**")
        for metric in metrics:
            avg_score = df[metric].mean()
            st.text(f"{metric.replace('_', ' ').title()}: {avg_score:.3f}")
    
    with col2:
        st.markdown("**í‘œì¤€í¸ì°¨**")
        for metric in metrics:
            std_score = df[metric].std()
            st.text(f"{metric.replace('_', ' ').title()}: {std_score:.3f}")


def show_pattern_analysis_new(evaluation_data, latest_results, individual_scores):
    """íŒ¨í„´ ë¶„ì„ - ì¬ì‘ì„±"""
    st.subheader("ğŸ¯ ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„")
    
    # ê°„ë‹¨í•œ í†µê³„ ë¶„ì„
    qa_count = len(evaluation_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“ ë°ì´í„°ì…‹ ê°œìš”")
        st.metric("ì´ QA ê°œìˆ˜", qa_count)
        
        # ì§ˆë¬¸ ê¸¸ì´ ë¶„ì„
        question_lengths = [len(qa['question'].split()) for qa in evaluation_data]
        avg_q_length = sum(question_lengths) / len(question_lengths)
        st.metric("í‰ê·  ì§ˆë¬¸ ê¸¸ì´", f"{avg_q_length:.1f} ë‹¨ì–´")
        
        # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ ë¶„ì„
        context_counts = [len(qa['contexts']) for qa in evaluation_data]
        avg_context_count = sum(context_counts) / len(context_counts)
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜", f"{avg_context_count:.1f}ê°œ")
    
    with col2:
        st.markdown("#### ğŸ“Š ì„±ëŠ¥ ìš”ì•½")
        if latest_results:
            st.metric("ì „ì²´ RAGAS ì ìˆ˜", f"{latest_results.get('ragas_score', 0):.3f}")
            
            # ìµœê³ /ìµœì € ë©”íŠ¸ë¦­
            metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
            scores = {m: latest_results.get(m, 0) for m in metrics}
            
            best_metric = max(scores, key=scores.get)
            worst_metric = min(scores, key=scores.get)
            
            st.text(f"ìµœê³  ì„±ëŠ¥: {best_metric.replace('_', ' ').title()} ({scores[best_metric]:.3f})")
            st.text(f"ê°œì„  í•„ìš”: {worst_metric.replace('_', ' ').title()} ({scores[worst_metric]:.3f})")
    
    # ê°œì„  ì œì•ˆ
    st.markdown("#### ğŸ’¡ ê°œì„  ì œì•ˆ")
    
    if latest_results:
        suggestions = []
        
        if latest_results.get('faithfulness', 0) < 0.7:
            suggestions.append("ğŸ¯ Faithfulness ê°œì„ : ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„ ê°•í™”, í™˜ê° ë°©ì§€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
        
        if latest_results.get('answer_relevancy', 0) < 0.7:
            suggestions.append("ğŸ¯ Answer Relevancy ê°œì„ : ì§ˆë¬¸ ì˜ë„ íŒŒì•… ê°•í™”, ê°„ê²°í•œ ë‹µë³€ ìƒì„±")
        
        if latest_results.get('context_recall', 0) < 0.7:
            suggestions.append("ğŸ¯ Context Recall ê°œì„ : ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€, ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ í™œìš©")
        
        if latest_results.get('context_precision', 0) < 0.7:
            suggestions.append("ğŸ¯ Context Precision ê°œì„ : ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§, ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ")
        
        if not suggestions:
            suggestions.append("âœ… ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì–‘í˜¸í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤!")
        
        for suggestion in suggestions:
            st.info(suggestion)