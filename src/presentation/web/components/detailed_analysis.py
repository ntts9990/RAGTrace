"""
ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸ - ì‹¤ì œ í‰ê°€ ë°ì´í„° ê¸°ë°˜
ì‹¤ì œë¡œ í‰ê°€ëœ QA ë°ì´í„°ë§Œ í‘œì‹œí•˜ê³  Historical í˜ì´ì§€ì™€ ì—°ë™
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
import sqlite3
from pathlib import Path
from datetime import datetime


def get_db_path():
    """ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ data/db/evaluations.dbë¡œ ê²½ë¡œ ìˆ˜ì •
    project_root = Path(__file__).parent.parent.parent.parent.parent
    return project_root / "data" / "db" / "evaluations.db"


def load_all_evaluations():
    """ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¡œë“œ (Historical í˜ì´ì§€ ì—°ë™ìš©)"""
    try:
        db_path = get_db_path()
        if not db_path.exists():
            return []
        
        conn = sqlite3.connect(str(db_path))
        
        query = '''
            SELECT id, timestamp, faithfulness, answer_relevancy, 
                   context_recall, context_precision, ragas_score, raw_data
            FROM evaluations 
            ORDER BY timestamp DESC
        '''
        
        cursor = conn.execute(query)
        results = cursor.fetchall()
        conn.close()
        
        evaluations = []
        for row in results:
            evaluation = {
                'id': row[0],
                'timestamp': row[1],
                'faithfulness': row[2],
                'answer_relevancy': row[3],
                'context_recall': row[4],
                'context_precision': row[5],
                'ragas_score': row[6],
                'raw_data': json.loads(row[7]) if row[7] else None
            }
            evaluations.append(evaluation)
        
        return evaluations
        
    except Exception as e:
        st.error(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def load_evaluation_by_id(evaluation_id):
    """íŠ¹ì • í‰ê°€ IDë¡œ í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    try:
        db_path = get_db_path()
        if not db_path.exists():
            return None, []
        
        conn = sqlite3.connect(str(db_path))
        
        query = '''
            SELECT raw_data 
            FROM evaluations 
            WHERE id = ?
        '''
        
        result = conn.execute(query, (evaluation_id,)).fetchone()
        conn.close()
        
        if result and result[0]:
            raw_data = json.loads(result[0])
            individual_scores = raw_data.get('individual_scores', [])
            return raw_data, individual_scores
        
        return None, []
        
    except Exception as e:
        st.error(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, []


def load_actual_qa_data_from_dataset_simple(dataset_name, qa_count):
    """ê°„ë‹¨í•œ ë²„ì „ - ì§ì ‘ íŒŒì¼ ë¡œë“œ"""
    try:
        # í•˜ë“œì½”ë”©ëœ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        if "variant1" in dataset_name:
            path = "/Users/isle/PycharmProjects/ragas-test/data/evaluation_data_variant1.json"
        else:
            path = "/Users/isle/PycharmProjects/ragas-test/data/evaluation_data.json"
        
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data[:qa_count]
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_actual_qa_data_from_dataset(dataset_name, qa_count):
    """ë°ì´í„°ì…‹ íŒŒì¼ì—ì„œ ì‹¤ì œ QA ë°ì´í„° ë¡œë“œ"""
    import os
    
    # ë””ë²„ê·¸: í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ í™•ì¸
    current_file = Path(__file__).resolve()
    print(f"[DEBUG] Current file: {current_file}")
    
    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ project root ì°¾ê¸°
    # ë°©ë²• 1: í˜„ì¬ íŒŒì¼ì—ì„œ ìƒëŒ€ ê²½ë¡œ
    project_root = current_file.parent.parent.parent.parent
    
    # ë°©ë²• 2: cwdì—ì„œ ì°¾ê¸°  
    cwd = Path.cwd()
    if 'ragas-test' in cwd.parts:
        # cwdê°€ ragas-test ë‚´ë¶€ì— ìˆìœ¼ë©´
        idx = cwd.parts.index('ragas-test')
        project_root_alt = Path(*cwd.parts[:idx+1])
    else:
        project_root_alt = cwd
    
    # ë°©ë²• 3: ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© (í•˜ë“œì½”ë”©)
    absolute_data_paths = [
        Path("/Users/isle/PycharmProjects/ragas-test/data") / dataset_name,
        Path("/Users/isle/PycharmProjects/ragas-test/data/evaluation_data.json"),
        Path("/Users/isle/PycharmProjects/ragas-test/data/evaluation_data_variant1.json")
    ]
    
    print(f"[DEBUG] Project root (method 1): {project_root}")
    print(f"[DEBUG] Project root (method 2): {project_root_alt}")
    print(f"[DEBUG] Current working directory: {cwd}")
    
    # ëª¨ë“  ê°€ëŠ¥í•œ ê²½ë¡œ ì¡°í•©
    all_possible_paths = []
    
    # ê° project root ë°©ë²•ì— ëŒ€í•´
    for root in [project_root, project_root_alt]:
        all_possible_paths.extend([
            root / "data" / dataset_name,
            root / "data" / "evaluation_data.json",
            root / "data" / "evaluation_data_variant1.json"
        ])
    
    # ì ˆëŒ€ ê²½ë¡œ ì¶”ê°€
    all_possible_paths.extend(absolute_data_paths)
    
    # ì¤‘ë³µ ì œê±°
    unique_paths = list(dict.fromkeys(all_possible_paths))
    
    print(f"[DEBUG] Looking for dataset: {dataset_name}")
    print(f"[DEBUG] QA count requested: {qa_count}")
    print(f"[DEBUG] Checking {len(unique_paths)} unique paths")
    
    # ëª¨ë“  ê²½ë¡œ ì‹œë„
    for i, path in enumerate(unique_paths):
        print(f"[DEBUG] Checking path {i+1}: {path}")
        
        try:
            if path.exists() and path.is_file():
                print(f"[DEBUG] Found file at: {path}")
                with open(path, "r", encoding="utf-8") as f:
                    all_qa_data = json.load(f)
                
                print(f"[DEBUG] Successfully loaded JSON from {path}")
                print(f"[DEBUG] Total QA items in file: {len(all_qa_data) if isinstance(all_qa_data, list) else 'Not a list'}")
                
                if isinstance(all_qa_data, list) and len(all_qa_data) > 0:
                    # qa_countë§Œí¼ë§Œ ë°˜í™˜ (ì‹¤ì œ í‰ê°€ëœ ê°œìˆ˜)
                    result = all_qa_data[:qa_count]
                    print(f"[DEBUG] Returning {len(result)} QA items")
                    print(f"[DEBUG] First QA item preview: {result[0].get('question', 'No question')[:50] if result else 'No data'}")
                    return result
                else:
                    print(f"[DEBUG] File loaded but invalid format or empty")
                    
        except json.JSONDecodeError as e:
            print(f"[DEBUG] JSON decode error for {path}: {e}")
        except Exception as e:
            print(f"[DEBUG] Error with {path}: {type(e).__name__}: {e}")
    
    # ëª¨ë“  ê²½ë¡œì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°
    print("[DEBUG] Failed to load QA data from any path")
    print(f"[DEBUG] Final attempt: listing files in likely directories...")
    
    # ë§ˆì§€ë§‰ ì‹œë„: ê°€ëŠ¥í•œ data ë””ë ‰í† ë¦¬ ë‚´ìš© í‘œì‹œ
    for root in [project_root, project_root_alt, Path("/Users/isle/PycharmProjects/ragas-test")]:
        data_dir = root / "data"
        if data_dir.exists():
            print(f"[DEBUG] Found data dir at: {data_dir}")
            print(f"[DEBUG] Contents: {list(data_dir.iterdir())}")
    
    return None


def get_actual_qa_data_from_evaluation(raw_data, evaluation_db_id):
    """í‰ê°€ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ QA ë°ì´í„° ì¶”ì¶œ"""
    if not raw_data:
        return None
    
    # raw_dataì—ì„œ ì‹¤ì œ í‰ê°€ì— ì‚¬ìš©ëœ QA ë°ì´í„° ì°¾ê¸°
    metadata = raw_data.get('metadata', {})
    
    # individual_scoresì˜ ê°œìˆ˜ê°€ ì‹¤ì œ í‰ê°€ëœ QA ê°œìˆ˜
    individual_scores = raw_data.get('individual_scores', [])
    actual_qa_count = len(individual_scores)
    
    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ, ì—†ìœ¼ë©´ DB ID ì‚¬ìš©
    evaluation_id = metadata.get('evaluation_id', f"DB#{evaluation_db_id}")
    model_info = metadata.get('model', 'Gemini-2.5-Flash')
    dataset_info = metadata.get('dataset', 'evaluation_data.json')
    
    # ë°ì´í„°ì…‹ì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    if '/' in dataset_info:
        dataset_name = dataset_info.split('/')[-1]
    else:
        dataset_name = dataset_info
    
    # ì‹¤ì œ QA ë°ì´í„° ë¡œë“œ - ê°„ë‹¨í•œ ë²„ì „ ì‚¬ìš©
    actual_qa_data = load_actual_qa_data_from_dataset_simple(dataset_name, actual_qa_count)
    
    return {
        'qa_count': actual_qa_count,
        'dataset_size': metadata.get('dataset_size', actual_qa_count),
        'evaluation_id': evaluation_id,
        'timestamp': metadata.get('timestamp', 'unknown'),
        'model': model_info,
        'dataset': dataset_name,
        'qa_data': actual_qa_data
    }


def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ë©”ì¸ í™”ë©´ - ì‹¤ì œ í‰ê°€ ë°ì´í„° ê¸°ë°˜"""
    st.header("ğŸ” ìƒì„¸ ë¶„ì„")
    
    # í‰ê°€ ì„ íƒ ì„¹ì…˜
    st.subheader("ğŸ“‹ í‰ê°€ ì„ íƒ")
    
    # ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¡œë“œ
    all_evaluations = load_all_evaluations()
    
    if not all_evaluations:
        st.error("âŒ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ Overview í˜ì´ì§€ì—ì„œ 'ìƒˆ í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        return
    
    # í‰ê°€ ì„ íƒ ì˜µì…˜ ìƒì„±
    evaluation_options = []
    for i, eval_data in enumerate(all_evaluations):
        timestamp = eval_data['timestamp']
        qa_count = 0
        if eval_data['raw_data'] and eval_data['raw_data'].get('individual_scores'):
            qa_count = len(eval_data['raw_data']['individual_scores'])
        
        # timestampë¥¼ ë” ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
        try:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            formatted_time = dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            formatted_time = timestamp
        
        option_text = f"í‰ê°€ #{eval_data['id']} - {formatted_time} ({qa_count}ê°œ QA)"
        evaluation_options.append(option_text)
    
    # ì„¸ì…˜ ìƒíƒœë¡œ ì„ íƒëœ í‰ê°€ ê´€ë¦¬
    if "selected_evaluation_index" not in st.session_state:
        st.session_state.selected_evaluation_index = 0
    
    selected_eval_idx = st.selectbox(
        "ë¶„ì„í•  í‰ê°€ ì„ íƒ",
        range(len(evaluation_options)),
        format_func=lambda x: evaluation_options[x],
        index=st.session_state.selected_evaluation_index,
        key="evaluation_selector"
    )
    
    # ì„ íƒëœ í‰ê°€ ë°ì´í„° ë¡œë“œ
    selected_evaluation = all_evaluations[selected_eval_idx]
    evaluation_id = selected_evaluation['id']
    
    # ì„ íƒëœ í‰ê°€ì˜ ìƒì„¸ ë°ì´í„° ë¡œë“œ
    raw_data, individual_scores = load_evaluation_by_id(evaluation_id)
    
    if not raw_data:
        st.error(f"âŒ í‰ê°€ ID {evaluation_id}ì˜ ìƒì„¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤ì œ í‰ê°€ëœ QA ë°ì´í„° ì •ë³´
    qa_info = get_actual_qa_data_from_evaluation(raw_data, evaluation_id)
    
    if not qa_info or qa_info['qa_count'] == 0:
        st.error("âŒ ì´ í‰ê°€ì—ëŠ” ê°œë³„ QA ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í‰ê°€ ì •ë³´ í‘œì‹œ
    st.success(f"âœ… í‰ê°€ #{evaluation_id} ë¡œë“œ ì™„ë£Œ")
    
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("QA ê°œìˆ˜", qa_info['qa_count'])
    with col2:
        st.metric("í‰ê°€ ID", qa_info['evaluation_id'])
    with col3:
        st.metric("ëª¨ë¸", qa_info['model'])
    with col4:
        st.metric("ë°ì´í„°ì…‹", qa_info['dataset'])
    with col5:
        ragas_score = selected_evaluation.get('ragas_score', 0)
        st.metric("RAGAS ì ìˆ˜", f"{ragas_score:.3f}")
    
    # ê°œë³„ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¶„ì„ ì§„í–‰
    if not individual_scores:
        st.warning("âš ï¸ ì´ í‰ê°€ì—ëŠ” ê°œë³„ QA ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        show_overall_metrics_only(selected_evaluation)
        return
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š QA ê°œë³„ ë¶„ì„", "ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„í¬", "ğŸ¯ íŒ¨í„´ ë¶„ì„"])
    
    with tab1:
        # ë””ë²„ê·¸: qa_info ìƒíƒœ í™•ì¸
        if qa_info and 'qa_data' in qa_info:
            if qa_info['qa_data']:
                st.info(f"ğŸ“Š QA ë°ì´í„° ìƒíƒœ: ë¡œë“œë¨ ({len(qa_info['qa_data'])}ê°œ)")
            else:
                st.warning("ğŸ“Š QA ë°ì´í„° ìƒíƒœ: ë¹„ì–´ìˆìŒ")
                # í„°ë¯¸ë„ ì¶œë ¥ í™•ì¸ ì•ˆë‚´
                st.info("ğŸ’¡ í„°ë¯¸ë„ì—ì„œ ë””ë²„ê·¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        show_qa_analysis_actual(individual_scores, evaluation_id, qa_info.get('qa_data'))
    
    with tab2:
        show_metric_distribution_actual(individual_scores, selected_evaluation)
    
    with tab3:
        show_pattern_analysis_actual(individual_scores, selected_evaluation)


def show_overall_metrics_only(evaluation_data):
    """ê°œë³„ ì ìˆ˜ê°€ ì—†ì„ ë•Œ ì „ì²´ ë©”íŠ¸ë¦­ë§Œ í‘œì‹œ"""
    st.subheader("ğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼")
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    col1, col2, col3, col4 = st.columns(4)
    
    for i, metric in enumerate(metrics):
        with [col1, col2, col3, col4][i]:
            score = evaluation_data.get(metric, 0)
            st.metric(
                label=metric.replace('_', ' ').title(),
                value=f"{score:.3f}"
            )


def show_qa_analysis_actual(individual_scores, evaluation_id, qa_data=None):
    """ì‹¤ì œ í‰ê°€ëœ QA ê°œë³„ ë¶„ì„"""
    st.subheader("ğŸ“‹ QA ê°œë³„ ë¶„ì„")
    
    qa_count = len(individual_scores)
    
    if qa_count == 0:
        st.warning("ë¶„ì„í•  QA ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë””ë²„ê·¸: qa_data ìƒíƒœ í™•ì¸
    if qa_data is None:
        st.error("âš ï¸ QA ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        with st.expander("ğŸ” ë¬¸ì œ í•´ê²° ë°©ë²•"):
            st.markdown("""
            **ê°€ëŠ¥í•œ ì›ì¸:**
            1. í‰ê°€ ë°ì´í„° íŒŒì¼ì´ `/Users/isle/PycharmProjects/ragas-test/data/` ê²½ë¡œì— ì—†ìŒ
            2. íŒŒì¼ ì´ë¦„ì´ `evaluation_data.json` ë˜ëŠ” `evaluation_data_variant1.json`ì´ ì•„ë‹˜
            3. íŒŒì¼ ê¶Œí•œ ë¬¸ì œ
            
            **í•´ê²° ë°©ë²•:**
            - í„°ë¯¸ë„ì—ì„œ `ls -la /Users/isle/PycharmProjects/ragas-test/data/` ëª…ë ¹ìœ¼ë¡œ íŒŒì¼ í™•ì¸
            - í•„ìš”í•œ ê²½ìš° íŒŒì¼ ê¶Œí•œ ìˆ˜ì •: `chmod 644 /Users/isle/PycharmProjects/ragas-test/data/*.json`
            - í„°ë¯¸ë„ì—ì„œ [DEBUG] ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì •í™•í•œ ì˜¤ë¥˜ ìœ„ì¹˜ íŒŒì•…
            """)
    elif len(qa_data) == 0:
        st.error("âš ï¸ QA ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    else:
        st.success(f"âœ… {len(qa_data)}ê°œì˜ QA ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # QA ì„ íƒ ì˜µì…˜ ìƒì„± (ì‹¤ì œ ì ìˆ˜ì™€ ì§ˆë¬¸ ë‚´ìš© ê¸°ë°˜)
    qa_options = []
    for i, qa_score in enumerate(individual_scores):
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = 0
        if qa_score:
            avg_score = sum(qa_score.values()) / len(qa_score) if qa_score.values() else 0
        
        # ì§ˆë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
        question_preview = "ì§ˆë¬¸ ì •ë³´ ì—†ìŒ"
        if qa_data and i < len(qa_data):
            question = qa_data[i].get('question', '')
            if question:
                # ì§ˆë¬¸ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì²˜ë¦¬
                if len(question) > 50:
                    question_preview = question[:47] + "..."
                else:
                    question_preview = question
        
        qa_options.append(f"QA #{i+1}: {question_preview} (í‰ê· : {avg_score:.3f})")
    
    selected_qa_idx = st.selectbox(
        "ë¶„ì„í•  QA ì„ íƒ", 
        range(len(qa_options)), 
        format_func=lambda x: qa_options[x]
    )
    
    if selected_qa_idx is not None and selected_qa_idx < len(individual_scores):
        qa_scores = individual_scores[selected_qa_idx]
        qa_content = qa_data[selected_qa_idx] if qa_data and selected_qa_idx < len(qa_data) else None
        show_individual_qa_details_actual(selected_qa_idx + 1, qa_scores, evaluation_id, qa_content)


def show_individual_qa_details_actual(qa_number, qa_scores, evaluation_id, qa_content=None):
    """ì‹¤ì œ í‰ê°€ëœ ê°œë³„ QA ìƒì„¸ ì •ë³´ í‘œì‹œ"""
    st.markdown(f"### ğŸ“ QA {qa_number} ìƒì„¸ ë¶„ì„ (í‰ê°€ #{evaluation_id})")
    
    if not qa_scores:
        st.error("âŒ ì´ QAì— ëŒ€í•œ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # QA ë‚´ìš© í‘œì‹œ (ì‹¤ì œ ì§ˆë¬¸, ë‹µë³€, ì»¨í…ìŠ¤íŠ¸)
    if qa_content:
        st.markdown("#### ğŸ“‹ QA ë‚´ìš©")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ğŸ¤” ì§ˆë¬¸:**")
            st.info(qa_content.get('question', 'ì§ˆë¬¸ ì •ë³´ ì—†ìŒ'))
            
            st.markdown("**ğŸ’¡ ìƒì„±ëœ ë‹µë³€:**")
            st.success(qa_content.get('answer', 'ë‹µë³€ ì •ë³´ ì—†ìŒ'))
        
        with col2:
            st.markdown("**ğŸ“š ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:**")
            contexts = qa_content.get('contexts', [])
            for i, context in enumerate(contexts, 1):
                with st.expander(f"ì»¨í…ìŠ¤íŠ¸ {i}"):
                    st.text(context)
            
            st.markdown("**âœ… ì •ë‹µ (Ground Truth):**")
            st.info(qa_content.get('ground_truth', 'ì •ë‹µ ì •ë³´ ì—†ìŒ'))
        
        st.markdown("---")
    
    # ì ìˆ˜ ì¹´ë“œ í‘œì‹œ
    st.markdown("#### ğŸ“Š í‰ê°€ ì ìˆ˜")
    
    score_cols = st.columns(4)
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    
    for i, metric in enumerate(metrics):
        with score_cols[i]:
            score = qa_scores.get(metric, 0)
            color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
            st.metric(
                label=metric.replace('_', ' ').title(),
                value=f"{score:.3f}"
            )
    
    # ì ìˆ˜ ì‹œê°í™”
    show_qa_score_chart_actual(qa_scores, qa_number)
    
    # í‰ê°€ ê·¼ê±° (ì ìˆ˜ ê¸°ë°˜)
    show_evaluation_reasoning_actual(qa_number, qa_scores, qa_content)


def show_qa_score_chart_actual(scores, qa_number):
    """ì‹¤ì œ í‰ê°€ëœ QA ì ìˆ˜ ì°¨íŠ¸"""
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ì‹œê°í™”")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°” ì°¨íŠ¸
        metrics = list(scores.keys())
        values = list(scores.values())
        
        fig = go.Figure(data=[
            go.Bar(
                x=metrics, 
                y=values, 
                marker_color=['green' if v >= 0.8 else 'orange' if v >= 0.6 else 'red' for v in values],
                text=[f"{v:.3f}" for v in values],
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ì ìˆ˜",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ë ˆì´ë” ì°¨íŠ¸
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values + [values[0]],  # ì°¨íŠ¸ë¥¼ ë‹«ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ê°’ ì¶”ê°€
            theta=metrics + [metrics[0]],
            fill='toself',
            name=f'QA {qa_number}',
            line_color='rgb(32, 201, 151)'
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ê· í˜•ë„",
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)


def show_evaluation_reasoning_actual(qa_number, scores, qa_content=None):
    """ì‹¤ì œ í‰ê°€ ì ìˆ˜ ê¸°ë°˜ í‰ê°€ ê·¼ê±°"""
    st.markdown("#### ğŸ§  í‰ê°€ ê·¼ê±°")
    
    # QA ë‚´ìš© ìš”ì•½ í‘œì‹œ (í‰ê°€ ê·¼ê±°ì—ì„œ ì°¸ê³ ìš©)
    if qa_content:
        st.info(f"**ì°¸ê³ :** ì´ ë¶„ì„ì€ '{qa_content.get('question', '')[:50]}...' ì§ˆë¬¸ì— ëŒ€í•œ í‰ê°€ì…ë‹ˆë‹¤.")
    
    # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„
    metrics_analysis = {
        'faithfulness': {
            'description': 'ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •',
            'score': scores.get('faithfulness', 0),
            'analysis': generate_faithfulness_analysis_actual(scores.get('faithfulness', 0))
        },
        'answer_relevancy': {
            'description': 'ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •',
            'score': scores.get('answer_relevancy', 0),
            'analysis': generate_relevancy_analysis_actual(scores.get('answer_relevancy', 0))
        },
        'context_recall': {
            'description': 'Ground truthì˜ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ ë°œê²¬ë˜ëŠ”ì§€ ì¸¡ì •',
            'score': scores.get('context_recall', 0),
            'analysis': generate_recall_analysis_actual(scores.get('context_recall', 0))
        },
        'context_precision': {
            'description': 'ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •',
            'score': scores.get('context_precision', 0),
            'analysis': generate_precision_analysis_actual(scores.get('context_precision', 0))
        }
    }
    
    for metric, info in metrics_analysis.items():
        with st.expander(f"ğŸ“ {metric.replace('_', ' ').title()} ë¶„ì„ (ì ìˆ˜: {info['score']:.3f})"):
            st.markdown(f"**ì„¤ëª…:** {info['description']}")
            
            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ì„ ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ í‘œì‹œ
            analysis_lines = info['analysis'].split('\n')
            for line in analysis_lines:
                if line.strip():
                    st.markdown(line)
            
            # ì ìˆ˜ êµ¬ê°„ë³„ í•´ì„ ê°€ì´ë“œ
            st.markdown("---")
            st.markdown("**ì ìˆ˜ í•´ì„:**")
            if info['score'] >= 0.9:
                st.success("ğŸŒŸ ìš°ìˆ˜ (0.9+): ë§¤ìš° ë†’ì€ ì„±ëŠ¥")
            elif info['score'] >= 0.8:
                st.success("âœ… ì–‘í˜¸ (0.8-0.9): ì¢‹ì€ ì„±ëŠ¥")
            elif info['score'] >= 0.6:
                st.warning("âš ï¸ ë³´í†µ (0.6-0.8): ê°œì„  ì—¬ì§€ ìˆìŒ")
            else:
                st.error("âŒ ê°œì„ í•„ìš” (<0.6): ì¦‰ì‹œ ê°œì„  í•„ìš”")


def generate_faithfulness_analysis_actual(score):
    """Faithfulness ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""
    
    if score >= 0.9:
        base_analysis = """
        **ğŸŒŸ íƒì›”í•œ ì¶©ì‹¤ë„ (0.9+)**
        - ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë§¤ìš° ì¶©ì‹¤í•˜ê²Œ ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤
        - LLMì´ í™˜ê°(Hallucination) ì—†ì´ ì •í™•í•œ ì •ë³´ë§Œì„ í™œìš©í–ˆìŠµë‹ˆë‹¤
        - ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ê°€ëŠ¥í•œ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”. ì´ ì •ë„ ì¶©ì‹¤ë„ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì´ìƒì ì…ë‹ˆë‹¤."
        technical_details = "ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©ê³¼ ë‹µë³€ ê°„ ì¼ì¹˜ë„ê°€ 90% ì´ìƒìœ¼ë¡œ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì…ë‹ˆë‹¤."
        
    elif score >= 0.8:
        base_analysis = """
        **âœ… ìš°ìˆ˜í•œ ì¶©ì‹¤ë„ (0.8-0.9)**
        - ë‹µë³€ì˜ ëŒ€ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤
        - ì†Œìˆ˜ì˜ ì¶”ë¡ ì´ë‚˜ ì¼ë°˜í™”ê°€ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆì§€ë§Œ ì ì ˆí•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤
        - ì „ë°˜ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ê°œì„  ë°©ì•ˆ:**
        - í”„ë¡¬í”„íŠ¸ì— "ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬" ê°™ì€ ì œì•½ ì¡°ê±´ ì¶”ê°€
        - ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ë„ë¡ ìœ ë„
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì†Œìˆ˜ì˜ ì¶”ë¡  í¬í•¨ë˜ì—ˆì§€ë§Œ í—ˆìš© ë²”ìœ„ ë‚´ì…ë‹ˆë‹¤."
        
    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ì¶©ì‹¤ë„ (0.6-0.8)**
        - ë‹µë³€ì˜ ì¼ë¶€ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í™•ì¸ë©ë‹ˆë‹¤
        - ì¼ë¶€ ë‚´ìš©ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë„˜ì–´ì„  ì¶”ë¡ ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - ê²€ì¦ì´ í•„ìš”í•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ì¦‰ì‹œ ê°œì„  í•„ìš”:**
        - í”„ë¡¬í”„íŠ¸ì— "ì˜¤ì§ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©" ëª…ì‹œ
        - Temperature ê°’ì„ ë‚®ì¶° ë” ë³´ìˆ˜ì ì¸ ë‹µë³€ ìœ ë„
        - ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì •ë³´ ì‚¬ìš© ì‹œ ëª…ì‹œí•˜ë„ë¡ ì§€ì‹œ
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì •ë³´ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."
        
    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ì¶©ì‹¤ë„ (0.4-0.6)**
        - ë‹µë³€ì˜ ìƒë‹¹ ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë’·ë°›ì¹¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - í™˜ê°ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ì— ì˜ì¡´í•œ ë‚´ìš©ì´ ë§ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - ë‹µë³€ì˜ ì‹ ë¢°ì„±ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ê¸´ê¸‰ ìˆ˜ì • í•„ìš”:**
        - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì „ë©´ ì¬ê²€í† 
        - "ì ˆëŒ€ ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì •ë³´ ì‚¬ìš©í•˜ì§€ ë§ˆì‹œì˜¤" ëª…ì‹œ
        - RAG íŒŒì´í”„ë¼ì¸ì˜ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ê²€
        - ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì • (Top-p, Temperature ë“±)
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ê°€ ì ì¬ì  í™˜ê° ë˜ëŠ” ì™¸ë¶€ ì§€ì‹ì…ë‹ˆë‹¤."
        
    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ì¶©ì‹¤ë„ (<0.4)**
        - ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì™€ ê±°ì˜ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤
        - ì‹¬ê°í•œ í™˜ê° í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤
        - ì´ ë‹µë³€ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ì‹œìŠ¤í…œ ì „ë©´ ì ê²€ í•„ìš”:**
        - RAG ì‹œìŠ¤í…œ ì „ì²´ ì¬ì„¤ê³„ ê³ ë ¤
        - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê·¼ë³¸ì  ì¬ê²€í† 
        - ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê²€í† 
        - ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµì²´
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."
    
    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_relevancy_analysis_actual(score):
    """Answer Relevancy ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""
    
    if score >= 0.9:
        base_analysis = """
        **ğŸ¯ ì™„ë²½í•œ ê´€ë ¨ì„± (0.9+)**
        - ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í–ˆìŠµë‹ˆë‹¤
        - ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ì´ ì§ì ‘ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì™„ë²½í•˜ê²Œ ì œê³µí–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì´ìƒì ì¸ ë‹µë³€ì…ë‹ˆë‹¤. í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì„ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ë§¤ìš° ë†’ì€ ì •í™•ë„ì…ë‹ˆë‹¤."
        
    elif score >= 0.8:
        base_analysis = """
        **âœ… ë†’ì€ ê´€ë ¨ì„± (0.8-0.9)**
        - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì˜ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ëŒ€ì²´ë¡œ ì˜ ì´í•´í–ˆìŠµë‹ˆë‹¤
        - ì†Œìˆ˜ì˜ ë¶€ê°€ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆì§€ë§Œ ìœ ìš©í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ë¯¸ì„¸ ì¡°ì • ë°©ì•ˆ:**
        - ë‹µë³€ì„ ë” ê°„ê²°í•˜ê²Œ ë§Œë“¤ì–´ í•µì‹¬ ì§‘ì¤‘ë„ í–¥ìƒ
        - ì§ˆë¬¸ í‚¤ì›Œë“œì— ë” ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ” ë‹µë³€ êµ¬ì¡°
        """
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì•½ê°„ì˜ ì—¬ë¶„ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ê´€ë ¨ì„± (0.6-0.8)**
        - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì§€ë§Œ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆê±°ë‚˜ í•µì‹¬ì„ ì™„ì „íˆ ë‹¤ë£¨ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì— ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ê°œì„  ë°©ì•ˆ:**
        - ì§ˆë¬¸ ë¶„ì„ ë‹¨ê³„ ê°•í™” (í‚¤ì›Œë“œ ì¶”ì¶œ, ì˜ë„ ë¶„ë¥˜)
        - ë‹µë³€ ìƒì„± ì „ ì§ˆë¬¸ ì¬í™•ì¸ ë‹¨ê³„ ì¶”ê°€
        - ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª… ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‹µë³€
        - "ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì‹œì˜¤" í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        """
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê°„ì ‘ì  ê´€ë ¨ì„±ì„ ê°€ì§‘ë‹ˆë‹¤."
        
    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ê´€ë ¨ì„± (0.4-0.6)**
        - ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ë†“ì³¤ìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ë‹µë³€í–ˆê±°ë‚˜ ë„ˆë¬´ ì¼ë°˜ì ì…ë‹ˆë‹¤
        - ì§ˆë¬¸ìì˜ ì‹¤ì œ ë‹ˆì¦ˆë¥¼ ì œëŒ€ë¡œ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ì¦‰ì‹œ ê°œì„  í•„ìš”:**
        - ì§ˆë¬¸ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ (Few-shot ì˜ˆì‹œ ì¶”ê°€)
        - ë‹µë³€ ìƒì„± ì „ ì§ˆë¬¸ í‚¤ì›Œë“œ ëª…ì‹œì  í™•ì¸
        - ë” êµ¬ì²´ì ì´ê³  ì§ì ‘ì ì¸ ë‹µë³€ ìŠ¤íƒ€ì¼ë¡œ ë³€ê²½
        - ì§ˆë¬¸ ìœ í˜•ë³„ ë‹µë³€ í…œí”Œë¦¿ ë„ì…
        """
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì— ì¤‘ëŒ€í•œ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤."
        
    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ê´€ë ¨ì„± (<0.4)**
        - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê±°ì˜ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤
        - ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œì— ëŒ€í•´ ë‹µë³€í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ ì´í•´ ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ì‹œìŠ¤í…œ ì¬ì„¤ê³„ í•„ìš”:**
        - ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ì´í•´ ëª¨ë“ˆ ì™„ì „ ì¬êµ¬ì¶•
        - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë©´ ì¬ê²€í† 
        - ì§ˆë¬¸-ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ êµì²´
        - ë‹¤ë¥¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ê³ ë ¤
        """
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì‹œìŠ¤í…œì´ ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    
    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_recall_analysis_actual(score):
    """Context Recall ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""
    
    if score >= 0.9:
        base_analysis = """
        **ğŸ” íƒì›”í•œ ê²€ìƒ‰ ì™„ì„±ë„ (0.9+)**
        - Ground truthì˜ í•µì‹¬ ì •ë³´ê°€ ëª¨ë‘ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - í•„ìš”í•œ ì •ë³´ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ì•Šê³  ì™„ë²½í•˜ê²Œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ë§¤ìš° íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì™„ë²½í•œ ê²€ìƒ‰ ì„±ëŠ¥ì…ë‹ˆë‹¤. í˜„ì¬ ê²€ìƒ‰ ì „ëµì„ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    elif score >= 0.8:
        base_analysis = """
        **âœ… ìš°ìˆ˜í•œ ê²€ìƒ‰ ì™„ì„±ë„ (0.8-0.9)**
        - Ground truthì˜ ëŒ€ë¶€ë¶„ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤
        - ì£¼ìš” ì •ë³´ëŠ” ëª¨ë‘ í¬í•¨ë˜ì—ˆê³ , ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ë§Œ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ì „ë°˜ì ìœ¼ë¡œ íš¨ê³¼ì ì¸ ì •ë³´ ê²€ìƒ‰ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ê²€ìƒ‰ í–¥ìƒ ë°©ì•ˆ:**
        - ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ì–‘í™” (ë™ì˜ì–´, ê´€ë ¨ì–´ ì¶”ê°€)
        - ê²€ìƒ‰ ë²”ìœ„ ì†Œí­ í™•ì¥
        - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ì  ê²€ìƒ‰) ë„ì…
        """
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ëŒ€ë¶€ë¶„ì˜ ì¤‘ìš” ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ê²€ìƒ‰ ì™„ì„±ë„ (0.6-0.8)**
        - Ground truthì˜ ì¼ë¶€ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤
        - ì¤‘ìš”í•œ ì •ë³´ê°€ ì¼ë¶€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤
        - ê²€ìƒ‰ ì „ëµì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ê²€ìƒ‰ ê°œì„  ë°©ì•ˆ:**
        - ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥ ë° ë‹¤ê°í™”
        - ê²€ìƒ‰ ê¹Šì´ ì¦ê°€ (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)
        - ë‹¤ë‹¨ê³„ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ë„ì…
        - ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ê³ ë ¤
        - ì˜ë¯¸ì  ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¡°ì •
        """
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ê´€ë ¨ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ê²€ìƒ‰ ì™„ì„±ë„ (0.4-0.6)**
        - Ground truthì˜ ìƒë‹¹ ë¶€ë¶„ì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
        - ì¤‘ìš”í•œ ì •ë³´ê°€ ë§ì´ ëˆ„ë½ë˜ì–´ ë‹µë³€ í’ˆì§ˆì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ê·¼ë³¸ì  ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¬ê²€í†  í•„ìš”:**
        - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì „ë©´ ì¬í‰ê°€
        - ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ê³ ë ¤
        - ë¬¸ì„œ ì²­í‚¹ ì „ëµ ì¬ì„¤ê³„
        - ê²€ìƒ‰ ì¸ë±ìŠ¤ í’ˆì§ˆ ì ê²€
        - ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ë³‘í–‰ ì‚¬ìš©
        """
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ê²€ìƒ‰ ì™„ì„±ë„ (<0.4)**
        - Ground truthì˜ ëŒ€ë¶€ë¶„ì´ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤
        - ì´ ìˆ˜ì¤€ì—ì„œëŠ” ìœ ìš©í•œ ë‹µë³€ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ë©´ ì¬êµ¬ì¶• í•„ìš”:**
        - ê²€ìƒ‰ ì•„í‚¤í…ì²˜ ì™„ì „ ì¬ì„¤ê³„
        - ë‹¤ë¥¸ ê²€ìƒ‰ ê¸°ìˆ  ìŠ¤íƒ ë„ì…
        - ë¬¸ì„œ ì „ì²˜ë¦¬ ê³¼ì • ì¬ê²€í† 
        - ê²€ìƒ‰ ëª¨ë¸ êµì²´
        - ì „ë¬¸ê°€ ì»¨ì„¤íŒ… ê³ ë ¤
        """
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ì²´ê°€ ê¸°ëŠ¥í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."
    
    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_precision_analysis_actual(score):
    """Context Precision ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""
    
    if score >= 0.9:
        base_analysis = """
        **ğŸ¯ íƒì›”í•œ ê²€ìƒ‰ ì •í™•ë„ (0.9+)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ë§¤ìš° ì •í™•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤
        - ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ê±°ì˜ ì—†ì–´ ë§¤ìš° íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì…ë‹ˆë‹¤
        - ë…¸ì´ì¦ˆ ì—†ëŠ” ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì™„ë²½í•œ ê²€ìƒ‰ ì •í™•ë„ì…ë‹ˆë‹¤. í˜„ì¬ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ê±°ì˜ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ê°€ ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        
    elif score >= 0.8:
        base_analysis = """
        **âœ… ë†’ì€ ê²€ìƒ‰ ì •í™•ë„ (0.8-0.9)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì˜ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤
        - ëŒ€ë¶€ë¶„ì˜ ì •ë³´ê°€ ìœ ìš©í•˜ë©° ì†Œìˆ˜ì˜ ë¶€ê°€ ì •ë³´ë§Œ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ì •í™•ë„ í–¥ìƒ ë°©ì•ˆ:**
        - ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
        - ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§ ê·œì¹™ ì„¸ë°€í™”
        - ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì •
        """
        technical_details = f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ì†ŒëŸ‰ì˜ ë¶€ê°€ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ê²€ìƒ‰ ì •í™•ë„ (0.6-0.8)**
        - ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ìˆìŠµë‹ˆë‹¤
        - ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤
        - ê²€ìƒ‰ í•„í„°ë§ì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ì •í™•ë„ ê°œì„  ë°©ì•ˆ:**
        - ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ê°•í™”
        - ê´€ë ¨ì„± ì ìˆ˜ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        - ì¤‘ë³µ ì œê±° ë° ë…¸ì´ì¦ˆ í•„í„°ë§ ê°œì„ 
        - ì¿¼ë¦¬-ë¬¸ì„œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì •êµí™”
        - ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ë„ì…
        """
        technical_details = f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì  ê´€ë ¨ì„±ì„ ê°€ì§‘ë‹ˆë‹¤."
        
    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„ (0.4-0.6)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ë¬´ê´€í•œ ì •ë³´ê°€ ìƒë‹¹íˆ ë§ìŠµë‹ˆë‹¤
        - ë…¸ì´ì¦ˆê°€ ë§ì•„ ë‹µë³€ í’ˆì§ˆì— ë¶€ì •ì  ì˜í–¥ì„ ì¤ë‹ˆë‹¤
        - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒì´ ì‹œê¸‰í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ê²€ìƒ‰ í•„í„°ë§ ê°•í™” í•„ìš”:**
        - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì¬ì„¤ê³„
        - ë” ì—„ê²©í•œ ê´€ë ¨ì„± ê¸°ì¤€ ì ìš©
        - ë‹¤ë‹¨ê³„ í•„í„°ë§ í”„ë¡œì„¸ìŠ¤ ë„ì…
        - ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ëª¨ë¸ ê°œì„ 
        - ë¶ˆìš©ì–´ ë° ë…¸ì´ì¦ˆ ì œê±° ê°•í™”
        """
        technical_details = f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ìƒë‹¹ëŸ‰ì˜ ë¬´ê´€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„ (<0.4)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ëŒ€ë¶€ë¶„ì´ ì§ˆë¬¸ê³¼ ë¬´ê´€í•©ë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì§ˆë¬¸ì„ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
        - ì´ëŸ° ë‚®ì€ ì •í™•ë„ë¡œëŠ” ìœ ìš©í•œ ë‹µë³€ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ë©´ ì¬ê²€í†  í•„ìš”:**
        - ê²€ìƒ‰ ì—”ì§„ ì „ì²´ êµì²´ ê³ ë ¤
        - ì¿¼ë¦¬ ì´í•´ ëª¨ë“ˆ ì¬êµ¬ì¶•
        - ë¬¸ì„œ ì¸ë±ì‹± ë°©ì‹ ê·¼ë³¸ì  ë³€ê²½
        - ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì²´ê³„ ì¬ì„¤ê³„
        - ì™¸ë¶€ ê²€ìƒ‰ ì†”ë£¨ì…˜ ë„ì… ê²€í† 
        """
        technical_details = f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."
    
    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def show_metric_distribution_actual(individual_scores, evaluation_data):
    """ì‹¤ì œ í‰ê°€ëœ ë°ì´í„°ì˜ ë©”íŠ¸ë¦­ ë¶„í¬"""
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„")
    
    if not individual_scores:
        st.warning("ê°œë³„ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # DataFrame ìƒì„±
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    data = {'QA': [f'Q{i+1}' for i in range(len(individual_scores))]}
    
    for metric in metrics:
        data[metric] = [score.get(metric, 0) for score in individual_scores]
    
    df = pd.DataFrame(data)
    
    # íˆíŠ¸ë§µ
    st.markdown("#### ğŸ”¥ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ")
    
    heatmap_data = df[metrics].values
    
    fig = go.Figure(data=go.Heatmap(
        z=heatmap_data,
        x=[m.replace('_', ' ').title() for m in metrics],
        y=df['QA'],
        colorscale='RdYlGn',
        colorbar=dict(title="ì ìˆ˜")
    ))
    
    fig.update_layout(
        title="ì‹¤ì œ í‰ê°€ëœ QAë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥",
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # ë¶„í¬ í†µê³„
    st.markdown("#### ğŸ“ˆ ë¶„í¬ í†µê³„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**í‰ê·  ì ìˆ˜**")
        for metric in metrics:
            avg_score = df[metric].mean()
            st.text(f"{metric.replace('_', ' ').title()}: {avg_score:.3f}")
    
    with col2:
        st.markdown("**í‘œì¤€í¸ì°¨**")
        for metric in metrics:
            std_score = df[metric].std()
            st.text(f"{metric.replace('_', ' ').title()}: {std_score:.3f}")


def show_pattern_analysis_actual(individual_scores, evaluation_data):
    """ì‹¤ì œ í‰ê°€ ë°ì´í„°ì˜ íŒ¨í„´ ë¶„ì„"""
    st.subheader("ğŸ¯ ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„")
    
    qa_count = len(individual_scores)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“ í‰ê°€ ê°œìš”")
        st.metric("ì‹¤ì œ í‰ê°€ëœ QA ê°œìˆ˜", qa_count)
        
        # í‰ê°€ ì‹œê°„
        timestamp = evaluation_data.get('timestamp', '')
        if timestamp:
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                formatted_time = dt.strftime('%Yë…„ %mì›” %dì¼ %H:%M')
                st.text(f"í‰ê°€ ì‹œê°„: {formatted_time}")
            except:
                st.text(f"í‰ê°€ ì‹œê°„: {timestamp}")
    
    with col2:
        st.markdown("#### ğŸ“Š ì„±ëŠ¥ ìš”ì•½")
        ragas_score = evaluation_data.get('ragas_score', 0)
        st.metric("ì „ì²´ RAGAS ì ìˆ˜", f"{ragas_score:.3f}")
        
        # ìµœê³ /ìµœì € ë©”íŠ¸ë¦­
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        scores = {m: evaluation_data.get(m, 0) for m in metrics}
        
        if scores:
            best_metric = max(scores, key=scores.get)
            worst_metric = min(scores, key=scores.get)
            
            st.text(f"ìµœê³  ì„±ëŠ¥: {best_metric.replace('_', ' ').title()} ({scores[best_metric]:.3f})")
            st.text(f"ê°œì„  í•„ìš”: {worst_metric.replace('_', ' ').title()} ({scores[worst_metric]:.3f})")
    
    # ê°œì„  ì œì•ˆ
    st.markdown("#### ğŸ’¡ ì´ í‰ê°€ì— ëŒ€í•œ ê°œì„  ì œì•ˆ")
    
    suggestions = []
    
    if evaluation_data.get('faithfulness', 0) < 0.7:
        suggestions.append("ğŸ¯ Faithfulness ê°œì„ : ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„ ê°•í™”, í™˜ê° ë°©ì§€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
    
    if evaluation_data.get('answer_relevancy', 0) < 0.7:
        suggestions.append("ğŸ¯ Answer Relevancy ê°œì„ : ì§ˆë¬¸ ì˜ë„ íŒŒì•… ê°•í™”, ê°„ê²°í•œ ë‹µë³€ ìƒì„±")
    
    if evaluation_data.get('context_recall', 0) < 0.7:
        suggestions.append("ğŸ¯ Context Recall ê°œì„ : ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€, ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ í™œìš©")
    
    if evaluation_data.get('context_precision', 0) < 0.7:
        suggestions.append("ğŸ¯ Context Precision ê°œì„ : ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§, ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ")
    
    if not suggestions:
        suggestions.append("âœ… ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì–‘í˜¸í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤! í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")
    
    for suggestion in suggestions:
        st.info(suggestion)


# Historical í˜ì´ì§€ì™€ì˜ ì—°ë™ì„ ìœ„í•œ í•¨ìˆ˜
def set_selected_evaluation(evaluation_id):
    """Historical í˜ì´ì§€ì—ì„œ íŠ¹ì • í‰ê°€ë¥¼ ì„ íƒí–ˆì„ ë•Œ í˜¸ì¶œ"""
    all_evaluations = load_all_evaluations()
    for i, eval_data in enumerate(all_evaluations):
        if eval_data['id'] == evaluation_id:
            st.session_state.selected_evaluation_index = i
            break