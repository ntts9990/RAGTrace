"""
ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸ - ì‹¤ì œ í‰ê°€ ë°ì´í„° ê¸°ë°˜
ì‹¤ì œë¡œ í‰ê°€ëœ QA ë°ì´í„°ë§Œ í‘œì‹œí•˜ê³  Historical í˜ì´ì§€ì™€ ì—°ë™
"""

import json
import sqlite3
from datetime import datetime, timedelta
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import plotly.figure_factory as ff
import streamlit as st

from src.utils.paths import (
    DATABASE_PATH,
    get_available_datasets,
    get_evaluation_data_path,
)


def load_all_evaluations():
    """ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¡œë“œ (Historical í˜ì´ì§€ ì—°ë™ìš©)"""
    try:
        # Use DATABASE_PATH from paths module
        if not DATABASE_PATH.exists():
            return []

        conn = sqlite3.connect(str(DATABASE_PATH))

        query = """
            SELECT id, timestamp, faithfulness, answer_relevancy, 
                   context_recall, context_precision, answer_correctness, ragas_score, raw_data
            FROM evaluations 
            ORDER BY timestamp DESC
        """

        cursor = conn.execute(query)
        results = cursor.fetchall()
        conn.close()

        evaluations = []
        for row in results:
            evaluation = {
                "id": row[0],
                "timestamp": row[1],
                "faithfulness": row[2],
                "answer_relevancy": row[3],
                "context_recall": row[4],
                "context_precision": row[5],
                "answer_correctness": row[6],
                "ragas_score": row[7],
                "raw_data": json.loads(row[8]) if row[8] else None,
            }
            evaluations.append(evaluation)

        return evaluations

    except Exception as e:
        st.error(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def load_evaluation_by_id(evaluation_id):
    """íŠ¹ì • í‰ê°€ IDë¡œ í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    try:
        # Use DATABASE_PATH from paths module
        if not DATABASE_PATH.exists():
            return None, []

        conn = sqlite3.connect(str(DATABASE_PATH))

        query = """
            SELECT raw_data 
            FROM evaluations 
            WHERE id = ?
        """

        result = conn.execute(query, (evaluation_id,)).fetchone()
        conn.close()

        if result and result[0]:
            raw_data = json.loads(result[0])
            individual_scores = raw_data.get("individual_scores", [])
            return raw_data, individual_scores

        return None, []

    except Exception as e:
        st.error(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, []


def load_actual_qa_data_from_dataset_simple(dataset_name, qa_count):
    """ê°„ë‹¨í•œ ë²„ì „ - ì§ì ‘ íŒŒì¼ ë¡œë“œ"""
    try:
        # í•˜ë“œì½”ë”©ëœ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        if "variant1" in dataset_name:
            file_path = get_evaluation_data_path("evaluation_data_variant1.json")
        else:
            file_path = get_evaluation_data_path("evaluation_data.json")

        if not file_path:
            st.error(f"ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        with open(file_path, encoding="utf-8") as f:
            data = json.load(f)
            return data[:qa_count]
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def load_actual_qa_data_from_dataset(dataset_name, qa_count):
    """ë°ì´í„°ì…‹ íŒŒì¼ì—ì„œ ì‹¤ì œ QA ë°ì´í„° ë¡œë“œ (ê°œì„ ëœ ë²„ì „)"""
    try:
        # ì¤‘ì•™ ê²½ë¡œ ê´€ë¦¬ ëª¨ë“ˆ ì‚¬ìš©
        file_path = get_evaluation_data_path(dataset_name)

        if not file_path:
            st.error(f"ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ í‘œì‹œ
            available_datasets = get_available_datasets()
            if available_datasets:
                st.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {', '.join(available_datasets)}")
            return None

        # íŒŒì¼ ë¡œë“œ ë° íŒŒì‹±
        with open(file_path, encoding="utf-8") as f:
            all_qa_data = json.load(f)

        if not isinstance(all_qa_data, list) or len(all_qa_data) == 0:
            st.error(
                f"ë°ì´í„°ì…‹ '{dataset_name}'ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            )
            return None

        # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        result = all_qa_data[:qa_count]
        st.success(
            f"ë°ì´í„°ì…‹ '{file_path.name}'ì—ì„œ {len(result)}ê°œì˜ QA ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤."
        )
        return result

    except json.JSONDecodeError as e:
        st.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_actual_qa_data_from_evaluation(raw_data, evaluation_db_id):
    """í‰ê°€ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ QA ë°ì´í„° ì¶”ì¶œ"""
    if not raw_data:
        return None

    # raw_dataì—ì„œ ì‹¤ì œ í‰ê°€ì— ì‚¬ìš©ëœ QA ë°ì´í„° ì°¾ê¸°
    metadata = raw_data.get("metadata", {})

    # individual_scoresì˜ ê°œìˆ˜ê°€ ì‹¤ì œ í‰ê°€ëœ QA ê°œìˆ˜
    individual_scores = raw_data.get("individual_scores", [])
    actual_qa_count = len(individual_scores)

    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ, ì—†ìœ¼ë©´ DB ID ì‚¬ìš©
    evaluation_id = metadata.get("evaluation_id", f"DB#{evaluation_db_id}")
    model_info = metadata.get("model", "Gemini-2.5-Flash")
    dataset_info = metadata.get("dataset", "evaluation_data.json")

    # ë°ì´í„°ì…‹ì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    if "/" in dataset_info:
        dataset_name = dataset_info.split("/")[-1]
    else:
        dataset_name = dataset_info

    # ì‹¤ì œ QA ë°ì´í„° ë¡œë“œ - ê°„ë‹¨í•œ ë²„ì „ ì‚¬ìš©
    actual_qa_data = load_actual_qa_data_from_dataset_simple(
        dataset_name, actual_qa_count
    )

    return {
        "qa_count": actual_qa_count,
        "dataset_size": metadata.get("dataset_size", actual_qa_count),
        "evaluation_id": evaluation_id,
        "timestamp": metadata.get("timestamp", "unknown"),
        "model": model_info,
        "dataset": dataset_name,
        "qa_data": actual_qa_data,
    }


def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ë©”ì¸ í™”ë©´ - ì‹¤ì œ í‰ê°€ ë°ì´í„° ê¸°ë°˜"""
    st.header("ğŸ” ìƒì„¸ ë¶„ì„")

    # í‰ê°€ ì„ íƒ ì„¹ì…˜
    st.subheader("ğŸ“‹ í‰ê°€ ì„ íƒ")

    # ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¡œë“œ
    all_evaluations = load_all_evaluations()

    if not all_evaluations:
        st.error("âŒ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ Overview í˜ì´ì§€ì—ì„œ 'ìƒˆ í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        return

    # í‰ê°€ ì„ íƒ ì˜µì…˜ ìƒì„±
    evaluation_options = []
    for i, eval_data in enumerate(all_evaluations):
        timestamp = eval_data["timestamp"]
        qa_count = 0
        if eval_data["raw_data"] and eval_data["raw_data"].get("individual_scores"):
            qa_count = len(eval_data["raw_data"]["individual_scores"])

        # timestampë¥¼ ë” ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
        try:
            dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            formatted_time = timestamp

        option_text = f"í‰ê°€ #{eval_data['id']} - {formatted_time} ({qa_count}ê°œ QA)"
        evaluation_options.append(option_text)

    # ì„¸ì…˜ ìƒíƒœë¡œ ì„ íƒëœ í‰ê°€ ê´€ë¦¬
    if "selected_evaluation_index" not in st.session_state:
        st.session_state.selected_evaluation_index = 0

    selected_eval_idx = st.selectbox(
        "ë¶„ì„í•  í‰ê°€ ì„ íƒ",
        range(len(evaluation_options)),
        format_func=lambda x: evaluation_options[x],
        index=st.session_state.selected_evaluation_index,
        key="evaluation_selector",
    )

    # ì„ íƒëœ í‰ê°€ ë°ì´í„° ë¡œë“œ
    selected_evaluation = all_evaluations[selected_eval_idx]
    evaluation_id = selected_evaluation["id"]

    # ì„ íƒëœ í‰ê°€ì˜ ìƒì„¸ ë°ì´í„° ë¡œë“œ
    raw_data, individual_scores = load_evaluation_by_id(evaluation_id)

    if not raw_data:
        st.error(f"âŒ í‰ê°€ ID {evaluation_id}ì˜ ìƒì„¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹¤ì œ í‰ê°€ëœ QA ë°ì´í„° ì •ë³´
    qa_info = get_actual_qa_data_from_evaluation(raw_data, evaluation_id)

    if not qa_info or qa_info["qa_count"] == 0:
        st.error("âŒ ì´ í‰ê°€ì—ëŠ” ê°œë³„ QA ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í‰ê°€ ì •ë³´ í‘œì‹œ
    st.success(f"âœ… í‰ê°€ #{evaluation_id} ë¡œë“œ ì™„ë£Œ")

    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("QA ê°œìˆ˜", qa_info["qa_count"])
    with col2:
        st.metric("í‰ê°€ ID", qa_info["evaluation_id"])
    with col3:
        st.metric("ëª¨ë¸", qa_info["model"])
    with col4:
        st.metric("ë°ì´í„°ì…‹", qa_info["dataset"])
    with col5:
        ragas_score = selected_evaluation.get("ragas_score", 0)
        st.metric("RAGAS ì ìˆ˜", f"{ragas_score:.3f}")

    # ê°œë³„ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¶„ì„ ì§„í–‰
    if not individual_scores:
        st.warning("âš ï¸ ì´ í‰ê°€ì—ëŠ” ê°œë³„ QA ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        show_overall_metrics_only(selected_evaluation)
        return

    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ - ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“Š QA ê°œë³„ ë¶„ì„", 
        "ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„í¬", 
        "ğŸ¯ íŒ¨í„´ ë¶„ì„", 
        "ğŸ“Š EDA ë¶„ì„",
        "ğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„", 
        "ğŸš¨ ì´ìƒì¹˜ íƒì§€",
        "ğŸ” ê³ ê¸‰ í†µê³„"
    ])

    with tab1:
        # ë””ë²„ê·¸: qa_info ìƒíƒœ í™•ì¸
        if qa_info and "qa_data" in qa_info:
            if qa_info["qa_data"]:
                st.info(f"ğŸ“Š QA ë°ì´í„° ìƒíƒœ: ë¡œë“œë¨ ({len(qa_info['qa_data'])}ê°œ)")
            else:
                st.warning("ğŸ“Š QA ë°ì´í„° ìƒíƒœ: ë¹„ì–´ìˆìŒ")
                # í„°ë¯¸ë„ ì¶œë ¥ í™•ì¸ ì•ˆë‚´
                st.info("ğŸ’¡ í„°ë¯¸ë„ì—ì„œ ë””ë²„ê·¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        show_qa_analysis_actual(
            individual_scores, evaluation_id, qa_info.get("qa_data")
        )

    with tab2:
        show_metric_distribution_actual(individual_scores, selected_evaluation)

    with tab3:
        show_pattern_analysis_actual(individual_scores, selected_evaluation)

    with tab4:
        show_eda_analysis(all_evaluations, selected_evaluation)

    with tab5:
        show_time_series_analysis(all_evaluations, selected_evaluation)

    with tab6:
        show_anomaly_detection(all_evaluations, selected_evaluation)

    with tab7:
        show_advanced_statistics(individual_scores, selected_evaluation, all_evaluations)


def show_overall_metrics_only(evaluation_data):
    """ê°œë³„ ì ìˆ˜ê°€ ì—†ì„ ë•Œ ì „ì²´ ë©”íŠ¸ë¦­ë§Œ í‘œì‹œ"""
    st.subheader("ğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼")

    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    
    # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if "answer_correctness" in evaluation_data:
        metrics.append("answer_correctness")
        cols = st.columns(5)
    else:
        cols = st.columns(4)

    for i, metric in enumerate(metrics):
        with cols[i]:
            score = evaluation_data.get(metric, 0)
            st.metric(label=metric.replace("_", " ").title(), value=f"{score:.3f}")


def show_qa_analysis_actual(individual_scores, evaluation_id, qa_data=None):
    """ì‹¤ì œ í‰ê°€ëœ QA ê°œë³„ ë¶„ì„"""
    st.subheader("ğŸ“‹ QA ê°œë³„ ë¶„ì„")

    qa_count = len(individual_scores)

    if qa_count == 0:
        st.warning("ë¶„ì„í•  QA ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë””ë²„ê·¸: qa_data ìƒíƒœ í™•ì¸
    if qa_data is None:
        st.error("âš ï¸ QA ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        with st.expander("ğŸ” ë¬¸ì œ í•´ê²° ë°©ë²•"):
            st.markdown(
                """
            **ê°€ëŠ¥í•œ ì›ì¸:**
            1. í‰ê°€ ë°ì´í„° íŒŒì¼ì´ í”„ë¡œì íŠ¸ì˜ `data/` ë””ë ‰í† ë¦¬ì— ì—†ìŒ
            2. íŒŒì¼ ì´ë¦„ì´ `evaluation_data.json` ë˜ëŠ” `evaluation_data_variant1.json`ì´ ì•„ë‹˜
            3. íŒŒì¼ ê¶Œí•œ ë¬¸ì œ
            
            **í•´ê²° ë°©ë²•:**
            - í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ `data/` ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ í™•ì¸
            - í•„ìš”í•œ ê²½ìš° íŒŒì¼ ê¶Œí•œ ìˆ˜ì •
            - ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì •í™•í•œ ì˜¤ë¥˜ ìœ„ì¹˜ íŒŒì•…
            """
            )
    elif len(qa_data) == 0:
        st.error("âš ï¸ QA ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    else:
        st.success(f"âœ… {len(qa_data)}ê°œì˜ QA ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # QA ì„ íƒ ì˜µì…˜ ìƒì„± (ì‹¤ì œ ì ìˆ˜ì™€ ì§ˆë¬¸ ë‚´ìš© ê¸°ë°˜)
    qa_options = []
    for i, qa_score in enumerate(individual_scores):
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = 0
        if qa_score:
            avg_score = (
                sum(qa_score.values()) / len(qa_score) if qa_score.values() else 0
            )

        # ì§ˆë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
        question_preview = "ì§ˆë¬¸ ì •ë³´ ì—†ìŒ"
        if qa_data and i < len(qa_data):
            question = qa_data[i].get("question", "")
            if question:
                # ì§ˆë¬¸ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì²˜ë¦¬
                if len(question) > 50:
                    question_preview = question[:47] + "..."
                else:
                    question_preview = question

        qa_options.append(f"QA #{i+1}: {question_preview} (í‰ê· : {avg_score:.3f})")

    selected_qa_idx = st.selectbox(
        "ë¶„ì„í•  QA ì„ íƒ", range(len(qa_options)), format_func=lambda x: qa_options[x]
    )

    if selected_qa_idx is not None and selected_qa_idx < len(individual_scores):
        qa_scores = individual_scores[selected_qa_idx]
        qa_content = (
            qa_data[selected_qa_idx]
            if qa_data and selected_qa_idx < len(qa_data)
            else None
        )
        show_individual_qa_details_actual(
            selected_qa_idx + 1, qa_scores, evaluation_id, qa_content
        )


def show_individual_qa_details_actual(
    qa_number, qa_scores, evaluation_id, qa_content=None
):
    """ì‹¤ì œ í‰ê°€ëœ ê°œë³„ QA ìƒì„¸ ì •ë³´ í‘œì‹œ"""
    st.markdown(f"### ğŸ“ QA {qa_number} ìƒì„¸ ë¶„ì„ (í‰ê°€ #{evaluation_id})")

    if not qa_scores:
        st.error("âŒ ì´ QAì— ëŒ€í•œ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # QA ë‚´ìš© í‘œì‹œ (ì‹¤ì œ ì§ˆë¬¸, ë‹µë³€, ì»¨í…ìŠ¤íŠ¸)
    if qa_content:
        st.markdown("#### ğŸ“‹ QA ë‚´ìš©")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**ğŸ¤” ì§ˆë¬¸:**")
            st.info(qa_content.get("question", "ì§ˆë¬¸ ì •ë³´ ì—†ìŒ"))

            st.markdown("**ğŸ’¡ ìƒì„±ëœ ë‹µë³€:**")
            st.success(qa_content.get("answer", "ë‹µë³€ ì •ë³´ ì—†ìŒ"))

        with col2:
            st.markdown("**ğŸ“š ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:**")
            contexts = qa_content.get("contexts", [])
            for i, context in enumerate(contexts, 1):
                with st.expander(f"ì»¨í…ìŠ¤íŠ¸ {i}"):
                    st.text(context)

            st.markdown("**âœ… ì •ë‹µ (Ground Truth):**")
            st.info(qa_content.get("ground_truth", "ì •ë‹µ ì •ë³´ ì—†ìŒ"))

        st.markdown("---")

    # ì ìˆ˜ ì¹´ë“œ í‘œì‹œ
    st.markdown("#### ğŸ“Š í‰ê°€ ì ìˆ˜")

    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    
    # answer_correctnessê°€ qa_scoresì— ìˆìœ¼ë©´ ì¶”ê°€
    if "answer_correctness" in qa_scores:
        metrics.append("answer_correctness")
        score_cols = st.columns(5)
    else:
        score_cols = st.columns(4)

    for i, metric in enumerate(metrics):
        with score_cols[i]:
            score = qa_scores.get(metric, 0)
            color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
            st.metric(label=metric.replace("_", " ").title(), value=f"{score:.3f}")

    # ì ìˆ˜ ì‹œê°í™”
    show_qa_score_chart_actual(qa_scores, qa_number)

    # í‰ê°€ ê·¼ê±° (ì ìˆ˜ ê¸°ë°˜)
    show_evaluation_reasoning_actual(qa_number, qa_scores, qa_content)


def show_qa_score_chart_actual(scores, qa_number):
    """ì‹¤ì œ í‰ê°€ëœ QA ì ìˆ˜ ì°¨íŠ¸"""
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ì‹œê°í™”")

    col1, col2 = st.columns(2)

    with col1:
        # ë°” ì°¨íŠ¸
        metrics = list(scores.keys())
        values = list(scores.values())

        fig = go.Figure(
            data=[
                go.Bar(
                    x=metrics,
                    y=values,
                    marker_color=[
                        "green" if v >= 0.8 else "orange" if v >= 0.6 else "red"
                        for v in values
                    ],
                    text=[f"{v:.3f}" for v in values],
                    textposition="auto",
                )
            ]
        )

        fig.update_layout(
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ì ìˆ˜",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=300,
        )

        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # ë ˆì´ë” ì°¨íŠ¸
        fig = go.Figure()

        fig.add_trace(
            go.Scatterpolar(
                r=values + [values[0]],  # ì°¨íŠ¸ë¥¼ ë‹«ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ê°’ ì¶”ê°€
                theta=metrics + [metrics[0]],
                fill="toself",
                name=f"QA {qa_number}",
                line_color="rgb(32, 201, 151)",
            )
        )

        fig.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            showlegend=True,
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ê· í˜•ë„",
            height=300,
        )

        st.plotly_chart(fig, use_container_width=True)


def show_evaluation_reasoning_actual(qa_number, scores, qa_content=None):
    """ì‹¤ì œ í‰ê°€ ì ìˆ˜ ê¸°ë°˜ í‰ê°€ ê·¼ê±°"""
    st.markdown("#### ğŸ§  í‰ê°€ ê·¼ê±°")

    # QA ë‚´ìš© ìš”ì•½ í‘œì‹œ (í‰ê°€ ê·¼ê±°ì—ì„œ ì°¸ê³ ìš©)
    if qa_content:
        st.info(
            f"**ì°¸ê³ :** ì´ ë¶„ì„ì€ '{qa_content.get('question', '')[:50]}...' ì§ˆë¬¸ì— ëŒ€í•œ í‰ê°€ì…ë‹ˆë‹¤."
        )

    # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„
    metrics_analysis = {
        "faithfulness": {
            "description": "ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •",
            "score": scores.get("faithfulness", 0),
            "analysis": generate_faithfulness_analysis_actual(
                scores.get("faithfulness", 0)
            ),
        },
        "answer_relevancy": {
            "description": "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •",
            "score": scores.get("answer_relevancy", 0),
            "analysis": generate_relevancy_analysis_actual(
                scores.get("answer_relevancy", 0)
            ),
        },
        "context_recall": {
            "description": "Ground truthì˜ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ ë°œê²¬ë˜ëŠ”ì§€ ì¸¡ì •",
            "score": scores.get("context_recall", 0),
            "analysis": generate_recall_analysis_actual(
                scores.get("context_recall", 0)
            ),
        },
        "context_precision": {
            "description": "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •",
            "score": scores.get("context_precision", 0),
            "analysis": generate_precision_analysis_actual(
                scores.get("context_precision", 0)
            ),
        },
    }
    
    # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if "answer_correctness" in scores:
        metrics_analysis["answer_correctness"] = {
            "description": "ìƒì„±ëœ ë‹µë³€ì´ ì •ë‹µ(ground truth)ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •",
            "score": scores.get("answer_correctness", 0),
            "analysis": generate_answer_correctness_analysis_actual(
                scores.get("answer_correctness", 0)
            ),
        }

    for metric, info in metrics_analysis.items():
        with st.expander(
            f"ğŸ“ {metric.replace('_', ' ').title()} ë¶„ì„ (ì ìˆ˜: {info['score']:.3f})"
        ):
            st.markdown(f"**ì„¤ëª…:** {info['description']}")

            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ì„ ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ í‘œì‹œ
            analysis_lines = info["analysis"].split("\n")
            for line in analysis_lines:
                if line.strip():
                    st.markdown(line)

            # ì ìˆ˜ êµ¬ê°„ë³„ í•´ì„ ê°€ì´ë“œ
            st.markdown("---")
            st.markdown("**ì ìˆ˜ í•´ì„:**")
            if info["score"] >= 0.9:
                st.success("ğŸŒŸ ìš°ìˆ˜ (0.9+): ë§¤ìš° ë†’ì€ ì„±ëŠ¥")
            elif info["score"] >= 0.8:
                st.success("âœ… ì–‘í˜¸ (0.8-0.9): ì¢‹ì€ ì„±ëŠ¥")
            elif info["score"] >= 0.6:
                st.warning("âš ï¸ ë³´í†µ (0.6-0.8): ê°œì„  ì—¬ì§€ ìˆìŒ")
            else:
                st.error("âŒ ê°œì„ í•„ìš” (<0.6): ì¦‰ì‹œ ê°œì„  í•„ìš”")


def generate_faithfulness_analysis_actual(score):
    """Faithfulness ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""

    if score >= 0.9:
        base_analysis = """
        **ğŸŒŸ íƒì›”í•œ ì¶©ì‹¤ë„ (0.9+)**
        - ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë§¤ìš° ì¶©ì‹¤í•˜ê²Œ ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤
        - LLMì´ í™˜ê°(Hallucination) ì—†ì´ ì •í™•í•œ ì •ë³´ë§Œì„ í™œìš©í–ˆìŠµë‹ˆë‹¤
        - ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ê°€ëŠ¥í•œ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”. ì´ ì •ë„ ì¶©ì‹¤ë„ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì´ìƒì ì…ë‹ˆë‹¤."
        technical_details = (
            "ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©ê³¼ ë‹µë³€ ê°„ ì¼ì¹˜ë„ê°€ 90% ì´ìƒìœ¼ë¡œ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì…ë‹ˆë‹¤."
        )

    elif score >= 0.8:
        base_analysis = """
        **âœ… ìš°ìˆ˜í•œ ì¶©ì‹¤ë„ (0.8-0.9)**
        - ë‹µë³€ì˜ ëŒ€ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤
        - ì†Œìˆ˜ì˜ ì¶”ë¡ ì´ë‚˜ ì¼ë°˜í™”ê°€ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆì§€ë§Œ ì ì ˆí•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤
        - ì „ë°˜ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ê°œì„  ë°©ì•ˆ:**
        - í”„ë¡¬í”„íŠ¸ì— "ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬" ê°™ì€ ì œì•½ ì¡°ê±´ ì¶”ê°€
        - ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ë„ë¡ ìœ ë„
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì†Œìˆ˜ì˜ ì¶”ë¡  í¬í•¨ë˜ì—ˆì§€ë§Œ í—ˆìš© ë²”ìœ„ ë‚´ì…ë‹ˆë‹¤."

    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ì¶©ì‹¤ë„ (0.6-0.8)**
        - ë‹µë³€ì˜ ì¼ë¶€ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í™•ì¸ë©ë‹ˆë‹¤
        - ì¼ë¶€ ë‚´ìš©ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë„˜ì–´ì„  ì¶”ë¡ ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - ê²€ì¦ì´ í•„ìš”í•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ì¦‰ì‹œ ê°œì„  í•„ìš”:**
        - í”„ë¡¬í”„íŠ¸ì— "ì˜¤ì§ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©" ëª…ì‹œ
        - Temperature ê°’ì„ ë‚®ì¶° ë” ë³´ìˆ˜ì ì¸ ë‹µë³€ ìœ ë„
        - ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì •ë³´ ì‚¬ìš© ì‹œ ëª…ì‹œí•˜ë„ë¡ ì§€ì‹œ
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì •ë³´ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."

    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ì¶©ì‹¤ë„ (0.4-0.6)**
        - ë‹µë³€ì˜ ìƒë‹¹ ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë’·ë°›ì¹¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - í™˜ê°ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ì— ì˜ì¡´í•œ ë‚´ìš©ì´ ë§ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - ë‹µë³€ì˜ ì‹ ë¢°ì„±ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ê¸´ê¸‰ ìˆ˜ì • í•„ìš”:**
        - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì „ë©´ ì¬ê²€í† 
        - "ì ˆëŒ€ ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì •ë³´ ì‚¬ìš©í•˜ì§€ ë§ˆì‹œì˜¤" ëª…ì‹œ
        - RAG íŒŒì´í”„ë¼ì¸ì˜ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ê²€
        - ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì • (Top-p, Temperature ë“±)
        """
        technical_details = f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ê°€ ì ì¬ì  í™˜ê° ë˜ëŠ” ì™¸ë¶€ ì§€ì‹ì…ë‹ˆë‹¤."

    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ì¶©ì‹¤ë„ (<0.4)**
        - ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì™€ ê±°ì˜ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤
        - ì‹¬ê°í•œ í™˜ê° í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤
        - ì´ ë‹µë³€ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ì‹œìŠ¤í…œ ì „ë©´ ì ê²€ í•„ìš”:**
        - RAG ì‹œìŠ¤í…œ ì „ì²´ ì¬ì„¤ê³„ ê³ ë ¤
        - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê·¼ë³¸ì  ì¬ê²€í† 
        - ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê²€í† 
        - ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµì²´
        """
        technical_details = (
            f"ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {score:.1%}. ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."
        )

    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_relevancy_analysis_actual(score):
    """Answer Relevancy ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""

    if score >= 0.9:
        base_analysis = """
        **ğŸ¯ ì™„ë²½í•œ ê´€ë ¨ì„± (0.9+)**
        - ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í–ˆìŠµë‹ˆë‹¤
        - ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ì´ ì§ì ‘ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì™„ë²½í•˜ê²Œ ì œê³µí–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì´ìƒì ì¸ ë‹µë³€ì…ë‹ˆë‹¤. í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì„ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ë§¤ìš° ë†’ì€ ì •í™•ë„ì…ë‹ˆë‹¤."

    elif score >= 0.8:
        base_analysis = """
        **âœ… ë†’ì€ ê´€ë ¨ì„± (0.8-0.9)**
        - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì˜ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ëŒ€ì²´ë¡œ ì˜ ì´í•´í–ˆìŠµë‹ˆë‹¤
        - ì†Œìˆ˜ì˜ ë¶€ê°€ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆì§€ë§Œ ìœ ìš©í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ë¯¸ì„¸ ì¡°ì • ë°©ì•ˆ:**
        - ë‹µë³€ì„ ë” ê°„ê²°í•˜ê²Œ ë§Œë“¤ì–´ í•µì‹¬ ì§‘ì¤‘ë„ í–¥ìƒ
        - ì§ˆë¬¸ í‚¤ì›Œë“œì— ë” ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ” ë‹µë³€ êµ¬ì¡°
        """
        technical_details = (
            f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì•½ê°„ì˜ ì—¬ë¶„ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ê´€ë ¨ì„± (0.6-0.8)**
        - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì§€ë§Œ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆê±°ë‚˜ í•µì‹¬ì„ ì™„ì „íˆ ë‹¤ë£¨ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì— ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ê°œì„  ë°©ì•ˆ:**
        - ì§ˆë¬¸ ë¶„ì„ ë‹¨ê³„ ê°•í™” (í‚¤ì›Œë“œ ì¶”ì¶œ, ì˜ë„ ë¶„ë¥˜)
        - ë‹µë³€ ìƒì„± ì „ ì§ˆë¬¸ ì¬í™•ì¸ ë‹¨ê³„ ì¶”ê°€
        - ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª… ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‹µë³€
        - "ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì‹œì˜¤" í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        """
        technical_details = f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê°„ì ‘ì  ê´€ë ¨ì„±ì„ ê°€ì§‘ë‹ˆë‹¤."

    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ê´€ë ¨ì„± (0.4-0.6)**
        - ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ë†“ì³¤ìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ë‹µë³€í–ˆê±°ë‚˜ ë„ˆë¬´ ì¼ë°˜ì ì…ë‹ˆë‹¤
        - ì§ˆë¬¸ìì˜ ì‹¤ì œ ë‹ˆì¦ˆë¥¼ ì œëŒ€ë¡œ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ì¦‰ì‹œ ê°œì„  í•„ìš”:**
        - ì§ˆë¬¸ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ (Few-shot ì˜ˆì‹œ ì¶”ê°€)
        - ë‹µë³€ ìƒì„± ì „ ì§ˆë¬¸ í‚¤ì›Œë“œ ëª…ì‹œì  í™•ì¸
        - ë” êµ¬ì²´ì ì´ê³  ì§ì ‘ì ì¸ ë‹µë³€ ìŠ¤íƒ€ì¼ë¡œ ë³€ê²½
        - ì§ˆë¬¸ ìœ í˜•ë³„ ë‹µë³€ í…œí”Œë¦¿ ë„ì…
        """
        technical_details = (
            f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì— ì¤‘ëŒ€í•œ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤."
        )

    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ê´€ë ¨ì„± (<0.4)**
        - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê±°ì˜ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤
        - ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œì— ëŒ€í•´ ë‹µë³€í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤
        - ì§ˆë¬¸ ì´í•´ ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ì‹œìŠ¤í…œ ì¬ì„¤ê³„ í•„ìš”:**
        - ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ì´í•´ ëª¨ë“ˆ ì™„ì „ ì¬êµ¬ì¶•
        - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë©´ ì¬ê²€í† 
        - ì§ˆë¬¸-ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ êµì²´
        - ë‹¤ë¥¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ê³ ë ¤
        """
        technical_details = (
            f"ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„±: {score:.1%}. ì‹œìŠ¤í…œì´ ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        )

    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_recall_analysis_actual(score):
    """Context Recall ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""

    if score >= 0.9:
        base_analysis = """
        **ğŸ” íƒì›”í•œ ê²€ìƒ‰ ì™„ì„±ë„ (0.9+)**
        - Ground truthì˜ í•µì‹¬ ì •ë³´ê°€ ëª¨ë‘ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - í•„ìš”í•œ ì •ë³´ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ì•Šê³  ì™„ë²½í•˜ê²Œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ë§¤ìš° íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì™„ë²½í•œ ê²€ìƒ‰ ì„±ëŠ¥ì…ë‹ˆë‹¤. í˜„ì¬ ê²€ìƒ‰ ì „ëµì„ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = (
            f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    elif score >= 0.8:
        base_analysis = """
        **âœ… ìš°ìˆ˜í•œ ê²€ìƒ‰ ì™„ì„±ë„ (0.8-0.9)**
        - Ground truthì˜ ëŒ€ë¶€ë¶„ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤
        - ì£¼ìš” ì •ë³´ëŠ” ëª¨ë‘ í¬í•¨ë˜ì—ˆê³ , ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ë§Œ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ì „ë°˜ì ìœ¼ë¡œ íš¨ê³¼ì ì¸ ì •ë³´ ê²€ìƒ‰ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ê²€ìƒ‰ í–¥ìƒ ë°©ì•ˆ:**
        - ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ì–‘í™” (ë™ì˜ì–´, ê´€ë ¨ì–´ ì¶”ê°€)
        - ê²€ìƒ‰ ë²”ìœ„ ì†Œí­ í™•ì¥
        - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ì  ê²€ìƒ‰) ë„ì…
        """
        technical_details = (
            f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ëŒ€ë¶€ë¶„ì˜ ì¤‘ìš” ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ê²€ìƒ‰ ì™„ì„±ë„ (0.6-0.8)**
        - Ground truthì˜ ì¼ë¶€ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤
        - ì¤‘ìš”í•œ ì •ë³´ê°€ ì¼ë¶€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤
        - ê²€ìƒ‰ ì „ëµì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ê²€ìƒ‰ ê°œì„  ë°©ì•ˆ:**
        - ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥ ë° ë‹¤ê°í™”
        - ê²€ìƒ‰ ê¹Šì´ ì¦ê°€ (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)
        - ë‹¤ë‹¨ê³„ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ë„ì…
        - ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ê³ ë ¤
        - ì˜ë¯¸ì  ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¡°ì •
        """
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ê´€ë ¨ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ê²€ìƒ‰ ì™„ì„±ë„ (0.4-0.6)**
        - Ground truthì˜ ìƒë‹¹ ë¶€ë¶„ì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
        - ì¤‘ìš”í•œ ì •ë³´ê°€ ë§ì´ ëˆ„ë½ë˜ì–´ ë‹µë³€ í’ˆì§ˆì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ê·¼ë³¸ì  ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¬ê²€í†  í•„ìš”:**
        - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì „ë©´ ì¬í‰ê°€
        - ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ê³ ë ¤
        - ë¬¸ì„œ ì²­í‚¹ ì „ëµ ì¬ì„¤ê³„
        - ê²€ìƒ‰ ì¸ë±ìŠ¤ í’ˆì§ˆ ì ê²€
        - ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ë³‘í–‰ ì‚¬ìš©
        """
        technical_details = f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ê²€ìƒ‰ ì™„ì„±ë„ (<0.4)**
        - Ground truthì˜ ëŒ€ë¶€ë¶„ì´ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤
        - ì´ ìˆ˜ì¤€ì—ì„œëŠ” ìœ ìš©í•œ ë‹µë³€ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ë©´ ì¬êµ¬ì¶• í•„ìš”:**
        - ê²€ìƒ‰ ì•„í‚¤í…ì²˜ ì™„ì „ ì¬ì„¤ê³„
        - ë‹¤ë¥¸ ê²€ìƒ‰ ê¸°ìˆ  ìŠ¤íƒ ë„ì…
        - ë¬¸ì„œ ì „ì²˜ë¦¬ ê³¼ì • ì¬ê²€í† 
        - ê²€ìƒ‰ ëª¨ë¸ êµì²´
        - ì „ë¬¸ê°€ ì»¨ì„¤íŒ… ê³ ë ¤
        """
        technical_details = (
            f"ì •ë³´ ê²€ìƒ‰ ì™„ì„±ë„: {score:.1%}. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ì²´ê°€ ê¸°ëŠ¥í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."
        )

    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_precision_analysis_actual(score):
    """Context Precision ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""

    if score >= 0.9:
        base_analysis = """
        **ğŸ¯ íƒì›”í•œ ê²€ìƒ‰ ì •í™•ë„ (0.9+)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ë§¤ìš° ì •í™•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤
        - ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ê±°ì˜ ì—†ì–´ ë§¤ìš° íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì…ë‹ˆë‹¤
        - ë…¸ì´ì¦ˆ ì—†ëŠ” ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì™„ë²½í•œ ê²€ìƒ‰ ì •í™•ë„ì…ë‹ˆë‹¤. í˜„ì¬ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = (
            f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ê±°ì˜ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ê°€ ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        )

    elif score >= 0.8:
        base_analysis = """
        **âœ… ë†’ì€ ê²€ìƒ‰ ì •í™•ë„ (0.8-0.9)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì˜ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤
        - ëŒ€ë¶€ë¶„ì˜ ì •ë³´ê°€ ìœ ìš©í•˜ë©° ì†Œìˆ˜ì˜ ë¶€ê°€ ì •ë³´ë§Œ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ì •í™•ë„ í–¥ìƒ ë°©ì•ˆ:**
        - ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
        - ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§ ê·œì¹™ ì„¸ë°€í™”
        - ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì •
        """
        technical_details = (
            f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ì†ŒëŸ‰ì˜ ë¶€ê°€ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ê²€ìƒ‰ ì •í™•ë„ (0.6-0.8)**
        - ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ìˆìŠµë‹ˆë‹¤
        - ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤
        - ê²€ìƒ‰ í•„í„°ë§ì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ì •í™•ë„ ê°œì„  ë°©ì•ˆ:**
        - ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ê°•í™”
        - ê´€ë ¨ì„± ì ìˆ˜ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        - ì¤‘ë³µ ì œê±° ë° ë…¸ì´ì¦ˆ í•„í„°ë§ ê°œì„ 
        - ì¿¼ë¦¬-ë¬¸ì„œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì •êµí™”
        - ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ë„ì…
        """
        technical_details = f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì  ê´€ë ¨ì„±ì„ ê°€ì§‘ë‹ˆë‹¤."

    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„ (0.4-0.6)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ë¬´ê´€í•œ ì •ë³´ê°€ ìƒë‹¹íˆ ë§ìŠµë‹ˆë‹¤
        - ë…¸ì´ì¦ˆê°€ ë§ì•„ ë‹µë³€ í’ˆì§ˆì— ë¶€ì •ì  ì˜í–¥ì„ ì¤ë‹ˆë‹¤
        - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒì´ ì‹œê¸‰í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ê²€ìƒ‰ í•„í„°ë§ ê°•í™” í•„ìš”:**
        - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì¬ì„¤ê³„
        - ë” ì—„ê²©í•œ ê´€ë ¨ì„± ê¸°ì¤€ ì ìš©
        - ë‹¤ë‹¨ê³„ í•„í„°ë§ í”„ë¡œì„¸ìŠ¤ ë„ì…
        - ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ëª¨ë¸ ê°œì„ 
        - ë¶ˆìš©ì–´ ë° ë…¸ì´ì¦ˆ ì œê±° ê°•í™”
        """
        technical_details = (
            f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ìƒë‹¹ëŸ‰ì˜ ë¬´ê´€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„ (<0.4)**
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ëŒ€ë¶€ë¶„ì´ ì§ˆë¬¸ê³¼ ë¬´ê´€í•©ë‹ˆë‹¤
        - ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì§ˆë¬¸ì„ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
        - ì´ëŸ° ë‚®ì€ ì •í™•ë„ë¡œëŠ” ìœ ìš©í•œ ë‹µë³€ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ë©´ ì¬ê²€í†  í•„ìš”:**
        - ê²€ìƒ‰ ì—”ì§„ ì „ì²´ êµì²´ ê³ ë ¤
        - ì¿¼ë¦¬ ì´í•´ ëª¨ë“ˆ ì¬êµ¬ì¶•
        - ë¬¸ì„œ ì¸ë±ì‹± ë°©ì‹ ê·¼ë³¸ì  ë³€ê²½
        - ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì²´ê³„ ì¬ì„¤ê³„
        - ì™¸ë¶€ ê²€ìƒ‰ ì†”ë£¨ì…˜ ë„ì… ê²€í† 
        """
        technical_details = (
            f"ê²€ìƒ‰ ì •í™•ë„: {score:.1%}. ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."
        )

    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def generate_answer_correctness_analysis_actual(score):
    """Answer Correctness ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ ë¶„ì„"""
    base_analysis = ""
    improvement_tips = ""
    technical_details = ""

    if score >= 0.9:
        base_analysis = """
        **ğŸŒŸ ì™„ë²½í•œ ì •í™•ë„ (0.9+)**
        - ìƒì„±ëœ ë‹µë³€ì´ ì •ë‹µ(ground truth)ê³¼ ê±°ì˜ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•©ë‹ˆë‹¤
        - ì˜ë¯¸ì , ì‚¬ì‹¤ì  ì¼ì¹˜ë„ê°€ ëª¨ë‘ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤
        - ì‚¬ìš©ìê°€ ê¸°ëŒ€í•˜ëŠ” ë‹µë³€ì„ ì •í™•íˆ ì œê³µí–ˆìŠµë‹ˆë‹¤
        """
        improvement_tips = "âœ… ì´ìƒì ì¸ ì •í™•ë„ì…ë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”."
        technical_details = f"ì •ë‹µ ì¼ì¹˜ë„: {score:.1%}. ë§¤ìš° ë†’ì€ ì •í™•ì„±ì„ ë³´ì…ë‹ˆë‹¤."

    elif score >= 0.8:
        base_analysis = """
        **âœ… ë†’ì€ ì •í™•ë„ (0.8-0.9)**
        - ë‹µë³€ì´ ì •ë‹µê³¼ ì˜ ì¼ì¹˜í•©ë‹ˆë‹¤
        - í•µì‹¬ ì •ë³´ëŠ” ëª¨ë‘ í¬í•¨ë˜ì—ˆê³ , í‘œí˜„ ë°©ì‹ì—ë§Œ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ì „ë°˜ì ìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ë‹µë³€ í’ˆì§ˆì…ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ’¡ **ë¯¸ì„¸ ì¡°ì • ë°©ì•ˆ:**
        - ë‹µë³€ í˜•ì‹ì„ ì •ë‹µê³¼ ë” ìœ ì‚¬í•˜ê²Œ ì¡°ì •
        - í•µì‹¬ í‚¤ì›Œë“œ ì‚¬ìš© ë¹ˆë„ ê°œì„ 
        - ë¬¸ì²´ë‚˜ í†¤ ì¼ì¹˜ë„ í–¥ìƒ
        """
        technical_details = f"ì •ë‹µ ì¼ì¹˜ë„: {score:.1%}. í‘œí˜„ìƒ ì°¨ì´ê°€ ì•½ê°„ ìˆì§€ë§Œ ë‚´ìš©ì€ ì •í™•í•©ë‹ˆë‹¤."

    elif score >= 0.6:
        base_analysis = """
        **âš ï¸ ë³´í†µ ì •í™•ë„ (0.6-0.8)**
        - ë‹µë³€ì´ ì •ë‹µê³¼ ë¶€ë¶„ì ìœ¼ë¡œ ì¼ì¹˜í•©ë‹ˆë‹¤
        - ì£¼ìš” ë‚´ìš©ì€ í¬í•¨ë˜ì—ˆì§€ë§Œ ì¼ë¶€ ì •ë³´ê°€ ëˆ„ë½ë˜ê±°ë‚˜ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ë‹µë³€ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ”§ **ê°œì„  ë°©ì•ˆ:**
        - Ground truthì™€ ìœ ì‚¬í•œ ë‹µë³€ ìŠ¤íƒ€ì¼ í•™ìŠµ
        - í•µì‹¬ ì •ë³´ ëˆ„ë½ ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì‘ì„±
        - ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ ë‹¨ê³„ ì¶”ê°€
        - Few-shot ì˜ˆì‹œì— ì •ë‹µê³¼ ìœ ì‚¬í•œ í˜•ì‹ í¬í•¨
        """
        technical_details = f"ì •ë‹µ ì¼ì¹˜ë„: {score:.1%}. ì•½ {(1-score)*100:.0f}%ì˜ ì •ë³´ê°€ ë¶€ì •í™•í•˜ê±°ë‚˜ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    elif score >= 0.4:
        base_analysis = """
        **âŒ ë‚®ì€ ì •í™•ë„ (0.4-0.6)**
        - ë‹µë³€ì´ ì •ë‹µê³¼ ìƒë‹¹í•œ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤
        - ì¤‘ìš”í•œ ì •ë³´ê°€ ë§ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì˜ëª»ëœ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤
        - ë‹µë³€ì˜ ì‹ ë¢°ì„±ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸš¨ **ì¦‰ì‹œ ê°œì„  í•„ìš”:**
        - ë‹µë³€ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì „ë°˜ì  ì¬ê²€í† 
        - Ground truth ê¸°ë°˜ í›ˆë ¨ ë°ì´í„° ë³´ê°•
        - ë‹µë³€ ê²€ì¦ ì‹œìŠ¤í…œ ë„ì…
        - ì •í™•ì„± ìš°ì„ ì˜ ë³´ìˆ˜ì  ë‹µë³€ ì „ëµ ì±„íƒ
        - ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì • (Temperature ë‚®ì¶”ê¸°)
        """
        technical_details = f"ì •ë‹µ ì¼ì¹˜ë„: {score:.1%}. ë‹µë³€ í’ˆì§ˆì´ ê¸°ëŒ€ì¹˜ì— ë¯¸ì¹˜ì§€ ëª»í•©ë‹ˆë‹¤."

    else:
        base_analysis = """
        **ğŸ”´ ë§¤ìš° ë‚®ì€ ì •í™•ë„ (<0.4)**
        - ë‹µë³€ì´ ì •ë‹µê³¼ ê±°ì˜ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - ì˜ëª»ëœ ì •ë³´ê°€ ëŒ€ë¶€ë¶„ì´ê±°ë‚˜ ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©ì…ë‹ˆë‹¤
        - ì´ëŸ° ìˆ˜ì¤€ì˜ ë‹µë³€ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
        """
        improvement_tips = """
        ğŸ†˜ **ì‹œìŠ¤í…œ ì „ë©´ ì¬ê²€í†  í•„ìš”:**
        - RAG ì‹œìŠ¤í…œ ì „ì²´ ì•„í‚¤í…ì²˜ ì¬ì„¤ê³„
        - ë‹¤ë¥¸ LLM ëª¨ë¸ ì‚¬ìš© ê²€í† 
        - ë‹µë³€ ìƒì„± ë¡œì§ ê·¼ë³¸ì  ë³€ê²½
        - ì •ë‹µ ê¸°ë°˜ supervised learning ë„ì…
        - ì „ë¬¸ê°€ ë¦¬ë·° ì‹œìŠ¤í…œ êµ¬ì¶•
        """
        technical_details = f"ì •ë‹µ ì¼ì¹˜ë„: {score:.1%}. ì‹œìŠ¤í…œì´ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤."

    return f"{base_analysis}\n\n{improvement_tips}\n\n**ğŸ“Š ê¸°ìˆ ì  ë¶„ì„:** {technical_details}"


def show_metric_distribution_actual(individual_scores, evaluation_data):
    """ì‹¤ì œ í‰ê°€ëœ ë°ì´í„°ì˜ ë©”íŠ¸ë¦­ ë¶„í¬"""
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„")

    if not individual_scores:
        st.warning("ê°œë³„ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # DataFrame ìƒì„±
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    
    # answer_correctnessê°€ í¬í•¨ëœ ì ìˆ˜ê°€ ìˆëŠ”ì§€ í™•ì¸
    has_answer_correctness = any("answer_correctness" in score for score in individual_scores)
    if has_answer_correctness:
        metrics.append("answer_correctness")
    
    data = {"QA": [f"Q{i+1}" for i in range(len(individual_scores))]}

    for metric in metrics:
        data[metric] = [score.get(metric, 0) for score in individual_scores]

    df = pd.DataFrame(data)

    # íˆíŠ¸ë§µ
    st.markdown("#### ğŸ”¥ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ")

    heatmap_data = df[metrics].values

    fig = go.Figure(
        data=go.Heatmap(
            z=heatmap_data,
            x=[m.replace("_", " ").title() for m in metrics],
            y=df["QA"],
            colorscale="RdYlGn",
            colorbar=dict(title="ì ìˆ˜"),
        )
    )

    fig.update_layout(title="ì‹¤ì œ í‰ê°€ëœ QAë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥", height=400)

    st.plotly_chart(fig, use_container_width=True)

    # ë¶„í¬ í†µê³„
    st.markdown("#### ğŸ“ˆ ë¶„í¬ í†µê³„")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**í‰ê·  ì ìˆ˜**")
        for metric in metrics:
            avg_score = df[metric].mean()
            st.text(f"{metric.replace('_', ' ').title()}: {avg_score:.3f}")

    with col2:
        st.markdown("**í‘œì¤€í¸ì°¨**")
        for metric in metrics:
            std_score = df[metric].std()
            st.text(f"{metric.replace('_', ' ').title()}: {std_score:.3f}")


def show_pattern_analysis_actual(individual_scores, evaluation_data):
    """ì‹¤ì œ í‰ê°€ ë°ì´í„°ì˜ íŒ¨í„´ ë¶„ì„"""
    st.subheader("ğŸ¯ ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„")

    qa_count = len(individual_scores)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### ğŸ“ í‰ê°€ ê°œìš”")
        st.metric("ì‹¤ì œ í‰ê°€ëœ QA ê°œìˆ˜", qa_count)

        # í‰ê°€ ì‹œê°„
        timestamp = evaluation_data.get("timestamp", "")
        if timestamp:
            try:
                dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
                formatted_time = dt.strftime("%Yë…„ %mì›” %dì¼ %H:%M")
                st.text(f"í‰ê°€ ì‹œê°„: {formatted_time}")
            except:
                st.text(f"í‰ê°€ ì‹œê°„: {timestamp}")

    with col2:
        st.markdown("#### ğŸ“Š ì„±ëŠ¥ ìš”ì•½")
        ragas_score = evaluation_data.get("ragas_score", 0)
        st.metric("ì „ì²´ RAGAS ì ìˆ˜", f"{ragas_score:.3f}")

        # ìµœê³ /ìµœì € ë©”íŠ¸ë¦­
        metrics = [
            "faithfulness",
            "answer_relevancy",
            "context_recall",
            "context_precision",
        ]
        
        # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if "answer_correctness" in evaluation_data:
            metrics.append("answer_correctness")
        
        scores = {m: evaluation_data.get(m, 0) for m in metrics}

        if scores:
            best_metric = max(scores, key=scores.get)
            worst_metric = min(scores, key=scores.get)

            st.text(
                f"ìµœê³  ì„±ëŠ¥: {best_metric.replace('_', ' ').title()} ({scores[best_metric]:.3f})"
            )
            st.text(
                f"ê°œì„  í•„ìš”: {worst_metric.replace('_', ' ').title()} ({scores[worst_metric]:.3f})"
            )

    # ê°œì„  ì œì•ˆ
    st.markdown("#### ğŸ’¡ ì´ í‰ê°€ì— ëŒ€í•œ ê°œì„  ì œì•ˆ")

    suggestions = []

    if evaluation_data.get("faithfulness", 0) < 0.7:
        suggestions.append(
            "ğŸ¯ Faithfulness ê°œì„ : ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„ ê°•í™”, í™˜ê° ë°©ì§€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"
        )

    if evaluation_data.get("answer_relevancy", 0) < 0.7:
        suggestions.append(
            "ğŸ¯ Answer Relevancy ê°œì„ : ì§ˆë¬¸ ì˜ë„ íŒŒì•… ê°•í™”, ê°„ê²°í•œ ë‹µë³€ ìƒì„±"
        )

    if evaluation_data.get("context_recall", 0) < 0.7:
        suggestions.append(
            "ğŸ¯ Context Recall ê°œì„ : ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€, ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ í™œìš©"
        )

    if evaluation_data.get("context_precision", 0) < 0.7:
        suggestions.append(
            "ğŸ¯ Context Precision ê°œì„ : ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§, ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ"
        )

    if not suggestions:
        suggestions.append(
            "âœ… ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì–‘í˜¸í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤! í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”."
        )

    for suggestion in suggestions:
        st.info(suggestion)


# Historical í˜ì´ì§€ì™€ì˜ ì—°ë™ì„ ìœ„í•œ í•¨ìˆ˜
def set_selected_evaluation(evaluation_id):
    """Historical í˜ì´ì§€ì—ì„œ íŠ¹ì • í‰ê°€ë¥¼ ì„ íƒí–ˆì„ ë•Œ í˜¸ì¶œ"""
    all_evaluations = load_all_evaluations()
    for i, eval_data in enumerate(all_evaluations):
        if eval_data["id"] == evaluation_id:
            st.session_state.selected_evaluation_index = i
            break


# =============================================================================
# ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ë“¤
# =============================================================================

def show_eda_analysis(all_evaluations, selected_evaluation):
    """EDA (Exploratory Data Analysis) - íƒìƒ‰ì  ë°ì´í„° ë¶„ì„"""
    st.header("ğŸ“Š EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)")
    
    if len(all_evaluations) < 2:
        st.warning("ğŸ“Š EDA ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ í‰ê°€ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ì¤€ë¹„
    df = pd.DataFrame(all_evaluations)
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness']
    
    # ê¸°ë³¸ í†µê³„ ì •ë³´
    st.subheader("ğŸ“ˆ ê¸°ì´ˆ í†µê³„ ìš”ì•½")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸ“Š ë°ì´í„°ì…‹ ê°œìš”**")
        st.metric("ì´ í‰ê°€ ìˆ˜", len(df))
        st.metric("í‰ê°€ ê¸°ê°„", f"{df['timestamp'].min()[:10]} ~ {df['timestamp'].max()[:10]}")
        
        # RAGAS ì ìˆ˜ ë¶„í¬
        ragas_scores = df['ragas_score'].dropna()
        if len(ragas_scores) > 0:
            st.metric("í‰ê·  RAGAS ì ìˆ˜", f"{ragas_scores.mean():.3f}")
            st.metric("RAGAS ì ìˆ˜ ë²”ìœ„", f"{ragas_scores.min():.3f} ~ {ragas_scores.max():.3f}")
    
    with col2:
        st.write("**ğŸ“‹ ê¸°ì´ˆ í†µê³„ëŸ‰**")
        stats_df = df[metrics].describe().round(3)
        st.dataframe(stats_df, use_container_width=True)
    
    # ë©”íŠ¸ë¦­ ë¶„í¬ ì‹œê°í™”
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ì‹œê°í™”") 
    
    col1, col2 = st.columns(2)
    
    with col1:
        # íˆìŠ¤í† ê·¸ë¨
        selected_metric = st.selectbox("íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë³¼ ë©”íŠ¸ë¦­", metrics, key="eda_hist_metric")
        
        fig = px.histogram(
            df, 
            x=selected_metric, 
            nbins=20,
            title=f"{selected_metric} ë¶„í¬",
            color_discrete_sequence=['#1f77b4']
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ë°•ìŠ¤í”Œë¡¯
        fig = go.Figure()
        
        for metric in metrics:
            fig.add_trace(go.Box(
                y=df[metric].dropna(),
                name=metric,
                boxpoints='outliers'
            ))
        
        fig.update_layout(
            title="ë©”íŠ¸ë¦­ë³„ ë°•ìŠ¤í”Œë¡¯ (ì´ìƒì¹˜ í¬í•¨)",
            yaxis_title="ì ìˆ˜",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ìƒê´€ê´€ê³„ ë¶„ì„
    st.subheader("ğŸ”— ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„")
    
    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
    correlation_matrix = df[metrics].corr()
    
    fig = px.imshow(
        correlation_matrix,
        text_auto=True,
        aspect="auto",
        color_continuous_scale='RdBu_r',
        title="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ"
    )
    fig.update_layout(height=500)
    st.plotly_chart(fig, use_container_width=True)
    
    # ìƒê´€ê´€ê³„ í•´ì„
    st.write("**ğŸ” ìƒê´€ê´€ê³„ í•´ì„:**")
    strong_correlations = []
    for i in range(len(metrics)):
        for j in range(i+1, len(metrics)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > 0.7:
                relationship = "ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„" if corr_val > 0.7 else "ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„"
                strong_correlations.append(f"â€¢ {metrics[i]} â†” {metrics[j]}: {relationship} ({corr_val:.3f})")
    
    if strong_correlations:
        for corr in strong_correlations:
            st.info(corr)
    else:
        st.info("ğŸ“Š ê°•í•œ ìƒê´€ê´€ê³„(|r| > 0.7)ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤
    st.subheader("ğŸ“ˆ ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤")
    
    # ì„ íƒëœ ë©”íŠ¸ë¦­ë“¤ë¡œ ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    selected_metrics = st.multiselect(
        "ë¶„ì„í•  ë©”íŠ¸ë¦­ ì„ íƒ (2-4ê°œ ê¶Œì¥)", 
        metrics, 
        default=metrics[:3],
        key="eda_scatter_metrics"
    )
    
    if len(selected_metrics) >= 2:
        fig = px.scatter_matrix(
            df,
            dimensions=selected_metrics,
            title="ì„ íƒëœ ë©”íŠ¸ë¦­ë“¤ì˜ ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤",
            height=600
        )
        st.plotly_chart(fig, use_container_width=True)


def show_time_series_analysis(all_evaluations, selected_evaluation):
    """ì‹œê³„ì—´ ë¶„ì„ - ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”"""
    st.header("ğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„")
    
    if len(all_evaluations) < 3:
        st.warning("ğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 3ê°œ ì´ìƒì˜ í‰ê°€ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ì¤€ë¹„
    df = pd.DataFrame(all_evaluations)
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness', 'ragas_score']
    
    # ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
    st.subheader("ğŸ“Š ì„±ëŠ¥ íŠ¸ë Œë“œ")
    
    # ë©”íŠ¸ë¦­ ì„ íƒ
    selected_metrics = st.multiselect(
        "ë¶„ì„í•  ë©”íŠ¸ë¦­ ì„ íƒ", 
        metrics, 
        default=['ragas_score', 'faithfulness', 'answer_relevancy'],
        key="ts_metrics"
    )
    
    if selected_metrics:
        fig = go.Figure()
        
        for metric in selected_metrics:
            fig.add_trace(go.Scatter(
                x=df['timestamp'],
                y=df[metric],
                mode='lines+markers',
                name=metric,
                line=dict(width=2),
                marker=dict(size=8)
            ))
        
        fig.update_layout(
            title="ì‹œê°„ì— ë”°ë¥¸ ë©”íŠ¸ë¦­ ë³€í™”",
            xaxis_title="ì‹œê°„",
            yaxis_title="ì ìˆ˜",
            height=500,
            hovermode='x unified'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ì´ë™í‰ê·  ë¶„ì„
    st.subheader("ğŸ“Š ì´ë™í‰ê·  ë¶„ì„")
    
    window_size = st.slider("ì´ë™í‰ê·  ìœˆë„ìš° í¬ê¸°", 2, min(10, len(df)-1), 3, key="ts_window")
    
    if len(selected_metrics) > 0:
        fig = go.Figure()
        
        for metric in selected_metrics:
            # ì›ë³¸ ë°ì´í„°
            fig.add_trace(go.Scatter(
                x=df['timestamp'],
                y=df[metric],
                mode='markers',
                name=f'{metric} (ì›ë³¸)',
                opacity=0.5,
                marker=dict(size=6)
            ))
            
            # ì´ë™í‰ê· 
            moving_avg = df[metric].rolling(window=window_size, center=True).mean()
            fig.add_trace(go.Scatter(
                x=df['timestamp'],
                y=moving_avg,
                mode='lines',
                name=f'{metric} (ì´ë™í‰ê· )',
                line=dict(width=3)
            ))
        
        fig.update_layout(
            title=f"{window_size}-í¬ì¸íŠ¸ ì´ë™í‰ê· ",
            xaxis_title="ì‹œê°„",
            yaxis_title="ì ìˆ˜",
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ë³€í™”ìœ¨ ë¶„ì„
    st.subheader("ğŸ“ˆ ë³€í™”ìœ¨ ë¶„ì„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸ“Š ìµœê·¼ ê°œì„ /ì•…í™” ì¶”ì„¸**")
        
        for metric in metrics:
            if len(df[metric].dropna()) >= 2:
                recent_values = df[metric].dropna().tail(3)
                if len(recent_values) >= 2:
                    change = recent_values.iloc[-1] - recent_values.iloc[0]
                    change_pct = (change / recent_values.iloc[0]) * 100 if recent_values.iloc[0] != 0 else 0
                    
                    if abs(change_pct) > 5:  # 5% ì´ìƒ ë³€í™”
                        emoji = "ğŸ“ˆ" if change > 0 else "ğŸ“‰"
                        st.metric(
                            metric, 
                            f"{recent_values.iloc[-1]:.3f}",
                            f"{change:+.3f} ({change_pct:+.1f}%)"
                        )
    
    with col2:
        # ë³€ë™ì„± ë¶„ì„
        st.write("**ğŸ“Š ë³€ë™ì„± ë¶„ì„ (í‘œì¤€í¸ì°¨)**")
        
        volatility_data = []
        for metric in metrics:
            std_val = df[metric].std()
            mean_val = df[metric].mean()
            cv = (std_val / mean_val) * 100 if mean_val != 0 else 0  # ë³€ë™ê³„ìˆ˜
            volatility_data.append({
                'Metric': metric,
                'Standard Deviation': f"{std_val:.3f}",
                'Coefficient of Variation': f"{cv:.1f}%"
            })
        
        volatility_df = pd.DataFrame(volatility_data)
        st.dataframe(volatility_df, use_container_width=True, hide_index=True)
    
    # ì£¼ê¸°ì„± ë¶„ì„
    if len(df) >= 7:
        st.subheader("ğŸ”„ ì£¼ê¸°ì„± ë¶„ì„")
        
        # ìš”ì¼ë³„ íŒ¨í„´ (ì‹œê°„ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        df['weekday'] = df['timestamp'].dt.day_name()
        df['hour'] = df['timestamp'].dt.hour
        
        col1, col2 = st.columns(2)
        
        with col1:
            if len(df['weekday'].value_counts()) > 1:
                weekday_avg = df.groupby('weekday')['ragas_score'].mean().reset_index()
                
                fig = px.bar(
                    weekday_avg,
                    x='weekday',
                    y='ragas_score',
                    title="ìš”ì¼ë³„ í‰ê·  RAGAS ì ìˆ˜"
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if len(df['hour'].value_counts()) > 1:
                hour_avg = df.groupby('hour')['ragas_score'].mean().reset_index()
                
                fig = px.line(
                    hour_avg,
                    x='hour',
                    y='ragas_score',
                    title="ì‹œê°„ëŒ€ë³„ í‰ê·  RAGAS ì ìˆ˜"
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)


def show_anomaly_detection(all_evaluations, selected_evaluation):
    """ì´ìƒì¹˜ íƒì§€ - ë¹„ì •ìƒì ì¸ í‰ê°€ ê²°ê³¼ ì‹ë³„"""
    st.header("ğŸš¨ ì´ìƒì¹˜ íƒì§€")
    
    if len(all_evaluations) < 5:
        st.warning("ğŸš¨ ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 5ê°œ ì´ìƒì˜ í‰ê°€ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ì¤€ë¹„
    df = pd.DataFrame(all_evaluations)
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness']
    
    # ì´ìƒì¹˜ íƒì§€ ë°©ë²• ì„ íƒ
    st.subheader("ğŸ”§ ì´ìƒì¹˜ íƒì§€ ì„¤ì •")
    
    detection_method = st.selectbox(
        "íƒì§€ ë°©ë²• ì„ íƒ",
        ["IQR (Interquartile Range)", "Z-Score", "Isolation Forest"],
        key="anomaly_method"
    )
    
    # ë©”íŠ¸ë¦­ ì„ íƒ
    selected_metrics = st.multiselect(
        "ë¶„ì„í•  ë©”íŠ¸ë¦­ ì„ íƒ",
        metrics,
        default=metrics,
        key="anomaly_metrics"
    )
    
    if not selected_metrics:
        st.warning("ë¶„ì„í•  ë©”íŠ¸ë¦­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return
    
    # ì´ìƒì¹˜ íƒì§€ ì‹¤í–‰
    anomalies = {}
    anomaly_scores = {}
    
    for metric in selected_metrics:
        metric_data = df[metric].dropna()
        
        if detection_method == "IQR (Interquartile Range)":
            Q1 = metric_data.quantile(0.25)
            Q3 = metric_data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = metric_data[(metric_data < lower_bound) | (metric_data > upper_bound)]
            anomalies[metric] = outliers.index.tolist()
            
            # ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
            scores = []
            for val in metric_data:
                if val < lower_bound:
                    scores.append(abs(val - lower_bound) / IQR)
                elif val > upper_bound:
                    scores.append(abs(val - upper_bound) / IQR)
                else:
                    scores.append(0)
            anomaly_scores[metric] = scores
            
        elif detection_method == "Z-Score":
            z_threshold = st.slider(f"Z-Score ì„ê³„ê°’", 1.5, 3.5, 2.5, 0.1, key=f"zscore_{metric}")
            
            z_scores = np.abs(stats.zscore(metric_data))
            outliers_mask = z_scores > z_threshold
            anomalies[metric] = metric_data[outliers_mask].index.tolist()
            anomaly_scores[metric] = z_scores.tolist()
            
        elif detection_method == "Isolation Forest":
            contamination = st.slider("ì˜¤ì—¼ë„ (ì´ìƒì¹˜ ë¹„ìœ¨)", 0.05, 0.3, 0.1, 0.01, key=f"isolation_{metric}")
            
            isolation_forest = IsolationForest(contamination=contamination, random_state=42)
            outlier_labels = isolation_forest.fit_predict(metric_data.values.reshape(-1, 1))
            
            anomalies[metric] = [i for i, label in enumerate(outlier_labels) if label == -1]
            anomaly_scores[metric] = isolation_forest.decision_function(metric_data.values.reshape(-1, 1)).tolist()
    
    # ì´ìƒì¹˜ ê²°ê³¼ í‘œì‹œ
    st.subheader("ğŸ“Š ì´ìƒì¹˜ íƒì§€ ê²°ê³¼")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸš¨ íƒì§€ëœ ì´ìƒì¹˜ ê°œìˆ˜**")
        for metric in selected_metrics:
            anomaly_count = len(anomalies[metric])
            total_count = len(df[metric].dropna())
            percentage = (anomaly_count / total_count) * 100 if total_count > 0 else 0
            
            st.metric(
                metric,
                f"{anomaly_count}ê°œ",
                f"{percentage:.1f}%"
            )
    
    with col2:
        # ì´ìƒì¹˜ ì ìˆ˜ ë¶„í¬
        st.write("**ğŸ“ˆ ì´ìƒì¹˜ ì ìˆ˜ ë¶„í¬**")
        
        selected_metric_viz = st.selectbox(
            "ì‹œê°í™”í•  ë©”íŠ¸ë¦­",
            selected_metrics,
            key="anomaly_viz_metric"
        )
        
        if selected_metric_viz in anomaly_scores:
            scores = anomaly_scores[selected_metric_viz]
            fig = px.histogram(
                x=scores,
                nbins=20,
                title=f"{selected_metric_viz} ì´ìƒì¹˜ ì ìˆ˜ ë¶„í¬"
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
    
    # ì´ìƒì¹˜ ìƒì„¸ ë¶„ì„
    st.subheader("ğŸ” ì´ìƒì¹˜ ìƒì„¸ ë¶„ì„")
    
    # ì „ì²´ ì´ìƒì¹˜ í‰ê°€ ëª©ë¡
    all_anomaly_indices = set()
    for metric_anomalies in anomalies.values():
        all_anomaly_indices.update(metric_anomalies)
    
    if all_anomaly_indices:
        st.write(f"**ğŸš¨ ì´ {len(all_anomaly_indices)}ê°œì˜ í‰ê°€ì—ì„œ ì´ìƒì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:**")
        
        anomalous_evaluations = []
        for idx in sorted(all_anomaly_indices):
            if idx < len(df):
                eval_data = df.iloc[idx]
                anomalous_metrics = [metric for metric in selected_metrics if idx in anomalies[metric]]
                
                anomalous_evaluations.append({
                    'Evaluation ID': eval_data['id'],
                    'Timestamp': eval_data['timestamp'][:19] if eval_data['timestamp'] else 'N/A',
                    'RAGAS Score': f"{eval_data['ragas_score']:.3f}" if eval_data['ragas_score'] else 'N/A',
                    'Anomalous Metrics': ', '.join(anomalous_metrics)
                })
        
        anomaly_df = pd.DataFrame(anomalous_evaluations)
        st.dataframe(anomaly_df, use_container_width=True, hide_index=True)
        
        # ì´ìƒì¹˜ ì‹œê°í™”
        st.subheader("ğŸ“Š ì´ìƒì¹˜ ì‹œê°í™”")
        
        # ì‹œê³„ì—´ ì´ìƒì¹˜ í‘œì‹œ
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        df_sorted = df.sort_values('timestamp').reset_index(drop=True)
        
        fig = go.Figure()
        
        for metric in selected_metrics:
            # ì •ìƒ ë°ì´í„°
            normal_mask = ~df_sorted.index.isin(anomalies[metric])
            fig.add_trace(go.Scatter(
                x=df_sorted[normal_mask]['timestamp'],
                y=df_sorted[normal_mask][metric],
                mode='markers',
                name=f'{metric} (ì •ìƒ)',
                marker=dict(size=8),
                opacity=0.7
            ))
            
            # ì´ìƒì¹˜ ë°ì´í„°
            anomaly_mask = df_sorted.index.isin(anomalies[metric])
            if anomaly_mask.any():
                fig.add_trace(go.Scatter(
                    x=df_sorted[anomaly_mask]['timestamp'],
                    y=df_sorted[anomaly_mask][metric],
                    mode='markers',
                    name=f'{metric} (ì´ìƒì¹˜)',
                    marker=dict(size=12, symbol='x', line=dict(width=2)),
                ))
        
        fig.update_layout(
            title="ì‹œê°„ì— ë”°ë¥¸ ì´ìƒì¹˜ ë¶„í¬",
            xaxis_title="ì‹œê°„",
            yaxis_title="ì ìˆ˜",
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)
        
    else:
        st.success("âœ… ì„ íƒëœ ë©”íŠ¸ë¦­ì—ì„œ ì´ìƒì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    # ì´ìƒì¹˜ ì›ì¸ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
    st.subheader("ğŸ’¡ ì´ìƒì¹˜ ì›ì¸ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­")
    
    if all_anomaly_indices:
        st.write("**ğŸ” ê°€ëŠ¥í•œ ì›ì¸ë“¤:**")
        st.info("â€¢ **ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ**: ì…ë ¥ ë°ì´í„°ë‚˜ Ground Truthì˜ í’ˆì§ˆ ì´ìŠˆ")
        st.info("â€¢ **ëª¨ë¸ ì„±ëŠ¥ ë³€í™”**: LLM ëª¨ë¸ì´ë‚˜ ì„¤ì •ì˜ ë³€ê²½")
        st.info("â€¢ **í‰ê°€ í™˜ê²½ ë³€í™”**: ë„¤íŠ¸ì›Œí¬ ìƒíƒœ, API ì‘ë‹µ ì‹œê°„ ë“±")
        st.info("â€¢ **íŠ¹ì´í•œ ì§ˆë¬¸ ìœ í˜•**: ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš´ íŠ¹ìˆ˜í•œ ì§ˆë¬¸")
        
        st.write("**ğŸ› ï¸ ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­:**")
        st.info("â€¢ ì´ìƒì¹˜ í‰ê°€ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€í† ")
        st.info("â€¢ í•´ë‹¹ ì‹œì ì˜ í‰ê°€ í™˜ê²½ì´ë‚˜ ì„¤ì • ë³€ê²½ì‚¬í•­ í™•ì¸")
        st.info("â€¢ ë°˜ë³µì ìœ¼ë¡œ ì´ìƒì¹˜ê°€ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ì´ ìˆëŠ”ì§€ ë¶„ì„")
        st.info("â€¢ í•„ìš”ì‹œ í•´ë‹¹ í‰ê°€ ê²°ê³¼ë¥¼ ì œì™¸í•˜ê³  ì¬ë¶„ì„")


def show_advanced_statistics(individual_scores, selected_evaluation, all_evaluations):
    """ê³ ê¸‰ í†µê³„ ë¶„ì„ - ì‹¬í™” í†µê³„ ë¶„ì„ ë° ê²€ì •"""
    st.header("ğŸ” ê³ ê¸‰ í†µê³„ ë¶„ì„")
    
    if not individual_scores or len(all_evaluations) < 2:
        st.warning("ğŸ” ê³ ê¸‰ í†µê³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ê°œë³„ ì ìˆ˜ ë°ì´í„°ì™€ ì—¬ëŸ¬ í‰ê°€ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.")  
        return
    
    # ë°ì´í„° ì¤€ë¹„
    df_all = pd.DataFrame(all_evaluations)
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness']
    
    # í˜„ì¬ í‰ê°€ì˜ ê°œë³„ ì ìˆ˜
    current_scores = pd.DataFrame(individual_scores)
    
    # 1. ì •ê·œì„± ê²€ì •
    st.subheader("ğŸ“Š ì •ê·œì„± ê²€ì •")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸ“ˆ Shapiro-Wilk ì •ê·œì„± ê²€ì •**")
        normality_results = []
        
        for metric in metrics:
            if metric in current_scores.columns:
                metric_data = current_scores[metric].dropna()
                if len(metric_data) >= 3:
                    statistic, p_value = stats.shapiro(metric_data)
                    is_normal = p_value > 0.05
                    
                    normality_results.append({
                        'Metric': metric,
                        'Statistic': f"{statistic:.4f}",
                        'P-Value': f"{p_value:.4f}",
                        'Normal': "âœ… Yes" if is_normal else "âŒ No"
                    })
        
        if normality_results:
            normality_df = pd.DataFrame(normality_results)
            st.dataframe(normality_df, use_container_width=True, hide_index=True)
        else:
            st.info("ê°œë³„ ì ìˆ˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    
    with col2:
        # Q-Q í”Œë¡¯
        st.write("**ğŸ“Š Q-Q í”Œë¡¯ (ì •ê·œì„± ì‹œê°í™”)**")
        
        selected_metric_qq = st.selectbox(
            "Q-Q í”Œë¡¯ì„ ë³¼ ë©”íŠ¸ë¦­",
            [m for m in metrics if m in current_scores.columns],
            key="qq_metric"
        )
        
        if selected_metric_qq and selected_metric_qq in current_scores.columns:
            metric_data = current_scores[selected_metric_qq].dropna()
            if len(metric_data) >= 3:
                # Q-Q í”Œë¡¯ ìƒì„±
                theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(metric_data)))
                sample_quantiles = np.sort(metric_data)
                
                fig = go.Figure()
                
                # ì‹¤ì œ ë°ì´í„° ì ë“¤
                fig.add_trace(go.Scatter(
                    x=theoretical_quantiles,
                    y=sample_quantiles,
                    mode='markers',
                    name='ì‹¤ì œ ë°ì´í„°',
                    marker=dict(size=8)
                ))
                
                # ì´ë¡ ì  ì§ì„ 
                fig.add_trace(go.Scatter(
                    x=theoretical_quantiles,
                    y=theoretical_quantiles * np.std(sample_quantiles) + np.mean(sample_quantiles),
                    mode='lines',
                    name='ì´ë¡ ì  ì •ê·œë¶„í¬',
                    line=dict(color='red', dash='dash')
                ))
                
                fig.update_layout(
                    title=f"{selected_metric_qq} Q-Q í”Œë¡¯",
                    xaxis_title="ì´ë¡ ì  ë¶„ìœ„ìˆ˜",
                    yaxis_title="í‘œë³¸ ë¶„ìœ„ìˆ˜",
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)
    
    # 2. ì‹ ë¢°êµ¬ê°„ ë¶„ì„
    st.subheader("ğŸ“Š ì‹ ë¢°êµ¬ê°„ ë¶„ì„")
    
    confidence_level = st.slider("ì‹ ë¢°ë„ ìˆ˜ì¤€", 0.90, 0.99, 0.95, 0.01, key="confidence_level")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write(f"**ğŸ“ˆ {confidence_level*100:.0f}% ì‹ ë¢°êµ¬ê°„**")
        
        ci_results = []
        for metric in metrics:
            if metric in current_scores.columns:
                metric_data = current_scores[metric].dropna()
                if len(metric_data) >= 2:
                    mean_val = np.mean(metric_data)
                    sem = stats.sem(metric_data)  # í‘œì¤€ì˜¤ì°¨
                    ci = stats.t.interval(confidence_level, df=len(metric_data)-1, loc=mean_val, scale=sem)
                    
                    ci_results.append({
                        'Metric': metric,
                        'Mean': f"{mean_val:.4f}",
                        'Lower CI': f"{ci[0]:.4f}",
                        'Upper CI': f"{ci[1]:.4f}",
                        'Width': f"{ci[1] - ci[0]:.4f}"
                    })
        
        if ci_results:
            ci_df = pd.DataFrame(ci_results)
            st.dataframe(ci_df, use_container_width=True, hide_index=True)
    
    with col2:
        # ì‹ ë¢°êµ¬ê°„ ì‹œê°í™”
        if ci_results:
            metrics_names = [r['Metric'] for r in ci_results]
            means = [float(r['Mean']) for r in ci_results]
            lower_cis = [float(r['Lower CI']) for r in ci_results]
            upper_cis = [float(r['Upper CI']) for r in ci_results]
            
            fig = go.Figure()
            
            for i, metric in enumerate(metrics_names):
                fig.add_trace(go.Scatter(
                    x=[means[i]],
                    y=[metric],
                    error_x=dict(
                        type='data',
                        symmetric=False,
                        array=[upper_cis[i] - means[i]],
                        arrayminus=[means[i] - lower_cis[i]]
                    ),
                    mode='markers',
                    marker=dict(size=10),
                    name=metric
                ))
            
            fig.update_layout(
                title=f"{confidence_level*100:.0f}% ì‹ ë¢°êµ¬ê°„ ì‹œê°í™”",
                xaxis_title="ì ìˆ˜",
                height=400,
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # 3. ê°€ì„¤ ê²€ì •
    if len(all_evaluations) >= 2:
        st.subheader("ğŸ§ª ê°€ì„¤ ê²€ì •")
        
        # í˜„ì¬ í‰ê°€ vs ê³¼ê±° í‰ê°€ë“¤ ë¹„êµ
        st.write("**ğŸ“Š í˜„ì¬ í‰ê°€ vs ê³¼ê±° í‰ê°€ ì„±ëŠ¥ ë¹„êµ**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # t-ê²€ì •
            st.write("**ğŸ“ˆ ë…ë¦½í‘œë³¸ t-ê²€ì •**")
            
            ttest_results = []
            for metric in metrics:
                if metric in current_scores.columns:
                    current_data = current_scores[metric].dropna()
                    historical_data = df_all[df_all['id'] != selected_evaluation['id']][metric].dropna()
                    
                    if len(current_data) >= 2 and len(historical_data) >= 2:
                        # ë“±ë¶„ì‚° ê²€ì •
                        levene_stat, levene_p = stats.levene(current_data, historical_data)
                        equal_var = levene_p > 0.05
                        
                        # t-ê²€ì •
                        t_stat, p_value = stats.ttest_ind(current_data, historical_data, equal_var=equal_var)
                        
                        significance = "ìœ ì˜í•¨" if p_value < 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
                        direction = "í–¥ìƒ" if np.mean(current_data) > np.mean(historical_data) else "í•˜ë½"
                        
                        ttest_results.append({
                            'Metric': metric,
                            'T-Statistic': f"{t_stat:.4f}",
                            'P-Value': f"{p_value:.4f}",
                            'Result': f"{direction} ({significance})"
                        })
            
            if ttest_results:
                ttest_df = pd.DataFrame(ttest_results)
                st.dataframe(ttest_df, use_container_width=True, hide_index=True)
        
        with col2:
            # íš¨ê³¼ í¬ê¸° (Cohen's d)
            st.write("**ğŸ“Š íš¨ê³¼ í¬ê¸° (Cohen's d)**")
            
            effect_size_results = []
            for metric in metrics:
                if metric in current_scores.columns:
                    current_data = current_scores[metric].dropna()
                    historical_data = df_all[df_all['id'] != selected_evaluation['id']][metric].dropna()
                    
                    if len(current_data) >= 2 and len(historical_data) >= 2:
                        # Cohen's d ê³„ì‚°
                        pooled_std = np.sqrt(((len(current_data) - 1) * np.var(current_data, ddof=1) +
                                            (len(historical_data) - 1) * np.var(historical_data, ddof=1)) /
                                           (len(current_data) + len(historical_data) - 2))
                        
                        cohens_d = (np.mean(current_data) - np.mean(historical_data)) / pooled_std
                        
                        # íš¨ê³¼ í¬ê¸° í•´ì„
                        if abs(cohens_d) < 0.2:
                            interpretation = "ì‘ì€ íš¨ê³¼"
                        elif abs(cohens_d) < 0.5:
                            interpretation = "ì¤‘ê°„ íš¨ê³¼"
                        elif abs(cohens_d) < 0.8:
                            interpretation = "í° íš¨ê³¼"
                        else:
                            interpretation = "ë§¤ìš° í° íš¨ê³¼"
                        
                        effect_size_results.append({
                            'Metric': metric,
                            "Cohen's d": f"{cohens_d:.4f}",
                            'Interpretation': interpretation
                        })
            
            if effect_size_results:
                effect_df = pd.DataFrame(effect_size_results)
                st.dataframe(effect_df, use_container_width=True, hide_index=True)
    
    # 4. ë¶„í¬ ë¶„ì„
    st.subheader("ğŸ“Š ê³ ê¸‰ ë¶„í¬ ë¶„ì„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ì™œë„ì™€ ì²¨ë„
        st.write("**ğŸ“ˆ ì™œë„(Skewness)ì™€ ì²¨ë„(Kurtosis)**")
        
        distribution_results = []
        for metric in metrics:
            if metric in current_scores.columns:
                metric_data = current_scores[metric].dropna()
                if len(metric_data) >= 3:
                    skewness = stats.skew(metric_data)
                    kurtosis = stats.kurtosis(metric_data)
                    
                    # í•´ì„
                    skew_interpretation = "ì¢Œí¸í–¥" if skewness < -0.5 else "ìš°í¸í–¥" if skewness > 0.5 else "ëŒ€ì¹­ì "
                    kurt_interpretation = "ë¾°ì¡±í•¨" if kurtosis > 0.5 else "í‰í‰í•¨" if kurtosis < -0.5 else "ì •ìƒ"
                    
                    distribution_results.append({
                        'Metric': metric,
                        'Skewness': f"{skewness:.4f}",
                        'Skew Type': skew_interpretation,
                        'Kurtosis': f"{kurtosis:.4f}",
                        'Kurt Type': kurt_interpretation
                    })
        
        if distribution_results:
            dist_df = pd.DataFrame(distribution_results)
            st.dataframe(dist_df, use_container_width=True, hide_index=True)
    
    with col2:
        # ë¶„í¬ ì í•©ë„ ê²€ì •
        st.write("**ğŸ§ª ë¶„í¬ ì í•©ë„ ê²€ì •**")
        
        selected_metric_dist = st.selectbox(
            "ê²€ì •í•  ë©”íŠ¸ë¦­",
            [m for m in metrics if m in current_scores.columns],
            key="dist_test_metric"
        )
        
        if selected_metric_dist and selected_metric_dist in current_scores.columns:
            metric_data = current_scores[selected_metric_dist].dropna()
            
            if len(metric_data) >= 8:  # KS ê²€ì •ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                # Kolmogorov-Smirnov ê²€ì • (ì •ê·œë¶„í¬)
                ks_stat, ks_p = stats.kstest(
                    (metric_data - np.mean(metric_data)) / np.std(metric_data),
                    'norm'
                )
                
                # Anderson-Darling ê²€ì • (ì •ê·œë¶„í¬)
                ad_stat, ad_critical, ad_significance = stats.anderson(metric_data, dist='norm')
                
                st.write(f"**K-S ê²€ì • (ì •ê·œë¶„í¬):**")
                st.write(f"â€¢ í†µê³„ëŸ‰: {ks_stat:.4f}")
                st.write(f"â€¢ p-ê°’: {ks_p:.4f}")
                st.write(f"â€¢ ê²°ê³¼: {'ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„' if ks_p > 0.05 else 'ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ'}")
                
                st.write(f"**Anderson-Darling ê²€ì •:**")
                st.write(f"â€¢ í†µê³„ëŸ‰: {ad_stat:.4f}")
                st.write(f"â€¢ 5% ì„ê³„ê°’: {ad_critical[2]:.4f}")
                st.write(f"â€¢ ê²°ê³¼: {'ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„' if ad_stat < ad_critical[2] else 'ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ'}")
    
    # 5. ìš”ì•½ ë° ê¶Œì¥ì‚¬í•­
    st.subheader("ğŸ’¡ í†µê³„ ë¶„ì„ ìš”ì•½ ë° ê¶Œì¥ì‚¬í•­")
    
    summary_points = []
    
    # ë°ì´í„° í’ˆì§ˆ í‰ê°€
    if ci_results:
        avg_ci_width = np.mean([float(r['Width']) for r in ci_results])
        if avg_ci_width < 0.1:
            summary_points.append("âœ… **ë†’ì€ ì •ë°€ë„**: ì‹ ë¢°êµ¬ê°„ì´ ì¢ì•„ ì¶”ì •ì´ ì •í™•í•©ë‹ˆë‹¤.")
        elif avg_ci_width > 0.3:
            summary_points.append("âš ï¸ **ë‚®ì€ ì •ë°€ë„**: ì‹ ë¢°êµ¬ê°„ì´ ë„“ì–´ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            summary_points.append("ğŸ“Š **ì ì • ì •ë°€ë„**: í˜„ì¬ ë°ì´í„° ì–‘ì´ ì ì ˆí•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    
    # ì •ê·œì„± ê²€ì • ê²°ê³¼ ìš”ì•½
    if normality_results:
        normal_count = sum(1 for r in normality_results if "Yes" in r['Normal'])
        if normal_count >= len(normality_results) * 0.8:
            summary_points.append("âœ… **ì •ê·œë¶„í¬ ê°€ì • ë§Œì¡±**: ëŒ€ë¶€ë¶„ì˜ ë©”íŠ¸ë¦­ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤.")
        else:
            summary_points.append("âš ï¸ **ë¹„ì •ê·œë¶„í¬**: ë¹„ëª¨ìˆ˜ í†µê³„ ë°©ë²• ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
    
    # ì„±ëŠ¥ ë¹„êµ ê²°ê³¼
    if ttest_results:
        significant_improvements = sum(1 for r in ttest_results if "í–¥ìƒ" in r['Result'] and "ìœ ì˜í•¨" in r['Result'])
        if significant_improvements > 0:
            summary_points.append(f"ğŸ“ˆ **ì„±ëŠ¥ í–¥ìƒ ê°ì§€**: {significant_improvements}ê°œ ë©”íŠ¸ë¦­ì—ì„œ ìœ ì˜í•œ í–¥ìƒì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            summary_points.append("ğŸ“Š **ì„±ëŠ¥ ì•ˆì •ì„±**: í˜„ì¬ ì„±ëŠ¥ì´ ê³¼ê±°ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    for point in summary_points:
        st.info(point)
    
    if not summary_points:
        st.info("ğŸ“Š í†µê³„ ë¶„ì„ì„ ìœ„í•´ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì¶”ê°€ í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
