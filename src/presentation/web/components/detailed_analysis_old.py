"""
ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸
ê°œë³„ QA ìŒì˜ ìƒì„¸ í‰ê°€ ê²°ê³¼ ë¶„ì„
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
import sqlite3
from pathlib import Path
import os

def get_db_path():
    """ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
    return Path(__file__).parent.parent / "evaluations.db"

def load_evaluation_data():
    """í‰ê°€ìš© ë°ì´í„° ë¡œë“œ"""
    project_root = Path(__file__).parent.parent.parent.parent
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ íŒŒì¼ë“¤ í™•ì¸
    available_datasets = [
        "data/evaluation_data.json",
        "data/evaluation_data_variant1.json"
    ]
    
    # ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì…‹ ì°¾ê¸°
    for dataset_path in available_datasets:
        full_path = project_root / dataset_path
        if full_path.exists():
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                return data
            except Exception as e:
                st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    # ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
    return None

def load_latest_evaluation_results():
    """ìµœì‹  í‰ê°€ ê²°ê³¼ì™€ ê°œë³„ QA ì ìˆ˜ ë¡œë“œ"""
    try:
        db_path = get_db_path()
        if not db_path.exists():
            return None, []
        
        conn = sqlite3.connect(str(db_path))
        
        # ìµœì‹  í‰ê°€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        query = '''
            SELECT raw_data 
            FROM evaluations 
            ORDER BY timestamp DESC 
            LIMIT 1
        '''
        
        result = conn.execute(query).fetchone()
        conn.close()
        
        if result and result[0]:
            raw_data = json.loads(result[0])
            # ê°œë³„ QA ì ìˆ˜ê°€ ìˆë‹¤ë©´ ì¶”ì¶œ, ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            individual_scores = raw_data.get('individual_scores', [])
            return raw_data, individual_scores
        
        return None, []
        
    except Exception as e:
        st.error(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, []

def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ë©”ì¸ í™”ë©´"""
    st.header("ğŸ” ìƒì„¸ ë¶„ì„")
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    evaluation_data = load_evaluation_data()
    latest_results, individual_scores = load_latest_evaluation_results()
    
    if not evaluation_data:
        st.warning("ğŸ“ ë¶„ì„í•  í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    if not latest_results:
        st.warning("ğŸ“Š í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š QA ê°œë³„ ë¶„ì„", "ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„í¬", "ğŸ¯ íŒ¨í„´ ë¶„ì„"])
    
    with tab1:
        show_qa_analysis(evaluation_data, individual_scores)
    
    with tab2:
        show_metric_distribution(evaluation_data, latest_results, individual_scores)
    
    with tab3:
        show_pattern_analysis(evaluation_data, latest_results, individual_scores)

def show_qa_analysis(evaluation_data, individual_scores):
    """ê°œë³„ QA ë¶„ì„"""
    st.subheader("ğŸ“‹ ì§ˆë¬¸-ë‹µë³€ ìŒë³„ ìƒì„¸ ë¶„ì„")
    
    # QA ìŒ ì„ íƒ
    qa_options = [f"Q{i+1}: {qa['question'][:50]}..." for i, qa in enumerate(evaluation_data)]
    selected_qa_idx = st.selectbox("ë¶„ì„í•  QA ì„ íƒ", range(len(qa_options)), format_func=lambda x: qa_options[x])
    
    if selected_qa_idx is not None:
        qa_data = evaluation_data[selected_qa_idx]
        # í•´ë‹¹ QAì˜ ê°œë³„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        qa_scores = individual_scores[selected_qa_idx] if selected_qa_idx < len(individual_scores) else None
        show_individual_qa_details(qa_data, selected_qa_idx + 1, qa_scores)

def show_individual_qa_details(qa_data, qa_number, qa_scores=None):
    """ê°œë³„ QA ìƒì„¸ ì •ë³´ í‘œì‹œ"""
    st.markdown(f"### ğŸ“ QA {qa_number} ìƒì„¸ ë¶„ì„")
    
    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ¤” ì§ˆë¬¸")
        st.info(qa_data['question'])
        
        st.markdown("#### ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
        st.success(qa_data['answer'])
    
    with col2:
        st.markdown("#### ğŸ“š ì œê³µëœ ì»¨í…ìŠ¤íŠ¸")
        for i, context in enumerate(qa_data['contexts'], 1):
            st.markdown(f"**ì»¨í…ìŠ¤íŠ¸ {i}:**")
            st.text_area(f"context_{i}", context, height=80, disabled=True, key=f"context_{qa_number}_{i}")
        
        st.markdown("#### âœ… ì •ë‹µ (Ground Truth)")
        st.info(qa_data['ground_truth'])
    
    # ì‹¤ì œ í‰ê°€ ì ìˆ˜ í‘œì‹œ
    st.markdown("#### ğŸ“Š ì´ QAì˜ í‰ê°€ ì ìˆ˜")
    
    if qa_scores:
        # ì‹¤ì œ í‰ê°€ ê²°ê³¼ ì‚¬ìš©
        scores = qa_scores
    else:
        # í‰ê°€ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ì•ˆë‚´ ë©”ì‹œì§€
        st.warning("ğŸ“Š ì´ QAì— ëŒ€í•œ ê°œë³„ í‰ê°€ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í‰ê°€ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        # ì „ì²´ í‰ê·  ì ìˆ˜ë¡œ ëŒ€ì²´ (ì°¸ê³ ìš©)
        latest_results, _ = load_latest_evaluation_results()
        if latest_results:
            scores = {
                'faithfulness': latest_results.get('faithfulness', 0),
                'answer_relevancy': latest_results.get('answer_relevancy', 0),
                'context_recall': latest_results.get('context_recall', 0),
                'context_precision': latest_results.get('context_precision', 0)
            }
            st.info("ğŸ’¡ ì•„ë˜ëŠ” ì „ì²´ í‰ê°€ì˜ í‰ê·  ì ìˆ˜ì…ë‹ˆë‹¤. ê°œë³„ QA ì ìˆ˜ë¥¼ ë³´ë ¤ë©´ í‰ê°€ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        else:
            st.error("âŒ í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    if scores:
        score_cols = st.columns(4)
        for i, (metric, score) in enumerate(scores.items()):
            with score_cols[i]:
                color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
                st.metric(
                    label=metric.replace('_', ' ').title(),
                    value=f"{score:.3f}"
                )
        
        # ì ìˆ˜ ì‹œê°í™”
        show_qa_score_chart(scores, qa_number)
        
        # í‰ê°€ ê·¼ê±° (ì‹¤ì œ í…ìŠ¤íŠ¸ í¬í•¨)
        show_evaluation_reasoning(qa_data, qa_number, scores)

def show_qa_score_chart(scores, qa_number):
    """ê°œë³„ QA ì ìˆ˜ ì°¨íŠ¸"""
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ì‹œê°í™”")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°” ì°¨íŠ¸
        metrics = list(scores.keys())
        values = list(scores.values())
        
        fig = go.Figure(data=[
            go.Bar(x=metrics, y=values, 
                  marker_color=['green' if v >= 0.8 else 'orange' if v >= 0.6 else 'red' for v in values])
        ])
        
        fig.update_layout(
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ì ìˆ˜",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ê²Œì´ì§€ ì°¨íŠ¸ (í‰ê·  ì ìˆ˜)
        avg_score = sum(values) / len(values)
        
        fig = go.Figure(go.Indicator(
            mode = "gauge+number+delta",
            value = avg_score,
            domain = {'x': [0, 1], 'y': [0, 1]},
            title = {'text': f"QA {qa_number} ì¢…í•© ì ìˆ˜"},
            delta = {'reference': 0.5},
            gauge = {
                'axis': {'range': [None, 1]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 0.5], 'color': "lightgray"},
                    {'range': [0.5, 0.8], 'color': "yellow"},
                    {'range': [0.8, 1], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 0.9
                }
            }
        ))
        
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

def show_evaluation_reasoning(qa_data, qa_number, qa_scores=None):
    """í‰ê°€ ê·¼ê±° í‘œì‹œ (ì‹¤ì œ í…ìŠ¤íŠ¸ í¬í•¨)"""
    st.markdown("#### ğŸ§  í‰ê°€ ê·¼ê±°")
    
    # ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°
    question = qa_data['question']
    answer = qa_data['answer']
    contexts = qa_data['contexts']
    ground_truth = qa_data['ground_truth']
    
    # í–¥ìƒëœ í‰ê°€ ê·¼ê±° (ì‹¤ì œ í…ìŠ¤íŠ¸ í¬í•¨)
    detailed_reasoning = get_detailed_reasoning(qa_data, qa_number, qa_scores)
    
    for metric, analysis in detailed_reasoning.items():
        with st.expander(f"ğŸ“ {metric.replace('_', ' ').title()} í‰ê°€ ê·¼ê±°"):
            st.markdown(analysis['explanation'])
            
            if 'text_analysis' in analysis:
                st.markdown("##### ğŸ“„ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¶„ì„:")
                for item in analysis['text_analysis']:
                    if item['type'] == 'highlight':
                        st.markdown(f"**{item['label']}:**")
                        st.code(item['text'], language=None)
                    elif item['type'] == 'comparison':
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown(f"**{item['label1']}:**")
                            st.info(item['text1'])
                        with col2:
                            st.markdown(f"**{item['label2']}:**")
                            st.info(item['text2'])

def get_detailed_reasoning(qa_data, qa_number, qa_scores=None):
    """ìƒì„¸í•œ í‰ê°€ ê·¼ê±° ìƒì„± - ë™ì  ë°ì´í„° ê¸°ë°˜"""
    question = qa_data['question']
    answer = qa_data['answer']
    contexts = qa_data['contexts']
    ground_truth = qa_data['ground_truth']
    
    # ëª¨ë“  QAì— ëŒ€í•´ ë™ì  ë¶„ì„ ìƒì„±
    return generate_dynamic_analysis(answer, question, contexts, ground_truth, qa_scores)

def generate_dynamic_analysis(answer, question, contexts, ground_truth, scores):
    """ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ë¶„ì„ ìƒì„±"""
    
    # ê¸°ë³¸ ì ìˆ˜ (scoresê°€ ì—†ëŠ” ê²½ìš° 0.5ë¡œ ì„¤ì •)
    faithfulness_score = scores.get('faithfulness', 0.5) if scores else 0.5
    relevancy_score = scores.get('answer_relevancy', 0.5) if scores else 0.5
    recall_score = scores.get('context_recall', 0.5) if scores else 0.5
    precision_score = scores.get('context_precision', 0.5) if scores else 0.5
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
    answer_preview = answer[:200] + '...' if len(answer) > 200 else answer
    question_preview = question[:100] + '...' if len(question) > 100 else question
    context_text = ' '.join(contexts) if contexts else "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"
    context_preview = context_text[:300] + '...' if len(context_text) > 300 else context_text
    ground_truth_preview = ground_truth[:200] + '...' if len(ground_truth) > 200 else ground_truth
    
    return {
        'faithfulness': {
            'explanation': generate_faithfulness_explanation(faithfulness_score),
            'text_analysis': [
                {
                    'type': 'highlight',
                    'label': 'ë‹µë³€ ë‚´ìš©',
                    'text': answer_preview
                },
                {
                    'type': 'highlight',
                    'label': 'ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸',
                    'text': context_preview
                }
            ]
        },
        'answer_relevancy': {
            'explanation': generate_relevancy_explanation(relevancy_score),
            'text_analysis': [
                {
                    'type': 'comparison',
                    'label1': 'ì§ˆë¬¸',
                    'text1': question_preview,
                    'label2': 'ë‹µë³€',
                    'text2': answer_preview
                }
            ]
        },
        'context_recall': {
            'explanation': generate_recall_explanation(recall_score),
            'text_analysis': [
                {
                    'type': 'comparison',
                    'label1': 'Ground Truth',
                    'text1': ground_truth_preview,
                    'label2': 'ì œê³µëœ ì»¨í…ìŠ¤íŠ¸',
                    'text2': context_preview
                }
            ]
        },
        'context_precision': {
            'explanation': generate_precision_explanation(precision_score),
            'text_analysis': [
                {
                    'type': 'highlight',
                    'label': 'ì§ˆë¬¸',
                    'text': question_preview
                },
                {
                    'type': 'highlight',
                    'label': 'ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸',
                    'text': context_preview
                }
            ]
        }
    }

def generate_faithfulness_explanation(score):
    """Faithfulness ì ìˆ˜ì— ë”°ë¥¸ ì„¤ëª… ìƒì„±"""
    if score >= 0.8:
        return f"ë‹µë³€ì˜ ë‚´ìš©ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"ë‹µë³€ì˜ ì¼ë¶€ ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ë©ë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì¼ë¶€ ì •ë³´ëŠ” ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ë¡ í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"ë‹µë³€ì˜ ë‚´ìš©ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶©ë¶„íˆ ë’·ë°›ì¹¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ê°€ í¬í•¨ë˜ì—ˆê±°ë‚˜ ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

def generate_relevancy_explanation(score):
    """Answer Relevancy ì ìˆ˜ì— ë”°ë¥¸ ì„¤ëª… ìƒì„±"""
    if score >= 0.8:
        return f"ë‹µë³€ì´ ì§ˆë¬¸ì— ë§¤ìš° ì ì ˆí•˜ê²Œ ëŒ€ì‘í•©ë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì§€ë§Œ ì™„ì „íˆ ë§Œì¡±ìŠ¤ëŸ½ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆê±°ë‚˜ í•µì‹¬ì„ ë²—ì–´ë‚œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"ë‹µë³€ì´ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì§ˆë¬¸ì˜ ì˜ë„ì™€ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ë‹µë³€í–ˆê±°ë‚˜ í•µì‹¬ì„ ë†“ì³¤ì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."

def generate_recall_explanation(score):
    """Context Recall ì ìˆ˜ì— ë”°ë¥¸ ì„¤ëª… ìƒì„±"""
    if score >= 0.8:
        return f"Ground truthì˜ í•µì‹¬ ì •ë³´ê°€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ ë°œê²¬ë©ë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). í•„ìš”í•œ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"Ground truthì˜ ì¼ë¶€ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì¤‘ìš”í•œ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"Ground truthì˜ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶©ë¶„íˆ ë°œê²¬ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ì´ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤."

def generate_precision_explanation(score):
    """Context Precision ì ìˆ˜ì— ë”°ë¥¸ ì„¤ëª… ìƒì„±"""
    if score >= 0.8:
        return f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ì ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ìˆìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì¼ë¶€ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    else:
        return f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (ì ìˆ˜: {score:.3f}). ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ê°€ ë§ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."

# í•˜ë“œì½”ë”©ëœ ì½”ë“œ ì œê±° ì™„ë£Œ - ì´ì œ ë™ì  ë¶„ì„ë§Œ ì‚¬ìš©

def show_metric_distribution(evaluation_data, latest_results, individual_scores):
    """ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„"""
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„")
    
    if not individual_scores:
        st.warning("ğŸ“Š ê°œë³„ QA ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í‰ê°€ ê²°ê³¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")
        if latest_results:
            # ì „ì²´ í‰ê·  ì ìˆ˜ë§Œ í‘œì‹œ
            st.markdown("#### ì „ì²´ í‰ê°€ ê²°ê³¼")
            col1, col2, col3, col4 = st.columns(4)
            
            metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
            for i, metric in enumerate(metrics):
                with [col1, col2, col3, col4][i]:
                    score = latest_results.get(metric, 0)
                    st.metric(
                        label=metric.replace('_', ' ').title(),
                        value=f"{score:.3f}"
                    )
        return
    
    num_qa = len(evaluation_data)
    num_scores = len(individual_scores) if individual_scores else 0
    
    # ì•ˆì „í•œ ë°ì´í„° ê¸¸ì´ ë§ì¶¤
    actual_length = min(num_qa, num_scores) if num_scores > 0 else num_qa
    
    if actual_length == 0:
        st.warning("ğŸ“Š í‘œì‹œí•  í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì•ˆì „í•œ ê°œë³„ ì ìˆ˜ ë°ì´í„° ìƒì„±
    qa_data = {
        'QA': [f'Q{i+1}' for i in range(actual_length)],
        'faithfulness': [],
        'answer_relevancy': [],
        'context_recall': [],
        'context_precision': []
    }
    
    # ê° QAì— ëŒ€í•´ ì•ˆì „í•˜ê²Œ ì ìˆ˜ ì¶”ê°€
    for i in range(actual_length):
        if i < num_scores and individual_scores[i]:
            score = individual_scores[i]
            qa_data['faithfulness'].append(score.get('faithfulness', 0))
            qa_data['answer_relevancy'].append(score.get('answer_relevancy', 0))
            qa_data['context_recall'].append(score.get('context_recall', 0))
            qa_data['context_precision'].append(score.get('context_precision', 0))
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            qa_data['faithfulness'].append(0)
            qa_data['answer_relevancy'].append(0)
            qa_data['context_recall'].append(0)
            qa_data['context_precision'].append(0)
    
    df = pd.DataFrame(qa_data)
    
    # íˆíŠ¸ë§µ
    st.markdown("#### ğŸ”¥ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ")
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    heatmap_data = df[metrics].values
    
    # í˜¸ë²„ ì •ë³´ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ìƒì„±
    hover_text = []
    for i, qa in enumerate(df['QA']):
        row_text = []
        for j, metric in enumerate(metrics):
            score = heatmap_data[i, j]
            # ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰
            if score >= 0.9:
                grade = "ğŸŒŸ ìš°ìˆ˜"
            elif score >= 0.8:
                grade = "âœ… ì–‘í˜¸"
            elif score >= 0.6:
                grade = "âš ï¸ ë³´í†µ"
            else:
                grade = "âŒ ê°œì„ í•„ìš”"
            
            text = f"QA: {qa}<br>ë©”íŠ¸ë¦­: {metric.replace('_', ' ').title()}<br>ì ìˆ˜: {score:.3f}<br>ë“±ê¸‰: {grade}"
            row_text.append(text)
        hover_text.append(row_text)
    
    fig = go.Figure(data=go.Heatmap(
        z=heatmap_data,
        x=[m.replace('_', ' ').title() for m in metrics],
        y=df['QA'],
        colorscale='RdYlGn',
        colorbar=dict(title="ì ìˆ˜"),
        hovertemplate='%{hovertext}<extra></extra>',
        hovertext=hover_text
    ))
    
    fig.update_layout(
        title="QAë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ",
        height=400,
        xaxis_title="ë©”íŠ¸ë¦­",
        yaxis_title="ì§ˆë¬¸-ë‹µë³€"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # ë¶„í¬ ì°¨íŠ¸
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ë¶„í¬")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°•ìŠ¤í”Œë¡¯
        fig = go.Figure()
        
        for metric in metrics:
            fig.add_trace(go.Box(
                y=df[metric],
                name=metric.replace('_', ' ').title(),
                boxpoints='all'
            ))
        
        fig.update_layout(
            title="ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë¶„í¬",
            yaxis_title="ì ìˆ˜",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # íˆìŠ¤í† ê·¸ë¨
        selected_metric = st.selectbox("ë¶„í¬ë¥¼ ë³¼ ë©”íŠ¸ë¦­ ì„ íƒ", metrics)
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        metric_data = df[selected_metric].dropna()
        
        if len(metric_data) > 0 and not metric_data.isna().all():
            # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
            fig = go.Figure(data=[go.Histogram(
                x=metric_data, 
                nbinsx=min(10, len(metric_data)),  # ë°ì´í„° ê°œìˆ˜ì— ë”°ë¼ bin ìˆ˜ ì¡°ì •
                histnorm='probability density' if len(metric_data) > 1 else 'count'
            )])
            
            fig.update_layout(
                title=f"{selected_metric.replace('_', ' ').title()} ì ìˆ˜ ë¶„í¬",
                xaxis_title="ì ìˆ˜",
                yaxis_title="ë¹ˆë„",
                height=400,
                xaxis=dict(range=[0, 1])  # ì ìˆ˜ ë²”ìœ„ ê³ ì •
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€ (ì•ˆì „í•œ ê³„ì‚°)
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            with col_stat1:
                try:
                    mean_val = metric_data.mean()
                    st.metric("í‰ê· ", f"{mean_val:.3f}" if not pd.isna(mean_val) else "ê³„ì‚°ë¶ˆê°€")
                except:
                    st.metric("í‰ê· ", "ê³„ì‚°ë¶ˆê°€")
            with col_stat2:
                try:
                    std_val = metric_data.std()
                    st.metric("í‘œì¤€í¸ì°¨", f"{std_val:.3f}" if not pd.isna(std_val) else "ê³„ì‚°ë¶ˆê°€")
                except:
                    st.metric("í‘œì¤€í¸ì°¨", "ê³„ì‚°ë¶ˆê°€")
            with col_stat3:
                st.metric("ë°ì´í„° ê°œìˆ˜", len(metric_data))
        else:
            st.warning(f"{selected_metric} ë©”íŠ¸ë¦­ì— ëŒ€í•œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def show_pattern_analysis(evaluation_data, latest_results, individual_scores):
    """íŒ¨í„´ ë¶„ì„"""
    st.subheader("ğŸ¯ RAG ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒ¨í„´ ë¶„ì„")
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë” ë§ì€ ë¶„ì„ ì œê³µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ ì§ˆë¬¸ íŠ¹ì„±", "ğŸ“š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„", "ğŸ¯ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸", "ğŸ”— ìƒê´€ê´€ê³„"])
    
    with tab1:
        show_question_analysis(evaluation_data)
    
    with tab2:
        show_context_analysis(evaluation_data)
    
    with tab3:
        show_performance_insights(evaluation_data, latest_results, individual_scores)
    
    with tab4:
        show_correlation_analysis(evaluation_data, latest_results, individual_scores)

def show_question_analysis(evaluation_data):
    """ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“ ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„")
        
        # ì§ˆë¬¸ ê¸¸ì´ ë¶„ì„
        question_lengths = [len(qa['question'].split()) for qa in evaluation_data]
        avg_length = sum(question_lengths) / len(question_lengths)
        
        st.metric("í‰ê·  ì§ˆë¬¸ ê¸¸ì´", f"{avg_length:.1f} ë‹¨ì–´")
        
        # ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
        complex_indicators = 0
        for qa in evaluation_data:
            question = qa['question']
            if '?' in question or 'ë¬´ì—‡' in question or 'ì–´ë–»ê²Œ' in question:
                complex_indicators += 1
        
        complexity_ratio = complex_indicators / len(evaluation_data) * 100
        st.metric("ëª…í™•í•œ ì§ˆë¬¸ ë¹„ìœ¨", f"{complexity_ratio:.1f}%")
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„ (ë” ìƒì„¸í•œ ë¶„ë¥˜)
        question_types = []
        for qa in evaluation_data:
            question = qa['question'].lower()
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¶„ë¥˜ (ë” êµ¬ì²´ì ì¸ ê²ƒë¶€í„°)
            if 'ë¬´ì—‡' in question or 'what' in question:
                if 'ì°¨ì´' in question or 'ë‹¤ë¥¸' in question:
                    question_types.append('ğŸ”„ ë¹„êµí˜•')
                elif 'ì˜ë¯¸' in question or 'ì •ì˜' in question:
                    question_types.append('ğŸ“– ì •ì˜í˜•')
                else:
                    question_types.append('â“ ì„¤ëª…í˜•')
            elif 'ì–´ë–»ê²Œ' in question or 'how' in question:
                if 'ì„¤ì¹˜' in question or 'ì‹¤í–‰' in question:
                    question_types.append('âš™ï¸ ì„¤ì¹˜/ì‹¤í–‰í˜•')
                elif 'êµ¬í˜„' in question or 'ë§Œë“¤' in question:
                    question_types.append('ğŸ› ï¸ êµ¬í˜„í˜•')
                else:
                    question_types.append('ğŸ“‹ ë°©ë²•í˜•')
            elif 'ì™œ' in question or 'why' in question or 'ì´ìœ ' in question:
                question_types.append('ğŸ¤” ì´ìœ í˜•')
            elif 'ì–¸ì œ' in question or 'when' in question:
                question_types.append('â° ì‹œì í˜•')
            elif 'ì–´ë””' in question or 'where' in question:
                question_types.append('ğŸ“ ìœ„ì¹˜í˜•')
            elif 'ëˆ„ê°€' in question or 'who' in question:
                question_types.append('ğŸ‘¤ ì£¼ì²´í˜•')
            elif 'ëª‡' in question or 'ì–¼ë§ˆ' in question or 'how many' in question:
                question_types.append('ğŸ“Š ìˆ˜ëŸ‰í˜•')
            elif 'ì¥ì ' in question or 'ë‹¨ì ' in question or 'íŠ¹ì§•' in question:
                question_types.append('âš–ï¸ íŠ¹ì„±í˜•')
            elif 'ë°©ë²•' in question and ('ë¬´ì—‡' not in question and 'ì–´ë–»ê²Œ' not in question):
                question_types.append('ğŸ“‹ ë°©ë²•í˜•')
            elif '?' in question or 'ì¸ê°€' in question:
                question_types.append('â“ í™•ì¸í˜•')
            else:
                question_types.append('ğŸ“ ê¸°íƒ€')
        
        type_counts = pd.Series(question_types).value_counts()
        
        fig = go.Figure(data=[go.Pie(labels=type_counts.index, values=type_counts.values)])
        fig.update_layout(title="ì§ˆë¬¸ ìœ í˜• ë¶„í¬", height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ¯ ì§ˆë¬¸ ìµœì í™” ì œì•ˆ")
        
        # ì§ˆë¬¸ë³„ ë¬¸ì œì  ë¶„ì„
        for i, qa in enumerate(evaluation_data):
            question = qa['question']
            
            # ê°„ë‹¨í•œ ë¶„ì„
            issues = []
            if len(question.split()) > 15:
                issues.append("â“ ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹€")
            if '?' not in question and '?' not in question:
                issues.append("â“ ëª…í™•í•œ ì§ˆë¬¸ í˜•íƒœê°€ ì•„ë‹˜")
            if not any(word in question for word in ['ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ']):
                issues.append("â“ ë¶ˆëª…í™•í•œ ì˜ë„")
            
            if issues:
                with st.expander(f"Q{i+1} ê°œì„  ì œì•ˆ"):
                    st.write(f"**ì§ˆë¬¸:** {question}")
                    for issue in issues:
                        st.write(f"- {issue}")
            else:
                with st.expander(f"Q{i+1} âœ… ì¢‹ì€ ì§ˆë¬¸"):
                    st.write(f"**ì§ˆë¬¸:** {question}")
                    st.success("ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì§ˆë¬¸ì…ë‹ˆë‹¤.")

def show_context_analysis(evaluation_data):
    """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“š ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„±")
        
        # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ ë¶„ì„
        context_counts = [len(qa['contexts']) for qa in evaluation_data]
        avg_contexts = sum(context_counts) / len(context_counts)
        
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜", f"{avg_contexts:.1f} ê°œ")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
        all_context_lengths = []
        for qa in evaluation_data:
            for context in qa['contexts']:
                all_context_lengths.append(len(context.split()))
        
        avg_context_length = sum(all_context_lengths) / len(all_context_lengths)
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´", f"{avg_context_length:.1f} ë‹¨ì–´")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µë„ ë¶„ì„
        overlap_scores = []
        for qa in evaluation_data:
            contexts = qa['contexts']
            if len(contexts) > 1:
                # ê°„ë‹¨í•œ ì¤‘ë³µë„ ê³„ì‚° (ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨)
                all_words = set()
                for context in contexts:
                    all_words.update(context.split())
                
                common_ratio = len(all_words) / sum(len(c.split()) for c in contexts)
                overlap_scores.append(1 - common_ratio)
        
        avg_overlap = sum(overlap_scores) / len(overlap_scores) if overlap_scores else 0
        st.metric("ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µë„", f"{avg_overlap:.2f}")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬
        fig = go.Figure(data=[go.Histogram(x=all_context_lengths, nbinsx=10)])
        fig.update_layout(
            title="ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬",
            xaxis_title="ë‹¨ì–´ ìˆ˜",
            yaxis_title="ë¹ˆë„",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì œì•ˆ")
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ í’ˆì§ˆ ë¶„ì„
        for i, qa in enumerate(evaluation_data):
            contexts = qa['contexts']
            question = qa['question']
            
            with st.expander(f"Q{i+1} ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"):
                for j, context in enumerate(contexts):
                    context_length = len(context.split())
                    
                    # ê´€ë ¨ì„± ì¶”ì • (í‚¤ì›Œë“œ ë§¤ì¹­)
                    question_words = set(question.lower().split())
                    context_words = set(context.lower().split())
                    relevance = len(question_words & context_words) / len(question_words) if question_words else 0
                    
                    st.write(f"**ì»¨í…ìŠ¤íŠ¸ {j+1}:**")
                    st.write(f"- ê¸¸ì´: {context_length} ë‹¨ì–´")
                    st.write(f"- ì¶”ì • ê´€ë ¨ì„±: {relevance:.2f}")
                    
                    if context_length < 10:
                        st.warning("âš ï¸ ë„ˆë¬´ ì§§ì€ ì»¨í…ìŠ¤íŠ¸")
                    elif context_length > 100:
                        st.warning("âš ï¸ ë„ˆë¬´ ê¸´ ì»¨í…ìŠ¤íŠ¸")
                    elif relevance < 0.1:
                        st.warning("âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ìŒ")
                    else:
                        st.success("âœ… ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸")

def show_performance_insights(evaluation_data, latest_results, individual_scores):
    """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸"""
    st.markdown("#### ğŸ¯ RAG ì„±ëŠ¥ í–¥ìƒ ì¸ì‚¬ì´íŠ¸")
    
    if not latest_results:
        st.warning("ğŸ“Š í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("##### ğŸ” ê°œì„  ìš°ì„ ìˆœìœ„")
        
        # ì‹¤ì œ í‰ê°€ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        current_scores = {}
        
        if individual_scores:
            # ê°œë³„ ì ìˆ˜ê°€ ìˆìœ¼ë©´ í‰ê·  ê³„ì‚°
            for metric in metrics:
                scores = [score.get(metric, 0) for score in individual_scores if score.get(metric) is not None]
                current_scores[metric] = sum(scores) / len(scores) if scores else latest_results.get(metric, 0)
        else:
            # ì „ì²´ í‰ê°€ ê²°ê³¼ ì‚¬ìš©
            for metric in metrics:
                current_scores[metric] = latest_results.get(metric, 0)
        
        # ê°œì„  ìš°ì„ ìˆœìœ„ (ë‚®ì€ ì ìˆ˜ ìˆœ)
        sorted_metrics = sorted(current_scores.items(), key=lambda x: x[1])
        
        for i, (metric, score) in enumerate(sorted_metrics):
            priority = ["ğŸ”´ ìµœìš°ì„ ", "ğŸŸ¡ ì¤‘ìš”", "ğŸŸ¢ ì–‘í˜¸", "âœ… ìš°ìˆ˜"][i]
            st.write(f"{priority}: **{metric.replace('_', ' ').title()}** ({score:.3f})")
        
        # êµ¬ì²´ì  ê°œì„  ì œì•ˆ
        st.markdown("##### ğŸ’¡ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ")
        
        lowest_metric = sorted_metrics[0][0]
        if lowest_metric == 'context_precision':
            st.info("ğŸ¯ **Context Precision ê°œì„ :**\n- ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê±°\n- ë” ì •í™•í•œ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©\n- ì»¨í…ìŠ¤íŠ¸ ìˆœì„œ ìµœì í™”")
        elif lowest_metric == 'answer_relevancy':
            st.info("ğŸ¯ **Answer Relevancy ê°œì„ :**\n- ì§ˆë¬¸ ì˜ë„ íŒŒì•… ê°œì„ \n- ê°„ê²°í•œ ë‹µë³€ ìƒì„±\n- ë¶ˆí•„ìš”í•œ ë¶€ì—°ì„¤ëª… ì œê±°")
        elif lowest_metric == 'faithfulness':
            st.info("ğŸ¯ **Faithfulness ê°œì„ :**\n- í™˜ê° ë°©ì§€ í”„ë¡¬í”„íŠ¸ ì¶”ê°€\n- ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„ ê²€ì¦\n- ì¶œì²˜ ëª…ì‹œ ê°•í™”")
        else:
            st.info("ğŸ¯ **Context Recall ê°œì„ :**\n- ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€\n- ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ í™œìš©\n- ì¤‘ìš” ì •ë³´ ëˆ„ë½ ë°©ì§€")
    
    with col2:
        st.markdown("##### ğŸ¯ RAGAS ë…¼ë¬¸ ê¸°ë°˜ ì„±ëŠ¥ ê¸°ì¤€")
        
        # RAGAS ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ì‹¤ì œ ê¸°ì¤€ì ë“¤
        st.markdown("""
        **ğŸ“š RAGAS ì—°êµ¬ ê¸°ë°˜ ê¶Œì¥ ê¸°ì¤€:**
        
        **Faithfulness:**
        - ğŸŸ¢ 0.9+ : í”„ë¡œë•ì…˜ ê¶Œì¥ ìˆ˜ì¤€
        - ğŸŸ¡ 0.8-0.9 : ê°œì„  ê¶Œì¥
        - ğŸ”´ <0.8 : ì¦‰ì‹œ ê°œì„  í•„ìš”
        
        **Answer Relevancy:**
        - ğŸŸ¢ 0.8+ : ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìˆ˜ì¤€
        - ğŸŸ¡ 0.6-0.8 : ë³´í†µ ìˆ˜ì¤€
        - ğŸ”´ <0.6 : ê°œì„  í•„ìš”
        
        **Context Recall:**
        - ğŸŸ¢ 0.9+ : ìš°ìˆ˜í•œ ê²€ìƒ‰ ì„±ëŠ¥
        - ğŸŸ¡ 0.7-0.9 : ì ì ˆí•œ ìˆ˜ì¤€
        - ğŸ”´ <0.7 : ê²€ìƒ‰ ê°œì„  í•„ìš”
        
        **Context Precision:**
        - ğŸŸ¢ 0.8+ : íš¨ìœ¨ì ì¸ ê²€ìƒ‰
        - ğŸŸ¡ 0.6-0.8 : ë³´í†µ ìˆ˜ì¤€
        - ğŸ”´ <0.6 : ë…¸ì´ì¦ˆ ì œê±° í•„ìš”
        """)
        
        # í˜„ì¬ ì„±ëŠ¥ ìƒíƒœ ë¶„ì„
        st.markdown("##### ğŸ“Š í˜„ì¬ ë°ì´í„°ì…‹ ë¶„ì„")
        
        # current_scoresëŠ” ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ë¨
        current_avg = current_scores
        
        # ìƒíƒœ ë¶„ì„
        status_analysis = []
        for metric, avg_score in current_avg.items():
            metric_name = metric.replace('_', ' ').title()
            
            if metric == 'faithfulness':
                if avg_score >= 0.9:
                    status = "ğŸŸ¢ í”„ë¡œë•ì…˜ ìˆ˜ì¤€"
                elif avg_score >= 0.8:
                    status = "ğŸŸ¡ ê°œì„  ê¶Œì¥"
                else:
                    status = "ğŸ”´ ì¦‰ì‹œ ê°œì„  í•„ìš”"
            elif metric == 'answer_relevancy':
                if avg_score >= 0.8:
                    status = "ğŸŸ¢ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìˆ˜ì¤€"
                elif avg_score >= 0.6:
                    status = "ğŸŸ¡ ë³´í†µ ìˆ˜ì¤€"
                else:
                    status = "ğŸ”´ ê°œì„  í•„ìš”"
            elif metric == 'context_recall':
                if avg_score >= 0.9:
                    status = "ğŸŸ¢ ìš°ìˆ˜í•œ ê²€ìƒ‰"
                elif avg_score >= 0.7:
                    status = "ğŸŸ¡ ì ì ˆí•œ ìˆ˜ì¤€"
                else:
                    status = "ğŸ”´ ê²€ìƒ‰ ê°œì„  í•„ìš”"
            else:  # context_precision
                if avg_score >= 0.8:
                    status = "ğŸŸ¢ íš¨ìœ¨ì ì¸ ê²€ìƒ‰"
                elif avg_score >= 0.6:
                    status = "ğŸŸ¡ ë³´í†µ ìˆ˜ì¤€"
                else:
                    status = "ğŸ”´ ë…¸ì´ì¦ˆ ì œê±° í•„ìš”"
            
            status_analysis.append(f"**{metric_name}**: {avg_score:.3f} - {status}")
        
        for analysis in status_analysis:
            st.write(analysis)
        
        # ì‹¤ìš©ì  ê°œì„  ê°€ì´ë“œ
        st.markdown("##### ğŸ’¡ ì‹¤ìš©ì  ê°œì„  ê°€ì´ë“œ")
        
        improvement_guide = """
        **1. ë¹ ë¥¸ ê°œì„  (1-2ì¼):**
        - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
        - Temperature ì¡°ì •
        - ë‹µë³€ ê¸¸ì´ ì œí•œ
        
        **2. ì¤‘ê¸° ê°œì„  (1-2ì£¼):**
        - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ íŠœë‹
        - ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§ ê°•í™”
        - í‰ê°€ ë°ì´í„° í™•ì¥
        
        **3. ì¥ê¸° ê°œì„  (1ê°œì›”+):**
        - ëª¨ë¸ íŒŒì¸íŠœë‹
        - ë„ë©”ì¸ë³„ ì„ë² ë”©
        - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„
        """
        
        st.markdown(improvement_guide)

def show_correlation_analysis(evaluation_data, latest_results, individual_scores):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    st.markdown("#### ğŸ”— ì„±ëŠ¥ ìƒê´€ê´€ê³„ ë¶„ì„")
    
    if not latest_results:
        st.warning("ğŸ“Š í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìƒê´€ê´€ê³„ ë¶„ì„
    analysis_data = []
    
    for i, qa in enumerate(evaluation_data):
        question_length = len(qa['question'].split())
        context_count = len(qa['contexts'])
        total_context_length = sum(len(c.split()) for c in qa['contexts'])
        avg_context_length = total_context_length / context_count if context_count > 0 else 0
        
        # ì‹¤ì œ í‰ê°€ ì ìˆ˜ ì‚¬ìš©
        if individual_scores and i < len(individual_scores):
            scores = individual_scores[i]
        else:
            # ê°œë³„ ì ìˆ˜ê°€ ì—†ìœ¼ë©´ ì „ì²´ í‰ê·  ì‚¬ìš©
            scores = {
                'faithfulness': latest_results.get('faithfulness', 0),
                'answer_relevancy': latest_results.get('answer_relevancy', 0),
                'context_recall': latest_results.get('context_recall', 0),
                'context_precision': latest_results.get('context_precision', 0)
            }
        
        analysis_data.append({
            'qa_id': f'Q{i+1}',
            'question_length': question_length,
            'context_count': context_count,
            'avg_context_length': avg_context_length,
            'total_context_length': total_context_length,
            **scores
        })
    
    df_analysis = pd.DataFrame(analysis_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("##### ğŸ“Š íŠ¹ì„±ë³„ ì„±ëŠ¥ ì˜í–¥")
        
        # ì§ˆë¬¸ ê¸¸ì´ì™€ ì„±ëŠ¥ ìƒê´€ê´€ê³„
        fig = go.Figure()
        
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        colors = ['blue', 'green', 'orange', 'red']
        
        for metric, color in zip(metrics, colors):
            fig.add_trace(go.Scatter(
                x=df_analysis['question_length'],
                y=df_analysis[metric],
                mode='markers+lines',
                name=metric.replace('_', ' ').title(),
                marker=dict(color=color, size=10)
            ))
        
        fig.update_layout(
            title="ì§ˆë¬¸ ê¸¸ì´ vs ì„±ëŠ¥",
            xaxis_title="ì§ˆë¬¸ ê¸¸ì´ (ë‹¨ì–´)",
            yaxis_title="ì ìˆ˜",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ìƒê´€ê´€ê³„ ìˆ˜ì¹˜ (ì•ˆì „í•œ ê³„ì‚°)
        st.markdown("##### ğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„")
        
        if len(df_analysis) > 1:
            correlations = []
            for metric in metrics:
                try:
                    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                    x_data = df_analysis['question_length'].dropna()
                    y_data = df_analysis[metric].dropna()
                    
                    if len(x_data) > 1 and len(y_data) > 1 and x_data.std() > 0 and y_data.std() > 0:
                        corr = x_data.corr(y_data)
                        if pd.isna(corr):
                            corr_text = "ê³„ì‚°ë¶ˆê°€"
                            interpretation = "ë°ì´í„° ë¶€ì¡±"
                        else:
                            corr_text = f"{corr:.3f}"
                            if abs(corr) > 0.7:
                                interpretation = 'ê°•í•œ ìƒê´€ê´€ê³„'
                            elif abs(corr) > 0.3:
                                interpretation = 'ë³´í†µ ìƒê´€ê´€ê³„'
                            elif abs(corr) > 0.1:
                                interpretation = 'ì•½í•œ ìƒê´€ê´€ê³„'
                            else:
                                interpretation = 'ë¬´ìƒê´€'
                    else:
                        corr_text = "ê³„ì‚°ë¶ˆê°€"
                        interpretation = "ë¶„ì‚° ë¶€ì¡±"
                    
                    correlations.append({
                        'ë©”íŠ¸ë¦­': metric.replace('_', ' ').title(),
                        'ìƒê´€ê³„ìˆ˜': corr_text,
                        'í•´ì„': interpretation
                    })
                except Exception:
                    correlations.append({
                        'ë©”íŠ¸ë¦­': metric.replace('_', ' ').title(),
                        'ìƒê´€ê³„ìˆ˜': "ì˜¤ë¥˜",
                        'í•´ì„': "ê³„ì‚° ì‹¤íŒ¨"
                    })
            
            st.dataframe(pd.DataFrame(correlations), use_container_width=True)
        else:
            st.info("ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ í‰ê°€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with col2:
        st.markdown("##### ğŸ“š ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± vs ì„±ëŠ¥")
        
        # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ì™€ precision ìƒê´€ê´€ê³„
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=df_analysis['context_count'],
            y=df_analysis['context_precision'],
            mode='markers',
            marker=dict(size=df_analysis['total_context_length'], 
                       color=df_analysis['answer_relevancy'],
                       colorscale='Viridis',
                       showscale=True,
                       colorbar=dict(title="Answer Relevancy")),
            text=df_analysis['qa_id'],
            name='Context Precision vs Count'
        ))
        
        fig.update_layout(
            title="ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ vs Precision<br>(í¬ê¸°=ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´, ìƒ‰ìƒ=Answer Relevancy)",
            xaxis_title="ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜",
            yaxis_title="Context Precision",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        st.markdown("##### ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸")
        
        insights = [
            "ğŸ¯ **ì§ˆë¬¸ ê¸¸ì´**: ì ì ˆí•œ ê¸¸ì´(7-12ë‹¨ì–´)ê°€ ìµœì  ì„±ëŠ¥ì„ ë³´ì„",
            "ğŸ“š **ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜**: 3-5ê°œê°€ precisionê³¼ recallì˜ ê· í˜•ì ",
            "ğŸ” **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: ë„ˆë¬´ ê¸¸ë©´ precision ì €í•˜, ë„ˆë¬´ ì§§ìœ¼ë©´ recall ì €í•˜",
            "âš¡ **ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„**: Precisionê³¼ Recallì€ ë°˜ë¹„ë¡€ ê´€ê³„",
            "ğŸ¨ **ìµœì í™” ì „ëµ**: Context í’ˆì§ˆ > Context ì–‘"
        ]
        
        for insight in insights:
            st.write(insight)

