"""
RAGTrace Dashboard - Enhanced with Full Features

main ë¸Œëœì¹˜ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•œ ì™„ì „í•œ RAGTrace ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤.
"""

import json
import random
import sqlite3
from datetime import datetime
import warnings

# torch ê´€ë ¨ ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="torch")
warnings.filterwarnings("ignore", message=".*torch.classes.*")

import pandas as pd
import plotly.graph_objects as go
import streamlit as st

# í˜ì´ì§€ ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
st.set_page_config(
    page_title="RAGTrace ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

from src.domain.prompts import PromptType
from src.utils.paths import (
    DATABASE_PATH,
    get_available_datasets,
    get_evaluation_data_path,
)
from src.presentation.web.components.llm_selector import show_llm_selector
from src.presentation.web.components.embedding_selector import show_embedding_selector
from src.presentation.web.components.prompt_selector import show_prompt_selector


# í˜ì´ì§€ ì •ì˜
def load_pages():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ì§€ ëª©ë¡ ë°˜í™˜"""
    return {
        "ğŸ¯ Overview": "ë©”ì¸ ëŒ€ì‹œë³´ë“œ",
        "ğŸš€ New Evaluation": "ìƒˆ í‰ê°€ ì‹¤í–‰",
        "ğŸ“ˆ Historical": "ê³¼ê±° í‰ê°€ ê²°ê³¼",
        "ğŸ“š Detailed Analysis": "ìƒì„¸ ë¶„ì„",
        "ğŸ“– Metrics Explanation": "ë©”íŠ¸ë¦­ ì„¤ëª…",
        "âš¡ Performance": "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
    }


# í˜ì´ì§€ ì„ íƒ ì½œë°± í•¨ìˆ˜
def on_page_change():
    st.session_state.selected_page = st.session_state.page_selector


# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - Streamlit ì•± ì‹œì‘ì """
    # ì‚¬ì´ë“œë°” ë° ë„¤ë¹„ê²Œì´ì…˜
    st.sidebar.title("ğŸ” RAGTrace ëŒ€ì‹œë³´ë“œ")
    
    pages = load_pages()
    page_keys = list(pages.keys())
    
    # í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ìƒíƒœ ê´€ë¦¬
    if "selected_page" not in st.session_state:
        st.session_state.selected_page = "ğŸ¯ Overview"
    
    # ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ìœ¼ë¡œ í˜ì´ì§€ ì´ë™ ì²˜ë¦¬
    if "navigate_to" in st.session_state:
        st.session_state.selected_page = st.session_state.navigate_to
        del st.session_state.navigate_to
    
    # ì‚¬ì´ë“œë°”ì—ì„œ í˜ì´ì§€ ì„ íƒ
    st.sidebar.selectbox(
        "í˜ì´ì§€ ì„ íƒ",
        page_keys,
        index=page_keys.index(st.session_state.selected_page),
        key="page_selector",
        on_change=on_page_change,
    )
    
    page = st.session_state.selected_page
    
    # ê³µí†µ í—¤ë”
    st.title("ğŸ” RAGTrace - RAG ì„±ëŠ¥ ì¶”ì  ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # í˜ì´ì§€ ë¼ìš°íŒ…
    if page == "ğŸ¯ Overview":
        show_overview()
    elif page == "ğŸš€ New Evaluation":
        show_new_evaluation_page()
    elif page == "ğŸ“ˆ Historical":
        show_historical()
    elif page == "ğŸ“š Detailed Analysis":
        show_detailed_analysis()
    elif page == "ğŸ“– Metrics Explanation":
        show_metrics_guide()
    elif page == "âš¡ Performance":
        show_performance()
    else:
        show_overview()


def show_overview():
    """ë©”ì¸ ì˜¤ë²„ë·° ëŒ€ì‹œë³´ë“œ"""
    st.header("ğŸ“Š í‰ê°€ ê²°ê³¼ ê°œìš”")
    
    # ë°©ê¸ˆ ì™„ë£Œëœ í‰ê°€ê°€ ìˆìœ¼ë©´ ì¶•í•˜ ë©”ì‹œì§€ í‘œì‹œ
    if st.session_state.get("evaluation_completed", False):
        st.success("ğŸ‰ ìƒˆë¡œìš´ í‰ê°€ê°€ ë°©ê¸ˆ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        # í•œ ë²ˆ í‘œì‹œ í›„ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.evaluation_completed = False

    # ì•¡ì…˜ ë²„íŠ¼ë“¤
    col1, col2, col3, col4 = st.columns([2, 1, 1, 2])

    with col1:
        if st.button(
            "ğŸš€ ìƒˆ í‰ê°€ ì‹¤í–‰", type="primary", help="ìƒˆë¡œìš´ RAG í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤",
            key="overview_new_eval_btn"
        ):
            st.session_state.navigate_to = "ğŸš€ New Evaluation"
            st.rerun()

    with col2:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", help="ìµœì‹  ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤", key="overview_refresh_btn"):
            st.rerun()

    with col3:
        if st.button("ğŸ“ˆ ì´ë ¥ë³´ê¸°", help="ê³¼ê±° í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤", key="overview_history_btn"):
            st.session_state.navigate_to = "ğŸ“ˆ Historical"
            st.rerun()

    with col4:
        if st.button("ğŸ“š ë©”íŠ¸ë¦­ ê°€ì´ë“œ", help="RAGAS ì ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ì•Œì•„ë³´ì„¸ìš”", key="overview_guide_btn"):
            st.session_state.navigate_to = "ğŸ“– Metrics Explanation"
            st.rerun()

    # ìµœì‹  í‰ê°€ ê²°ê³¼ ë¡œë“œ
    latest_result = load_latest_result()

    if latest_result:
        # í‰ê°€ ê¸°ë³¸ ì •ë³´ í‘œì‹œ
        show_evaluation_info(latest_result)
        show_metric_cards(latest_result)
        show_metric_charts(latest_result)
        show_export_section(latest_result)
        show_recent_trends()
    else:
        st.info(
            "ğŸ“ ì•„ì§ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. 'ìƒˆ í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì²« í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”!"
        )
        st.markdown("---")
        st.markdown("### ğŸ¤” RAGAS ë©”íŠ¸ë¦­ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")
        st.markdown(
            "ğŸ“š ì‚¬ì´ë“œë°”ì—ì„œ **'Metrics Guide'**ë¥¼ ì„ íƒí•˜ë©´ ê° ì ìˆ˜ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì‰½ê²Œ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
        )
    
    # ì»¨í…Œì´ë„ˆ í…ŒìŠ¤íŠ¸ ë²„íŠ¼ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
    st.markdown("---")
    if st.button("ğŸ§ª ì»¨í…Œì´ë„ˆ í…ŒìŠ¤íŠ¸", key="overview_container_test_btn"):
        try:
            with st.spinner("ì»¨í…Œì´ë„ˆ ë¡œë”© ì¤‘..."):
                from src.container import container
                from src.container.factories.evaluation_use_case_factory import EvaluationRequest
                
                request = EvaluationRequest(
                    llm_type="gemini",
                    embedding_type="gemini",
                    prompt_type=PromptType.DEFAULT
                )
                
                evaluation_use_case, llm_adapter, embedding_adapter = container.create_evaluation_use_case(request)
                st.success("âœ… ì»¨í…Œì´ë„ˆê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.info(f"LLM Adapter: {type(llm_adapter).__name__}")
                st.info(f"Embedding Adapter: {type(embedding_adapter).__name__}")
                
        except Exception as e:
            st.error(f"âŒ ì»¨í…Œì´ë„ˆ ë¡œë”© ì‹¤íŒ¨: {str(e)}")


def show_evaluation_info(result):
    """í‰ê°€ ê¸°ë³¸ ì •ë³´ í‘œì‹œ"""
    st.subheader("ğŸ“‹ í‰ê°€ ì •ë³´")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        qa_count = result.get('qa_count', 'N/A')
        st.markdown(f"**QA ê°œìˆ˜**")
        st.markdown(f"<span style='font-size: 16px;'>{qa_count}</span>", unsafe_allow_html=True)
    
    with col2:
        eval_id = result.get('evaluation_id', 'N/A')
        if eval_id != 'N/A' and len(str(eval_id)) > 8:
            eval_id = str(eval_id)[:8]
        st.markdown(f"**í‰ê°€ ID**")
        st.markdown(f"<span style='font-size: 16px;'>{eval_id}</span>", unsafe_allow_html=True)
    
    with col3:
        llm_model = result.get('llm_model', 'N/A')
        st.markdown(f"**LLM ëª¨ë¸**")
        st.markdown(f"<span style='font-size: 16px;'>{llm_model}</span>", unsafe_allow_html=True)
    
    with col4:
        embedding_model = result.get('embedding_model', 'N/A')
        st.markdown(f"**ì„ë² ë”© ëª¨ë¸**")
        st.markdown(f"<span style='font-size: 16px;'>{embedding_model}</span>", unsafe_allow_html=True)
    
    dataset_name = result.get('dataset_name', 'N/A')
    st.markdown(f"**ë°ì´í„°ì…‹**: {dataset_name}")
    
    st.markdown("---")


def show_metric_cards(result):
    """ë©”íŠ¸ë¦­ ì¹´ë“œ í‘œì‹œ"""
    st.subheader("ğŸ¯ í•µì‹¬ ì§€í‘œ")

    # answer_correctnessê°€ ìˆëŠ”ì§€ í™•ì¸
    has_answer_correctness = "answer_correctness" in result

    # ì»¬ëŸ¼ ìˆ˜ ë™ì  ì„¤ì •
    if has_answer_correctness:
        cols = st.columns(6)
    else:
        cols = st.columns(5)

    metrics = [
        ("ì¢…í•© ì ìˆ˜", result.get("ragas_score", 0), "ğŸ†"),
        ("Faithfulness", result.get("faithfulness", 0), "âœ…"),
        ("Answer Relevancy", result.get("answer_relevancy", 0), "ğŸ¯"),
        ("Context Recall", result.get("context_recall", 0), "ğŸ”„"),
        ("Context Precision", result.get("context_precision", 0), "ğŸ“"),
    ]
    
    # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if has_answer_correctness:
        metrics.append(("Answer Correctness", result.get("answer_correctness", 0), "âœ”ï¸"))

    for i, (name, value, icon) in enumerate(metrics):
        with cols[i]:
            # ì´ì „ í‰ê°€ì™€ì˜ ë¹„êµë¥¼ ìœ„í•œ ë¸íƒ€ ê³„ì‚°
            previous_result = get_previous_result()
            delta_value = None
            if previous_result and name.lower().replace(" ", "_") in previous_result:
                prev_value = previous_result[name.lower().replace(" ", "_")]
                # ì•ˆì „í•œ íƒ€ì… ì²´í¬ ë° ë³€í™˜
                if prev_value is not None and isinstance(prev_value, (int, float)) and isinstance(value, (int, float)):
                    delta_value = value - prev_value

            # ì•ˆì „í•œ value í‘œì‹œ
            try:
                value_str = f"{float(value):.3f}" if value is not None else "0.000"
            except (ValueError, TypeError):
                value_str = "0.000"
            
            st.metric(
                label=f"{icon} {name}",
                value=value_str,
                delta=f"{delta_value:.3f}" if delta_value is not None else None,
            )

    # í‰ê°€ ì‹œê°„ ì •ë³´ í‘œì‹œ
    metadata = result.get("metadata", {})
    if metadata.get("total_duration_seconds"):
        st.subheader("â±ï¸ í‰ê°€ ì„±ëŠ¥")
        
        time_col1, time_col2, time_col3 = st.columns(3)
        
        with time_col1:
            st.metric(
                label="â±ï¸ ì´ í‰ê°€ ì‹œê°„",
                value=f"{metadata.get('total_duration_minutes', 0):.1f}ë¶„",
                help=f"{metadata.get('total_duration_seconds', 0):.1f}ì´ˆ"
            )
        
        with time_col2:
            st.metric(
                label="ğŸ“Š í‰ê·  ë¬¸í•­ ì‹œê°„",
                value=f"{metadata.get('avg_time_per_item_seconds', 0):.1f}ì´ˆ",
                help="ë¬¸í•­ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„"
            )
        
        with time_col3:
            dataset_size = metadata.get('dataset_size', 0)
            throughput = dataset_size / metadata.get('total_duration_seconds', 1) * 60 if metadata.get('total_duration_seconds') else 0
            st.metric(
                label="ğŸš€ ì²˜ë¦¬ ì†ë„",
                value=f"{throughput:.1f}ë¬¸í•­/ë¶„",
                help=f"ì´ {dataset_size}ê°œ ë¬¸í•­"
            )


def show_metric_charts(result):
    """ë©”íŠ¸ë¦­ ì‹œê°í™” ì°¨íŠ¸"""
    st.subheader("ğŸ“ˆ ì‹œê°í™”")

    col1, col2 = st.columns(2)

    with col1:
        # ë ˆì´ë” ì°¨íŠ¸
        show_radar_chart(result)

    with col2:
        # ë°” ì°¨íŠ¸
        show_bar_chart(result)


def show_export_section(result):
    """ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì„¹ì…˜"""
    st.subheader("ğŸ“¥ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ“Š CSV ë‹¤ìš´ë¡œë“œ", key="download_csv_btn", help="ìƒì„¸ í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ"):
            try:
                from src.application.services.result_exporter import ResultExporter
                
                exporter = ResultExporter()
                csv_file = exporter.export_to_csv(result)
                
                # íŒŒì¼ ì½ê¸°
                with open(csv_file, 'rb') as f:
                    csv_data = f.read()
                
                st.download_button(
                    label="ğŸ“„ CSV íŒŒì¼ ì €ì¥",
                    data=csv_data,
                    file_name=f"ragas_detailed_{result.get('metadata', {}).get('evaluation_id', 'unknown')}.csv",
                    mime="text/csv",
                    key="csv_download"
                )
                
                st.success("âœ… CSV íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except Exception as e:
                st.error(f"âŒ CSV ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    with col2:
        if st.button("ğŸ“ˆ ìš”ì•½ í†µê³„ CSV", key="download_summary_btn", help="ë©”íŠ¸ë¦­ë³„ ê¸°ì´ˆ í†µê³„ë¥¼ CSVë¡œ ë‹¤ìš´ë¡œë“œ"):
            try:
                from src.application.services.result_exporter import ResultExporter
                
                exporter = ResultExporter()
                summary_file = exporter.export_summary_csv(result)
                
                # íŒŒì¼ ì½ê¸°
                with open(summary_file, 'rb') as f:
                    summary_data = f.read()
                
                st.download_button(
                    label="ğŸ“Š ìš”ì•½ CSV ì €ì¥",
                    data=summary_data,
                    file_name=f"ragas_summary_{result.get('metadata', {}).get('evaluation_id', 'unknown')}.csv",
                    mime="text/csv",
                    key="summary_download"
                )
                
                st.success("âœ… ìš”ì•½ í†µê³„ CSVê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except Exception as e:
                st.error(f"âŒ ìš”ì•½ CSV ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    with col3:
        if st.button("ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ", key="download_report_btn", help="ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"):
            try:
                from src.application.services.result_exporter import ResultExporter
                
                exporter = ResultExporter()
                report_file = exporter.generate_analysis_report(result)
                
                # íŒŒì¼ ì½ê¸°
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = f.read()
                
                st.download_button(
                    label="ğŸ“„ ë³´ê³ ì„œ ì €ì¥",
                    data=report_data.encode('utf-8'),
                    file_name=f"ragas_analysis_{result.get('metadata', {}).get('evaluation_id', 'unknown')}.md",
                    mime="text/markdown",
                    key="report_download"
                )
                
                st.success("âœ… ë¶„ì„ ë³´ê³ ì„œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except Exception as e:
                st.error(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    # ì „ì²´ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ
    st.markdown("---")
    
    if st.button("ğŸ“¦ ì „ì²´ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ", key="download_package_btn", help="CSV, ìš”ì•½, ë³´ê³ ì„œë¥¼ ëª¨ë‘ í¬í•¨í•œ ZIP íŒŒì¼"):
        try:
            import zipfile
            import tempfile
            from pathlib import Path
            
            from src.application.services.result_exporter import ResultExporter
            
            exporter = ResultExporter()
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ëª¨ë“  íŒŒì¼ ìƒì„±
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # ëª¨ë“  íŒŒì¼ ìƒì„±
                files = exporter.export_full_package(result)
                
                # ZIP íŒŒì¼ ìƒì„±
                zip_path = temp_path / "ragas_evaluation_package.zip"
                
                with zipfile.ZipFile(zip_path, 'w') as zip_file:
                    for file_type, file_path in files.items():
                        zip_file.write(file_path, Path(file_path).name)
                
                # ZIP íŒŒì¼ ì½ê¸°
                with open(zip_path, 'rb') as f:
                    zip_data = f.read()
                
                st.download_button(
                    label="ğŸ“¦ ZIP íŒ¨í‚¤ì§€ ì €ì¥",
                    data=zip_data,
                    file_name=f"ragas_package_{result.get('metadata', {}).get('evaluation_id', 'unknown')}.zip",
                    mime="application/zip",
                    key="package_download"
                )
                
                st.success("âœ… ì „ì²´ íŒ¨í‚¤ì§€ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.info("ğŸ“‹ í¬í•¨ëœ íŒŒì¼: ìƒì„¸ CSV, ìš”ì•½ CSV, ë¶„ì„ ë³´ê³ ì„œ")
                
        except Exception as e:
            st.error(f"âŒ íŒ¨í‚¤ì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            st.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")


def show_radar_chart(result):
    """ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    labels = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]
    
    # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if "answer_correctness" in result:
        metrics.append("answer_correctness")
        labels.append("Answer Correctness")
    
    values = [result.get(metric, 0) for metric in metrics]
    
    # ì•ˆì „í•œ ê°’ ë³€í™˜
    safe_values = []
    for v in values:
        try:
            safe_values.append(float(v) if v is not None else 0.0)
        except (ValueError, TypeError):
            safe_values.append(0.0)

    fig = go.Figure()

    fig.add_trace(
        go.Scatterpolar(
            r=safe_values + [safe_values[0]],  # ì²« ë²ˆì§¸ ê°’ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€í•˜ì—¬ ì°¨íŠ¸ë¥¼ ë‹«ìŒ
            theta=labels + [labels[0]],
            fill="toself",
            name="RAGAS ì ìˆ˜",
            line_color="rgb(32, 201, 151)",
        )
    )

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=True,
        title="ğŸ“Š ë©”íŠ¸ë¦­ ê· í˜•ë„",
        height=400,
    )

    st.plotly_chart(fig, use_container_width=True)


def show_bar_chart(result):
    """ë°” ì°¨íŠ¸ ìƒì„±"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    labels = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]
    
    # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if "answer_correctness" in result:
        metrics.append("answer_correctness")
        labels.append("Answer Correctness")
    
    values = [result.get(metric, 0) for metric in metrics]
    
    # ì•ˆì „í•œ ê°’ ë³€í™˜
    safe_values = []
    for v in values:
        try:
            safe_values.append(float(v) if v is not None else 0.0)
        except (ValueError, TypeError):
            safe_values.append(0.0)

    # ìƒ‰ìƒ ë§¤í•‘
    colors = ["green" if v >= 0.8 else "orange" if v >= 0.6 else "red" for v in safe_values]

    fig = go.Figure(data=[go.Bar(x=labels, y=safe_values, marker_color=colors)])

    fig.update_layout(
        title="ğŸ“Š ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥",
        yaxis_title="ì ìˆ˜",
        yaxis=dict(range=[0, 1]),
        height=400,
    )

    st.plotly_chart(fig, use_container_width=True)


def show_recent_trends():
    """ìµœê·¼ íŠ¸ë Œë“œ í‘œì‹œ"""
    st.subheader("ğŸ“ˆ ìµœê·¼ íŠ¸ë Œë“œ")

    # íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
    history = load_evaluation_history(limit=10)

    if len(history) > 1:
        df = pd.DataFrame(history)
        df["timestamp"] = pd.to_datetime(df["timestamp"])

        # íŠ¸ë Œë“œ ì°¨íŠ¸
        fig = go.Figure()

        metrics = [
            "faithfulness",
            "answer_relevancy",
            "context_recall",
            "context_precision",
            "ragas_score",
        ]
        colors = ["blue", "green", "orange", "red", "purple"]

        for metric, color in zip(metrics, colors, strict=False):
            if metric in df.columns:
                fig.add_trace(
                    go.Scatter(
                        x=df["timestamp"],
                        y=df[metric],
                        mode="lines+markers",
                        name=metric.replace("_", " ").title(),
                        line=dict(color=color),
                    )
                )

        fig.update_layout(
            title="ğŸ“ˆ í‰ê°€ ì ìˆ˜ íŠ¸ë Œë“œ",
            xaxis_title="ì‹œê°„",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=400,
        )

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ğŸ“Š íŠ¸ë Œë“œ í‘œì‹œë¥¼ ìœ„í•´ ë” ë§ì€ í‰ê°€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


def show_new_evaluation_page():
    """ìƒˆ í‰ê°€ ì‹¤í–‰ í˜ì´ì§€"""
    st.header("ğŸš€ ìƒˆ í‰ê°€ ì‹¤í–‰")
    
    # LLM ì„ íƒ UI í‘œì‹œ
    selected_llm = show_llm_selector()
    
    st.markdown("---")
    
    # ì„ë² ë”© ì„ íƒ UI í‘œì‹œ
    selected_embedding = show_embedding_selector()
    
    st.markdown("---")
    
    # í”„ë¡¬í”„íŠ¸ ì„ íƒ UI í‘œì‹œ
    selected_prompt_type = show_prompt_selector()
    
    st.markdown("---")
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    st.markdown("### ğŸ“Š ë°ì´í„°ì…‹ ì„ íƒ")
    
    # íƒ­ìœ¼ë¡œ ê¸°ì¡´ íŒŒì¼ ì„ íƒê³¼ ìƒˆ íŒŒì¼ ì—…ë¡œë“œ êµ¬ë¶„
    dataset_tab1, dataset_tab2 = st.tabs(["ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“‚ ê¸°ì¡´ ë°ì´í„°ì…‹"])
    
    with dataset_tab1:
        st.markdown("#### ìƒˆ ë°ì´í„°ì…‹ ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader(
            "í‰ê°€ ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=["json", "xlsx", "xls", "csv"],
            help="JSON, Excel(.xlsx, .xls), CSV íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤.",
            key="file_uploader"
        )
        
        if uploaded_file is not None:
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            file_extension = uploaded_file.name.split('.')[-1].lower()
            
            try:
                if file_extension == 'json':
                    # JSON íŒŒì¼ ì§ì ‘ ë¡œë“œ
                    qa_data = json.load(uploaded_file)
                    st.success(f"âœ… JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {uploaded_file.name} ({len(qa_data)}ê°œ í•­ëª©)")
                    
                elif file_extension in ['xlsx', 'xls', 'csv']:
                    # Excel/CSV íŒŒì¼ì€ ë³€í™˜ í•„ìš”
                    st.info("ğŸ“„ Excel/CSV íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘...")
                    
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    import tempfile
                    import os
                    from pathlib import Path
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_file:
                        tmp_file.write(uploaded_file.read())
                        tmp_file_path = tmp_file.name
                    
                    try:
                        # data importer ì‚¬ìš©
                        from src.infrastructure.data_import.importers import ImporterFactory
                        
                        importer = ImporterFactory.create_importer(tmp_file_path)
                        evaluation_data_list = importer.import_data(tmp_file_path)
                        
                        # EvaluationData ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
                        qa_data = [
                            {
                                "question": item.question,
                                "contexts": item.contexts,
                                "answer": item.answer,
                                "ground_truth": item.ground_truth
                            }
                            for item in evaluation_data_list
                        ]
                        
                        st.success(f"âœ… {file_extension.upper()} íŒŒì¼ ë³€í™˜ ì™„ë£Œ: {uploaded_file.name} ({len(qa_data)}ê°œ í•­ëª©)")
                        
                        # ë³€í™˜ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                        with st.expander("ğŸ“‹ ë³€í™˜ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3ê°œ)"):
                            for i, item in enumerate(qa_data[:3]):
                                st.markdown(f"**í•­ëª© {i+1}:**")
                                st.json(item)
                        
                    finally:
                        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        os.unlink(tmp_file_path)
                else:
                    st.error(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
                    qa_data = None
                    
                # ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ session_stateì— ì €ì¥
                if qa_data:
                    st.session_state.uploaded_data = qa_data
                    st.session_state.uploaded_filename = uploaded_file.name
                    st.session_state.use_uploaded_file = True
                    
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                st.info("íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    with dataset_tab2:
        st.markdown("#### ê¸°ì¡´ ë°ì´í„°ì…‹ ì„ íƒ")
        existing_datasets = get_available_datasets()
        if not existing_datasets:
            st.info("ğŸ“ ê¸°ì¡´ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ data/ ë””ë ‰í† ë¦¬ì— ì¶”ê°€í•˜ì„¸ìš”.")
            selected_dataset = None
        else:
            # session_stateì— ì„ íƒëœ ë°ì´í„°ì…‹ ì €ì¥
            if "selected_dataset" not in st.session_state:
                st.session_state.selected_dataset = existing_datasets[0]
            
            # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            try:
                current_index = existing_datasets.index(st.session_state.selected_dataset)
            except (ValueError, IndexError):
                current_index = 0
                st.session_state.selected_dataset = existing_datasets[0] if existing_datasets else None
            
            # ë°ì´í„°ì…‹ ì„ íƒ UI
            selected_dataset = st.selectbox(
                "í‰ê°€í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:",
                existing_datasets,
                index=current_index,
                key="dataset_selector_box",
                help="í‰ê°€ì— ì‚¬ìš©í•  QA ë°ì´í„°ì…‹ì„ ì„ íƒí•©ë‹ˆë‹¤."
            )
            
            # ì„ íƒì´ ë³€ê²½ë˜ë©´ session_state ì—…ë°ì´íŠ¸
            st.session_state.selected_dataset = selected_dataset
            st.session_state.use_uploaded_file = False
            
            # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
            dataset_path = get_evaluation_data_path(selected_dataset)
            if dataset_path:
                try:
                    from pathlib import Path
                    from src.infrastructure.data_import.importers import ExcelImporter, CSVImporter
                    
                    file_path_obj = Path(dataset_path)
                    file_extension = file_path_obj.suffix.lower()
                    
                    if file_extension in ['.xlsx', '.xls']:
                        # Excel íŒŒì¼ ì²˜ë¦¬
                        importer = ExcelImporter()
                        evaluation_data_list = importer.import_data(file_path_obj)
                        qa_count = len(evaluation_data_list)
                    elif file_extension == '.csv':
                        # CSV íŒŒì¼ ì²˜ë¦¬
                        importer = CSVImporter()
                        evaluation_data_list = importer.import_data(file_path_obj)
                        qa_count = len(evaluation_data_list)
                    else:
                        # JSON íŒŒì¼ ì²˜ë¦¬ (ê¸°ë³¸)
                        with open(dataset_path, encoding="utf-8") as f:
                            qa_data = json.load(f)
                            qa_count = len(qa_data)
                    
                    st.info(f"ğŸ“‹ ì„ íƒëœ ë°ì´í„°ì…‹: **{selected_dataset}** ({qa_count}ê°œ QA ìŒ)")
                except Exception as e:
                    st.warning(f"ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… ì„ íƒëœ ë°ì´í„°ì…‹ ì´ë¦„ ê²°ì •
    if st.session_state.get('use_uploaded_file', False) and st.session_state.get('uploaded_filename'):
        selected_dataset = st.session_state.uploaded_filename
    elif not st.session_state.get('use_uploaded_file', False) and dataset_tab2:
        selected_dataset = st.session_state.get('selected_dataset', None)
    
    st.markdown("---")
    
    # í‰ê°€ ì„¤ì • ìš”ì•½
    st.markdown("### ğŸ“‹ í‰ê°€ ì„¤ì • ìš”ì•½")
    col1, col2, col3, col4 = st.columns(4)
    
    # í‘œì‹œëª… ë§¤í•‘
    llm_display_names = {
        "gemini": "ğŸŒ Google Gemini 2.5 Flash",
        "hcx": "ğŸš€ NAVER HyperCLOVA X"
    }
    
    embedding_display_names = {
        "gemini": "ğŸŒ Google Gemini Embedding",
        "bge_m3": "ğŸ¯ BGE-M3 Local Embedding", 
        "hcx": "ğŸš€ NAVER HCX Embedding"
    }
    
    with col1:
        st.write(f"**ğŸ¤– LLM ëª¨ë¸:** {llm_display_names.get(selected_llm, selected_llm)}")
    with col2:
        st.write(f"**ğŸ” ì„ë² ë”© ëª¨ë¸:** {embedding_display_names.get(selected_embedding, selected_embedding)}")
    with col3:
        st.write(f"**ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…:** {selected_prompt_type.value}")
    with col4:
        st.write(f"**ğŸ“Š ë°ì´í„°ì…‹:** {selected_dataset}")
    
    st.markdown("---")
    
    # í‰ê°€ ì‹¤í–‰ ë²„íŠ¼ë“¤
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        if st.button("â† ë’¤ë¡œê°€ê¸°", key="new_eval_back_btn"):
            st.session_state.navigate_to = "ğŸ¯ Overview"
            st.rerun()
    
    with col2:
        if st.button("ğŸš€ í‰ê°€ ì‹œì‘", type="primary", use_container_width=True, key="new_eval_start_btn"):
            execute_evaluation(selected_prompt_type, selected_dataset, selected_llm, selected_embedding)
    
    with col3:
        st.write("")  # ë¹ˆ ê³µê°„


# ì»´í¬ë„ŒíŠ¸ í•¨ìˆ˜ë“¤ (ì§€ì—° ë¡œë”©)
def show_llm_selector():
    """LLM ì„ íƒê¸° (ì§€ì—° ë¡œë”©)"""
    try:
        from src.presentation.web.components.llm_selector import show_llm_selector as _show_llm_selector
        return _show_llm_selector()
    except ImportError:
        # ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ UI
        st.markdown("### ğŸ¤– LLM ëª¨ë¸ ì„ íƒ")
        llm_options = {
            "ğŸŒ Google Gemini 2.5 Flash": "gemini",
            "ğŸš€ NAVER HyperCLOVA X": "hcx"
        }
        selected_display = st.selectbox(
            "ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
            list(llm_options.keys()),
            key="llm_selector"
        )
        return llm_options[selected_display]


def show_embedding_selector():
    """ì„ë² ë”© ì„ íƒê¸° (ì§€ì—° ë¡œë”©)"""
    try:
        from src.presentation.web.components.embedding_selector import show_embedding_selector as _show_embedding_selector
        return _show_embedding_selector()
    except ImportError:
        # ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ UI
        st.markdown("### ğŸ” ì„ë² ë”© ëª¨ë¸ ì„ íƒ")
        embedding_options = {
            "ğŸŒ Google Gemini Embedding": "gemini",
            "ğŸ¯ BGE-M3 Local Embedding": "bge_m3",
            "ğŸš€ NAVER HCX Embedding": "hcx"
        }
        selected_display = st.selectbox(
            "ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
            list(embedding_options.keys()),
            key="embedding_selector"
        )
        return embedding_options[selected_display]


def show_prompt_selector():
    """í”„ë¡¬í”„íŠ¸ ì„ íƒê¸° (ì§€ì—° ë¡œë”©)"""
    try:
        from src.presentation.web.components.prompt_selector import show_prompt_selector as _show_prompt_selector
        return _show_prompt_selector()
    except ImportError:
        # ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ UI
        st.markdown("### ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ")
        prompt_options = [PromptType.DEFAULT, PromptType.KOREAN_FORMAL, PromptType.NUCLEAR_HYDRO_TECH]
        selected = st.selectbox("í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ", 
                               [p.value for p in prompt_options], 
                               key="prompt_selector")
        return next(p for p in prompt_options if p.value == selected)


def execute_evaluation(prompt_type: PromptType, dataset_name: str, llm_type: str, embedding_type: str):
    """í‰ê°€ ì‹¤í–‰ ë¡œì§"""
    with st.spinner("ğŸ”„ í‰ê°€ë¥¼ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if st.session_state.get('use_uploaded_file', False) and st.session_state.get('uploaded_data'):
                # ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                import tempfile
                import os
                from pathlib import Path
                
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json', encoding='utf-8') as tmp_file:
                    json.dump(st.session_state.uploaded_data, tmp_file, ensure_ascii=False, indent=2)
                    temp_dataset_path = tmp_file.name
                
                # ì„ì‹œ ë°ì´í„°ì…‹ ì´ë¦„ ì„¤ì •
                actual_dataset_name = f"uploaded_{dataset_name}"
            else:
                temp_dataset_path = None
                actual_dataset_name = dataset_name
            # í‰ê°€ ì„¤ì • ì •ë³´ í‘œì‹œ
            st.markdown("### ğŸ”§ í‰ê°€ ì„¤ì •")
            col1, col2 = st.columns(2)
            
            with col1:
                st.info(f"ğŸ¤– **LLM ëª¨ë¸**: {llm_type}")
                st.info(f"ğŸ“Š **ë°ì´í„°ì…‹**: {dataset_name}")
            
            with col2:
                st.info(f"ğŸ” **ì„ë² ë”© ëª¨ë¸**: {embedding_type}")
                st.info(f"ğŸ¯ **í”„ë¡¬í”„íŠ¸ íƒ€ì…**: {prompt_type.value}")
            
            # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ëª… ì¶”ê°€
            if prompt_type == PromptType.DEFAULT:
                st.success("ğŸ“ **ê¸°ë³¸ RAGAS í”„ë¡¬í”„íŠ¸ (ì˜ì–´)** - ë²”ìš©ì ì´ê³  ì•ˆì •ì ì¸ í‰ê°€")
            elif prompt_type == PromptType.NUCLEAR_HYDRO_TECH:
                st.success("âš›ï¸ **ì›ìë ¥/ìˆ˜ë ¥ ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™” í”„ë¡¬í”„íŠ¸** - ê¸°ìˆ  ì •í™•ì„±ê³¼ ì•ˆì „ ê·œì •ì— ìµœì í™”")
            elif prompt_type == PromptType.KOREAN_FORMAL:
                st.success("ğŸ“‹ **í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ íŠ¹í™” í”„ë¡¬í”„íŠ¸** - ì •ì±… ë¬¸ì„œì™€ ë²•ê·œ í•´ì„ì— ìµœì í™”")
            
            st.markdown("---")

            # HCX ì„ íƒ ì‹œ API í‚¤ í™•ì¸ ë° ì‚¬ìš©ì ì•ˆë‚´
            if llm_type == "hcx" or embedding_type == "hcx":
                from src.config import settings
                if not settings.CLOVA_STUDIO_API_KEY:
                    st.error("âŒ HCX ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
                    return
                else:
                    st.warning("âš ï¸ **HCX API ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­**")
                    st.markdown("""
                    - HCX APIëŠ” ìš”ì²­ í•œë„ê°€ ìˆì–´ 429 ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
                    - ì‹¤íŒ¨í•œ í‰ê°€ëŠ” ìë™ìœ¼ë¡œ ì¬ì‹œë„ë©ë‹ˆë‹¤ (ìµœëŒ€ 3íšŒ)
                    - ëŒ€ëŸ‰ í‰ê°€ ì‹œì—ëŠ” Gemini ëª¨ë¸ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤
                    """)
                    st.markdown("---")

            # ì»¨í…Œì´ë„ˆ ë¡œë”© (ì§€ì—° ë¡œë”©)
            st.info("ğŸ”§ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆ ë°©ì‹ ì‚¬ìš©
            from src.container import container
            from src.container.factories.evaluation_use_case_factory import EvaluationRequest
            
            request = EvaluationRequest(
                llm_type=llm_type,
                embedding_type=embedding_type,
                prompt_type=prompt_type
            )
            evaluation_use_case, llm_adapter, embedding_adapter = container.create_evaluation_use_case(request)
            
            st.info("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ë° ê²€ì¦ ì¤‘...")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_placeholder = st.empty()
            
            with progress_placeholder.container():
                if llm_type == "hcx" or embedding_type == "hcx":
                    st.info("âš¡ í‰ê°€ ì‹¤í–‰ ì¤‘... (HCX API ì‚¬ìš©ìœ¼ë¡œ 10-15ë¶„ ì†Œìš” ì˜ˆìƒ)")
                    st.warning("ğŸ”„ HCX API 429 ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì¤‘ë‹¨í•˜ì§€ ë§ˆì‹œê³  ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                    st.info("ğŸ“Š ì„¤ì •: ì›Œì»¤ 1ê°œ, ì¬ì‹œë„ 8íšŒ, ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©")
                else:
                    st.info("âš¡ í‰ê°€ ì‹¤í–‰ ì¤‘... (2-5ë¶„ ì†Œìš” ì˜ˆìƒ)")
                
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                # í‰ê°€ ì‹¤í–‰
                progress_text.text("í‰ê°€ ì‹œì‘...")
                progress_bar.progress(25)
                
                # ì—…ë¡œë“œëœ íŒŒì¼ì¸ ê²½ìš° ê²½ë¡œ ì§ì ‘ ì „ë‹¬
                if temp_dataset_path:
                    # ì„ì‹œë¡œ ë°ì´í„°ë¥¼ data/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
                    from pathlib import Path
                    import shutil
                    
                    data_dir = Path("data")
                    data_dir.mkdir(exist_ok=True)
                    temp_data_file = data_dir / f"temp_{dataset_name}.json"
                    
                    shutil.copy(temp_dataset_path, temp_data_file)
                    
                    try:
                        evaluation_result = evaluation_use_case.execute(
                            dataset_name=f"temp_{dataset_name}"
                        )
                    finally:
                        # í‰ê°€ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        if temp_data_file.exists():
                            temp_data_file.unlink()
                        if temp_dataset_path and os.path.exists(temp_dataset_path):
                            os.unlink(temp_dataset_path)
                else:
                    evaluation_result = evaluation_use_case.execute(
                        dataset_name=dataset_name
                    )
                
                progress_bar.progress(100)
                progress_text.text("í‰ê°€ ì™„ë£Œ!")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
            progress_placeholder.empty()

            # ê²°ê³¼ ì €ì¥
            result_dict = evaluation_result.to_dict()
            if "metadata" not in result_dict:
                result_dict["metadata"] = {}
            result_dict["metadata"]["llm_type"] = llm_type
            result_dict["metadata"]["embedding_type"] = embedding_type
            result_dict["metadata"]["dataset"] = dataset_name
            result_dict["metadata"]["prompt_type"] = prompt_type.value

            # ì¶”ê°€ ì •ë³´ ì €ì¥
            import uuid
            evaluation_id = str(uuid.uuid4())[:8]
            
            # LLMê³¼ ì„ë² ë”© ëª¨ë¸ í‘œì‹œëª… ë§¤í•‘
            llm_display_names = {
                "gemini": "Google Gemini 2.5 Flash",
                "hcx": "NAVER HyperCLOVA X"
            }
            
            embedding_display_names = {
                "gemini": "Google Gemini Embedding",
                "bge_m3": "BGE-M3 Local Embedding", 
                "hcx": "NAVER HCX Embedding"
            }
            
            result_dict["evaluation_id"] = evaluation_id
            result_dict["llm_model"] = llm_display_names.get(llm_type, llm_type)
            result_dict["embedding_model"] = embedding_display_names.get(embedding_type, embedding_type)
            result_dict["dataset_name"] = actual_dataset_name if 'actual_dataset_name' in locals() else dataset_name

            dataset_path = get_evaluation_data_path(dataset_name)
            if dataset_path:
                try:
                    from pathlib import Path
                    from src.infrastructure.data_import.importers import ExcelImporter, CSVImporter
                    
                    file_path_obj = Path(dataset_path)
                    file_extension = file_path_obj.suffix.lower()
                    
                    if file_extension in ['.xlsx', '.xls']:
                        # Excel íŒŒì¼ ì²˜ë¦¬
                        importer = ExcelImporter()
                        evaluation_data_list = importer.import_data(file_path_obj)
                        qa_data = [
                            {
                                "question": item.question,
                                "contexts": item.contexts,
                                "answer": item.answer,
                                "ground_truth": item.ground_truth
                            }
                            for item in evaluation_data_list
                        ]
                    elif file_extension == '.csv':
                        # CSV íŒŒì¼ ì²˜ë¦¬
                        importer = CSVImporter()
                        evaluation_data_list = importer.import_data(file_path_obj)
                        qa_data = [
                            {
                                "question": item.question,
                                "contexts": item.contexts,
                                "answer": item.answer,
                                "ground_truth": item.ground_truth
                            }
                            for item in evaluation_data_list
                        ]
                    else:
                        # JSON íŒŒì¼ ì²˜ë¦¬ (ê¸°ë³¸)
                        with open(dataset_path, encoding="utf-8") as f:
                            qa_data = json.load(f)
                    
                    qa_count = len(result_dict.get("individual_scores", []))
                    result_dict["qa_count"] = qa_count
                    result_dict["qa_data"] = qa_data[:qa_count]
                except Exception as e:
                    st.warning(f"QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                    result_dict["qa_count"] = len(result_dict.get("individual_scores", []))
            else:
                result_dict["qa_count"] = len(result_dict.get("individual_scores", []))

            save_evaluation_result(result_dict)

            st.success("âœ… í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.balloons()
            
            # í‰ê°€ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
            st.markdown("### ğŸ“Š í‰ê°€ ê²°ê³¼")
            
            # API ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í™•ì¸ ë° ì•ˆë‚´
            individual_scores = result_dict.get("individual_scores", [])
            failed_count = 0
            total_count = len(individual_scores)
            
            for scores in individual_scores:
                # 0.2ëŠ” API ì‹¤íŒ¨ ì‹œ ë¶€ì—¬í•˜ëŠ” ë¶€ë¶„ ì ìˆ˜
                if any(score == 0.2 for score in scores.values()):
                    failed_count += 1
            
            if failed_count > 0:
                st.warning(f"âš ï¸ **ì¼ë¶€ í‰ê°€ ì‹¤íŒ¨**: {total_count}ê°œ ì¤‘ {failed_count}ê°œê°€ API í•œë„ë¡œ ì¸í•´ ë¶€ë¶„ ì ìˆ˜ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ **ê°œì„  ë°©ë²•**: Gemini ëª¨ë¸ ì‚¬ìš© ë˜ëŠ” ì ì‹œ í›„ ì¬í‰ê°€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ† RAGAS Score", f"{result_dict.get('ragas_score', 0):.3f}")
            with col2:
                st.metric("âœ… Faithfulness", f"{result_dict.get('faithfulness', 0):.3f}")
            with col3:
                st.metric("ğŸ¯ Answer Relevancy", f"{result_dict.get('answer_relevancy', 0):.3f}")
            with col4:
                st.metric("ğŸ”„ Context Recall", f"{result_dict.get('context_recall', 0):.3f}")
            
            # ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™
            st.markdown("---")
            
            # í‰ê°€ ì™„ë£Œ ìƒíƒœ ì €ì¥
            st.session_state.evaluation_completed = True
            st.session_state.latest_evaluation_result = result_dict
            
            st.info("ğŸ’¡ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                if st.button("ğŸ“Š Overview í˜ì´ì§€ë¡œ ì´ë™", type="primary", use_container_width=True, key="goto_overview"):
                    st.session_state.navigate_to = "ğŸ¯ Overview"
                    st.rerun()
            
            with col2:
                if st.button("ğŸ“ˆ Historical í˜ì´ì§€ë¡œ ì´ë™", type="secondary", use_container_width=True, key="goto_historical"):
                    st.session_state.navigate_to = "ğŸ“ˆ Historical"
                    st.rerun()

        except Exception as e:
            st.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.exception(e)


def show_historical():
    """íˆìŠ¤í† ë¦¬ í˜ì´ì§€"""
    st.header("ğŸ“ˆ í‰ê°€ ì´ë ¥")

    history = load_evaluation_history()

    if history:
        df = pd.DataFrame(history)
        df["timestamp"] = pd.to_datetime(df["timestamp"])

        # í…Œì´ë¸” í‘œì‹œ
        st.subheader("ğŸ“‹ í‰ê°€ ì´ë ¥ í…Œì´ë¸”")

        # ê° í‰ê°€ì— ëŒ€í•œ ìƒì„¸ ì •ë³´
        for i, row in df.iterrows():
            # ì œëª©ì— ëª¨ë¸ ì •ë³´ í¬í•¨
            llm_model = row.get('llm_model', 'N/A')
            embedding_model = row.get('embedding_model', 'N/A')
            qa_count = row.get('qa_count', 'N/A')
            
            # ëª¨ë¸ëª…ì´ ê¸¸ë©´ ì¤„ì„
            if llm_model and len(str(llm_model)) > 20:
                llm_display = str(llm_model)[:20] + "..."
            else:
                llm_display = llm_model
                
            if embedding_model and len(str(embedding_model)) > 20:
                embedding_display = str(embedding_model)[:20] + "..."
            else:
                embedding_display = embedding_model
            
            with st.expander(
                f"í‰ê°€ #{i+1} - {row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')} | QA: {qa_count}"
            ):
                # í‰ê°€ ê¸°ë³¸ ì •ë³´
                st.markdown("**ğŸ“‹ í‰ê°€ ì •ë³´**")
                info_col1, info_col2 = st.columns(2)
                with info_col1:
                    st.markdown(f"**LLM ëª¨ë¸**: <span style='font-size: 14px;'>{llm_display}</span>", unsafe_allow_html=True)
                    st.markdown(f"**ì„ë² ë”© ëª¨ë¸**: <span style='font-size: 14px;'>{embedding_display}</span>", unsafe_allow_html=True)
                with info_col2:
                    st.markdown(f"**ë°ì´í„°ì…‹**: <span style='font-size: 14px;'>{row.get('dataset_name', 'N/A')}</span>", unsafe_allow_html=True)
                    st.markdown(f"**í‰ê°€ ID**: <span style='font-size: 14px;'>{str(row.get('evaluation_id', 'N/A'))[:8]}</span>", unsafe_allow_html=True)
                
                st.markdown("---")
                
                col1, col2, col3, col4 = st.columns([1.8, 1.8, 1.8, 1])

                with col1:
                    st.metric("RAGAS ì ìˆ˜", f"{row.get('ragas_score', 0):.3f}")
                    st.metric("Faithfulness", f"{row.get('faithfulness', 0):.3f}")

                with col2:
                    st.metric(
                        "Answer Relevancy", f"{row.get('answer_relevancy', 0):.3f}"
                    )
                    st.metric("Context Recall", f"{row.get('context_recall', 0):.3f}")

                with col3:
                    st.metric(
                        "Context Precision", f"{row.get('context_precision', 0):.3f}"
                    )
                    st.metric(
                        "Answer Correctness", f"{row.get('answer_correctness', 0):.3f}"
                    )

                with col4:
                    # ìƒì„¸ ë¶„ì„ í˜ì´ì§€ë¡œ ì´ë™ ë²„íŠ¼
                    if st.button("ğŸ” ìƒì„¸ ë¶„ì„", key=f"detail_btn_{i}"):
                        st.session_state.selected_evaluation_index = i
                        st.session_state.navigate_to = "ğŸ“š Detailed Analysis"
                        st.rerun()

        # ì „ì²´ í…Œì´ë¸” í‘œì‹œ
        st.subheader("ğŸ“Š ì „ì²´ í‰ê°€ ì´ë ¥")
        st.dataframe(df, use_container_width=True)

        # í‰ê°€ ë¹„êµ
        st.subheader("ğŸ“Š í‰ê°€ ë¹„êµ")

        if len(df) > 1:
            col1, col2 = st.columns(2)

            with col1:
                eval1_idx = st.selectbox(
                    "ì²« ë²ˆì§¸ í‰ê°€",
                    range(len(df)),
                    format_func=lambda x: f"{df.iloc[x]['timestamp'].strftime('%Y-%m-%d %H:%M')} (#{x+1})",
                )

            with col2:
                eval2_idx = st.selectbox(
                    "ë‘ ë²ˆì§¸ í‰ê°€",
                    range(len(df)),
                    index=min(1, len(df) - 1),
                    format_func=lambda x: f"{df.iloc[x]['timestamp'].strftime('%Y-%m-%d %H:%M')} (#{x+1})",
                )

            if eval1_idx != eval2_idx:
                show_comparison_chart(df.iloc[eval1_idx], df.iloc[eval2_idx])

    else:
        st.info("ğŸ“ ì•„ì§ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")


def show_comparison_chart(eval1, eval2):
    """ë‘ í‰ê°€ ê²°ê³¼ ë¹„êµ ì°¨íŠ¸"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    
    # answer_correctnessê°€ ë‘ í‰ê°€ ëª¨ë‘ì— ìˆìœ¼ë©´ ì¶”ê°€
    if "answer_correctness" in eval1 and "answer_correctness" in eval2:
        metrics.append("answer_correctness")
    
    metrics.append("ragas_score")

    fig = go.Figure()

    # ì•ˆì „í•œ ê°’ ë³€í™˜
    def safe_get_values(eval_dict, metric_list):
        values = []
        for m in metric_list:
            v = eval_dict.get(m, 0)
            try:
                values.append(float(v) if v is not None else 0.0)
            except (ValueError, TypeError):
                values.append(0.0)
        return values
    
    eval1_values = safe_get_values(eval1, metrics)
    eval2_values = safe_get_values(eval2, metrics)

    fig.add_trace(
        go.Bar(
            name=f'í‰ê°€ 1 ({eval1["timestamp"]})',
            x=metrics,
            y=eval1_values,
            marker_color="lightblue",
        )
    )

    fig.add_trace(
        go.Bar(
            name=f'í‰ê°€ 2 ({eval2["timestamp"]})',
            x=metrics,
            y=eval2_values,
            marker_color="darkblue",
        )
    )

    fig.update_layout(
        title="ğŸ“Š í‰ê°€ ê²°ê³¼ ë¹„êµ", barmode="group", yaxis=dict(range=[0, 1]), height=400
    )

    st.plotly_chart(fig, use_container_width=True)


def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ í˜ì´ì§€"""
    try:
        from src.presentation.web.components.detailed_analysis import (
            show_detailed_analysis as show_detailed_component,
        )
        show_detailed_component()
    except ImportError:
        st.header("ğŸ“š ìƒì„¸ ë¶„ì„")
        st.info("ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        st.write("ì´ ê¸°ëŠ¥ì€ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")


def show_metrics_guide():
    """ë©”íŠ¸ë¦­ ê°€ì´ë“œ í˜ì´ì§€"""
    try:
        from src.presentation.web.components.metrics_explanation import (
            show_metrics_explanation as show_metrics_component,
        )
        show_metrics_component()
    except ImportError:
        st.header("ğŸ“– ë©”íŠ¸ë¦­ ì„¤ëª…")
        st.markdown("""
        ### RAGAS ë©”íŠ¸ë¦­ ì„¤ëª…
        
        **ğŸ† RAGAS Score**: ì „ì²´ ì¢…í•© ì ìˆ˜
        - ëª¨ë“  ë©”íŠ¸ë¦­ì˜ ì¡°í™” í‰ê· 
        - 0.0 ~ 1.0 ë²”ìœ„
        
        **âœ… Faithfulness**: ë‹µë³€ì˜ ì‚¬ì‹¤ ì •í™•ì„±
        - ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •
        
        **ğŸ¯ Answer Relevancy**: ë‹µë³€ì˜ ê´€ë ¨ì„±
        - ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •
        
        **ğŸ”„ Context Recall**: ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨
        - ê´€ë ¨ ì •ë³´ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¸¡ì •
        
        **ğŸ“ Context Precision**: ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ ì¸¡ì •
        
        **âœ”ï¸ Answer Correctness**: ë‹µë³€ì˜ ì •í™•ë„
        - ìƒì„±ëœ ë‹µë³€ì´ ì •ë‹µ(ground truth)ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •
        - Semantic similarityì™€ factual similarityë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
        """)


def show_performance():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í˜ì´ì§€"""
    try:
        from src.presentation.web.components.performance_monitor import (
            show_performance_monitor as show_performance_component,
        )
        show_performance_component()
    except ImportError:
        st.header("âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        st.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        st.write("ì´ ê¸°ëŠ¥ì€ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")


# ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ë“¤
def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    db_path = DATABASE_PATH
    db_path.parent.mkdir(exist_ok=True)

    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()

    # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
    cursor.execute("PRAGMA table_info(evaluations)")
    columns = [row[1] for row in cursor.fetchall()]
    
    # ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ì´ ì—†ìœ¼ë©´ ì¶”ê°€
    new_columns = [
        'qa_count', 'evaluation_id', 'llm_model', 'embedding_model', 'dataset_name',
        'total_duration_seconds', 'total_duration_minutes', 'avg_time_per_item_seconds',
        'answer_correctness'
    ]
    
    for column in new_columns:
        if column not in columns:
            try:
                if column in ['qa_count', 'total_duration_seconds', 'total_duration_minutes', 'avg_time_per_item_seconds', 'answer_correctness']:
                    cursor.execute(f"ALTER TABLE evaluations ADD COLUMN {column} REAL")
                else:
                    cursor.execute(f"ALTER TABLE evaluations ADD COLUMN {column} TEXT")
                print(f"âœ… ì»¬ëŸ¼ '{column}' ì¶”ê°€ë¨")
            except Exception as e:
                print(f"âš ï¸ ì»¬ëŸ¼ '{column}' ì¶”ê°€ ì‹¤íŒ¨: {e}")

    # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS evaluations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            faithfulness REAL,
            answer_relevancy REAL,
            context_recall REAL,
            context_precision REAL,
            answer_correctness REAL,
            ragas_score REAL,
            raw_data TEXT,
            qa_count INTEGER,
            evaluation_id TEXT,
            llm_model TEXT,
            embedding_model TEXT,
            dataset_name TEXT,
            total_duration_seconds REAL,
            total_duration_minutes REAL,
            avg_time_per_item_seconds REAL
        )
    """
    )

    conn.commit()
    conn.close()


def save_evaluation_result(result):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    init_db()

    conn = sqlite3.connect(str(DATABASE_PATH))
    cursor = conn.cursor()

    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
    metadata = result.get("metadata", {})
    
    cursor.execute(
        """
        INSERT INTO evaluations (
            timestamp, faithfulness, answer_relevancy, 
            context_recall, context_precision, answer_correctness, ragas_score, raw_data,
            qa_count, evaluation_id, llm_model, embedding_model, dataset_name,
            total_duration_seconds, total_duration_minutes, avg_time_per_item_seconds
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """,
        (
            datetime.now().isoformat(),
            result.get("faithfulness", 0),
            result.get("answer_relevancy", 0),
            result.get("context_recall", 0),
            result.get("context_precision", 0),
            result.get("answer_correctness", 0),
            result.get("ragas_score", 0),
            json.dumps(result),
            result.get("qa_count", 0),
            result.get("evaluation_id", ""),
            result.get("llm_model", ""),
            result.get("embedding_model", ""),
            result.get("dataset_name", ""),
            metadata.get("total_duration_seconds", 0.0),
            metadata.get("total_duration_minutes", 0.0),
            metadata.get("avg_time_per_item_seconds", 0.0),
        ),
    )

    conn.commit()
    conn.close()


def load_latest_result():
    """ìµœì‹  í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    history = load_evaluation_history(limit=1)
    return history[0] if history else None


def load_evaluation_history(limit=None):
    """í‰ê°€ ì´ë ¥ ë¡œë“œ"""
    init_db()

    conn = sqlite3.connect(str(DATABASE_PATH))
    
    # í…Œì´ë¸” êµ¬ì¡° í™•ì¸
    cursor = conn.cursor()
    cursor.execute("PRAGMA table_info(evaluations)")
    columns = [row[1] for row in cursor.fetchall()]
    
    # answer_correctness ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'answer_correctness' in columns:
        query = """
            SELECT timestamp, faithfulness, answer_relevancy, 
                   context_recall, context_precision, answer_correctness, ragas_score,
                   qa_count, evaluation_id, llm_model, embedding_model, dataset_name,
                   total_duration_seconds, total_duration_minutes, avg_time_per_item_seconds
            FROM evaluations 
            ORDER BY timestamp DESC
        """
    else:
        # answer_correctness ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬
        query = """
            SELECT timestamp, faithfulness, answer_relevancy, 
                   context_recall, context_precision, 0 as answer_correctness, ragas_score,
                   qa_count, evaluation_id, llm_model, embedding_model, dataset_name,
                   total_duration_seconds, total_duration_minutes, avg_time_per_item_seconds
            FROM evaluations 
            ORDER BY timestamp DESC
        """

    if limit:
        query += f" LIMIT {limit}"

    df = pd.read_sql_query(query, conn)
    conn.close()

    return df.to_dict("records")


def get_previous_result():
    """ì´ì „ í‰ê°€ ê²°ê³¼ ë°˜í™˜"""
    history = load_evaluation_history(limit=2)
    return history[1] if len(history) > 1 else None


if __name__ == "__main__":
    main()