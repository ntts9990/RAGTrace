"""
RAGAS í‰ê°€ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
ì§ê´€ì ì´ê³  ë¹„êµ ê°€ëŠ¥í•œ ì‹œê°í™” ì œê³µ
"""

import json
import sqlite3
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd
import plotly.graph_objects as go
import streamlit as st

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from src.application.use_cases import RunEvaluationUseCase
from src.infrastructure.evaluation import RagasEvalAdapter
from src.infrastructure.llm.gemini_adapter import GeminiAdapter
from src.infrastructure.repository.file_adapter import FileRepositoryAdapter

# ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸
try:
    from src.presentation.web.components.detailed_analysis import \
        show_detailed_analysis as show_detailed_component
    from src.presentation.web.components.metrics_explanation import \
        show_metrics_explanation as show_metrics_component
    from src.presentation.web.components.performance_monitor import \
        show_performance_monitor as show_performance_component
except ImportError:
    # ê°œë°œ í™˜ê²½ì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œ ëŒ€ë¹„
    sys.path.append(str(project_root / "src/presentation/web"))
    from components.detailed_analysis import \
        show_detailed_analysis as show_detailed_component
    from components.metrics_explanation import \
        show_metrics_explanation as show_metrics_component
    from components.performance_monitor import \
        show_performance_monitor as show_performance_component

# íŽ˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAGAS í‰ê°€ ëŒ€ì‹œë³´ë“œ",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜
st.sidebar.title("ðŸ“Š RAGAS ëŒ€ì‹œë³´ë“œ")

# íŽ˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ìƒíƒœ ê´€ë¦¬
pages = [
    "ðŸŽ¯ Overview",
    "ðŸ“ˆ Historical",
    "ðŸ” Detailed Analysis",
    "ðŸ“š Metrics Guide",
    "âš¡ Performance",
]

# ì´ˆê¸° íŽ˜ì´ì§€ ì„¤ì •
if "selected_page" not in st.session_state:
    st.session_state.selected_page = "ðŸŽ¯ Overview"

# ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ìœ¼ë¡œ íŽ˜ì´ì§€ ì´ë™ ì²˜ë¦¬
if "navigate_to" in st.session_state:
    st.session_state.selected_page = st.session_state.navigate_to
    del st.session_state.navigate_to


# íŽ˜ì´ì§€ ì„ íƒ ì½œë°± í•¨ìˆ˜
def on_page_change():
    st.session_state.selected_page = st.session_state.page_selector


# ì‚¬ì´ë“œë°”ì—ì„œ íŽ˜ì´ì§€ ì„ íƒ
st.sidebar.selectbox(
    "íŽ˜ì´ì§€ ì„ íƒ",
    pages,
    index=pages.index(st.session_state.selected_page),
    key="page_selector",
    on_change=on_page_change,
)

page = st.session_state.selected_page

# ë©”ì¸ íƒ€ì´í‹€
st.title("ðŸŽ¯ RAGAS í‰ê°€ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ")
st.markdown("---")


def main():
    if page == "ðŸŽ¯ Overview":
        show_overview()
    elif page == "ðŸ“ˆ Historical":
        show_historical()
    elif page == "ðŸ” Detailed Analysis":
        show_detailed_analysis()
    elif page == "ðŸ“š Metrics Guide":
        show_metrics_guide()
    elif page == "âš¡ Performance":
        show_performance()


def show_overview():
    """ë©”ì¸ ì˜¤ë²„ë·° ëŒ€ì‹œë³´ë“œ"""
    st.header("ðŸ“Š í‰ê°€ ê²°ê³¼ ê°œìš”")

    # ìƒˆ í‰ê°€ ì‹¤í–‰ ë²„íŠ¼
    col1, col2, col3 = st.columns([1, 1, 2])

    with col1:
        if st.button("ðŸš€ ìƒˆ í‰ê°€ ì‹¤í–‰", type="primary"):
            run_new_evaluation()

    with col2:
        if st.button("ðŸ”„ ê²°ê³¼ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()

    with col3:
        st.markdown("**ðŸ“š ë„ì›€ë§**")
        if st.button("ðŸ“š ë©”íŠ¸ë¦­ ê°€ì´ë“œ", help="ì ìˆ˜ ì˜ë¯¸ ì´í•´í•˜ê¸°"):
            st.session_state.navigate_to = "ðŸ“š Metrics Guide"
            st.rerun()

    # ìµœì‹  í‰ê°€ ê²°ê³¼ ë¡œë“œ
    latest_result = load_latest_result()

    if latest_result:
        show_metric_cards(latest_result)
        show_metric_charts(latest_result)
        show_recent_trends()
    else:
        st.info(
            "ðŸ“ ì•„ì§ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. 'ìƒˆ í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì²« í‰ê°€ë¥¼ ì‹œìž‘í•˜ì„¸ìš”!"
        )
        st.markdown("---")
        st.markdown("### ðŸ¤” RAGAS ë©”íŠ¸ë¦­ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")
        st.markdown(
            "ðŸ“š ì‚¬ì´ë“œë°”ì—ì„œ **'Metrics Guide'**ë¥¼ ì„ íƒí•˜ë©´ ê° ì ìˆ˜ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì‰½ê²Œ ì•Œì•„ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤!"
        )


def show_metric_cards(result):
    """ë©”íŠ¸ë¦­ ì¹´ë“œ í‘œì‹œ"""
    st.subheader("ðŸŽ¯ í•µì‹¬ ì§€í‘œ")

    col1, col2, col3, col4, col5 = st.columns(5)

    metrics = [
        ("ì¢…í•© ì ìˆ˜", result.get("ragas_score", 0), "ðŸ†"),
        ("Faithfulness", result.get("faithfulness", 0), "âœ…"),
        ("Answer Relevancy", result.get("answer_relevancy", 0), "ðŸŽ¯"),
        ("Context Recall", result.get("context_recall", 0), "ðŸ”„"),
        ("Context Precision", result.get("context_precision", 0), "ðŸ“"),
    ]

    for i, (name, value, icon) in enumerate(metrics):
        with [col1, col2, col3, col4, col5][i]:
            # ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ
            if value >= 0.8:
                color = "green"
            elif value >= 0.6:
                color = "orange"
            else:
                color = "red"

            # ì´ì „ í‰ê°€ì™€ì˜ ë¹„êµë¥¼ ìœ„í•œ ë¸íƒ€ ê³„ì‚°
            previous_result = get_previous_result()
            delta_value = None
            if previous_result and name.lower().replace(" ", "_") in previous_result:
                prev_value = previous_result[name.lower().replace(" ", "_")]
                delta_value = value - prev_value

            st.metric(
                label=f"{icon} {name}",
                value=f"{value:.3f}",
                delta=f"{delta_value:.3f}" if delta_value is not None else None,
            )


def show_metric_charts(result):
    """ë©”íŠ¸ë¦­ ì‹œê°í™” ì°¨íŠ¸"""
    st.subheader("ðŸ“ˆ ì‹œê°í™”")

    col1, col2 = st.columns(2)

    with col1:
        # ë ˆì´ë” ì°¨íŠ¸
        show_radar_chart(result)

    with col2:
        # ë°” ì°¨íŠ¸
        show_bar_chart(result)


def show_radar_chart(result):
    """ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    values = [result.get(metric, 0) for metric in metrics]
    labels = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]

    fig = go.Figure()

    fig.add_trace(
        go.Scatterpolar(
            r=values + [values[0]],  # ì²« ë²ˆì§¸ ê°’ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€í•˜ì—¬ ì°¨íŠ¸ë¥¼ ë‹«ìŒ
            theta=labels + [labels[0]],
            fill="toself",
            name="RAGAS ì ìˆ˜",
            line_color="rgb(32, 201, 151)",
        )
    )

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=True,
        title="ðŸ“Š ë©”íŠ¸ë¦­ ê· í˜•ë„",
        height=400,
    )

    st.plotly_chart(fig, use_container_width=True)


def show_bar_chart(result):
    """ë°” ì°¨íŠ¸ ìƒì„±"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    values = [result.get(metric, 0) for metric in metrics]
    labels = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]

    # ìƒ‰ìƒ ë§¤í•‘
    colors = ["green" if v >= 0.8 else "orange" if v >= 0.6 else "red" for v in values]

    fig = go.Figure(data=[go.Bar(x=labels, y=values, marker_color=colors)])

    fig.update_layout(
        title="ðŸ“Š ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥",
        yaxis_title="ì ìˆ˜",
        yaxis=dict(range=[0, 1]),
        height=400,
    )

    st.plotly_chart(fig, use_container_width=True)


def show_recent_trends():
    """ìµœê·¼ íŠ¸ë Œë“œ í‘œì‹œ"""
    st.subheader("ðŸ“ˆ ìµœê·¼ íŠ¸ë Œë“œ")

    # ížˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
    history = load_evaluation_history(limit=10)

    if len(history) > 1:
        df = pd.DataFrame(history)
        df["timestamp"] = pd.to_datetime(df["timestamp"])

        # íŠ¸ë Œë“œ ì°¨íŠ¸
        fig = go.Figure()

        metrics = [
            "faithfulness",
            "answer_relevancy",
            "context_recall",
            "context_precision",
            "ragas_score",
        ]
        colors = ["blue", "green", "orange", "red", "purple"]

        for metric, color in zip(metrics, colors):
            if metric in df.columns:
                fig.add_trace(
                    go.Scatter(
                        x=df["timestamp"],
                        y=df[metric],
                        mode="lines+markers",
                        name=metric.replace("_", " ").title(),
                        line=dict(color=color),
                    )
                )

        fig.update_layout(
            title="ðŸ“ˆ í‰ê°€ ì ìˆ˜ íŠ¸ë Œë“œ",
            xaxis_title="ì‹œê°„",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=400,
        )

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ðŸ“Š íŠ¸ë Œë“œ í‘œì‹œë¥¼ ìœ„í•´ ë” ë§Žì€ í‰ê°€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


def run_new_evaluation():
    """ìƒˆë¡œìš´ í‰ê°€ ì‹¤í–‰"""
    with st.spinner("ðŸ”„ í‰ê°€ë¥¼ ì‹¤í–‰ ì¤‘ìž…ë‹ˆë‹¤..."):
        try:
            # ì‚¬ìš©ìžê°€ ì„ íƒí•  ìˆ˜ ìžˆëŠ” ë°ì´í„°ì…‹ ì˜µì…˜
            import os
            import random

            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ íŒŒì¼ë“¤
            available_datasets = [
                "data/evaluation_data.json",
                "data/evaluation_data_variant1.json",
            ]

            # ì¡´ìž¬í•˜ëŠ” ë°ì´í„°ì…‹ë§Œ í•„í„°ë§
            existing_datasets = [
                ds for ds in available_datasets if os.path.exists(project_root / ds)
            ]

            if not existing_datasets:
                st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ëžœë¤í•˜ê²Œ ë°ì´í„°ì…‹ ì„ íƒ (ë‹¤ì–‘ì„±ì„ ìœ„í•´)
            selected_dataset = random.choice(existing_datasets)
            st.info(f"ðŸ“Š ì„ íƒëœ ë°ì´í„°ì…‹: {selected_dataset.split('/')[-1]}")

            # ê¸°ì¡´ í‰ê°€ ì„œë¹„ìŠ¤ í™œìš©
            llm_adapter = GeminiAdapter(
                model_name="gemini-2.5-flash-preview-05-20", requests_per_minute=1000
            )

            repository_adapter = FileRepositoryAdapter(
                file_path=str(project_root / selected_dataset)
            )

            ragas_eval_adapter = RagasEvalAdapter()

            evaluation_use_case = RunEvaluationUseCase(
                llm_port=llm_adapter,
                repository_port=repository_adapter,
                evaluation_runner=ragas_eval_adapter,
            )

            # í‰ê°€ ì‹¤í–‰
            evaluation_result = evaluation_use_case.execute()

            # ê²°ê³¼ ì €ìž¥ (ë°ì´í„°ì…‹ ì •ë³´ í¬í•¨)
            result_dict = evaluation_result.to_dict()
            result_dict["metadata"]["dataset"] = selected_dataset

            # QA ë°ì´í„°ë„ í•¨ê»˜ ì €ìž¥
            try:
                with open(project_root / selected_dataset, "r", encoding="utf-8") as f:
                    qa_data = json.load(f)
                    # ì‹¤ì œ í‰ê°€ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì €ìž¥
                    qa_count = len(result_dict.get("individual_scores", []))
                    result_dict["qa_data"] = qa_data[:qa_count]
            except Exception as e:
                st.warning(f"QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

            save_evaluation_result(result_dict)

            st.success("âœ… í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

        except Exception as e:
            st.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def show_historical():
    """ížˆìŠ¤í† ë¦¬ íŽ˜ì´ì§€"""
    st.header("ðŸ“ˆ í‰ê°€ ì´ë ¥")

    # ìƒì„¸ ë¶„ì„ìœ¼ë¡œ ì´ë™í•˜ëŠ” ì•ˆë‚´
    st.info(
        "ðŸ’¡ íŠ¹ì • í‰ê°€ì˜ ìƒì„¸ ë¶„ì„ì„ ë³´ë ¤ë©´ 'ìƒì„¸ ë¶„ì„' íŽ˜ì´ì§€ì—ì„œ í‰ê°€ë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )

    history = load_evaluation_history()

    if history:
        df = pd.DataFrame(history)
        df["timestamp"] = pd.to_datetime(df["timestamp"])

        # í…Œì´ë¸” í‘œì‹œ ë° ìƒì„¸ ë¶„ì„ ë²„íŠ¼ ì¶”ê°€
        st.subheader("ðŸ“‹ í‰ê°€ ì´ë ¥ í…Œì´ë¸”")

        # ê° í‰ê°€ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ì™€ ìƒì„¸ ë¶„ì„ ë²„íŠ¼
        for i, row in df.iterrows():
            with st.expander(
                f"í‰ê°€ #{i+1} - {row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}"
            ):
                col1, col2, col3 = st.columns([2, 2, 1])

                with col1:
                    st.metric("RAGAS ì ìˆ˜", f"{row.get('ragas_score', 0):.3f}")
                    st.metric("Faithfulness", f"{row.get('faithfulness', 0):.3f}")

                with col2:
                    st.metric(
                        "Answer Relevancy", f"{row.get('answer_relevancy', 0):.3f}"
                    )
                    st.metric("Context Recall", f"{row.get('context_recall', 0):.3f}")

                with col3:
                    st.metric(
                        "Context Precision", f"{row.get('context_precision', 0):.3f}"
                    )

                    # ìƒì„¸ ë¶„ì„ íŽ˜ì´ì§€ë¡œ ì´ë™ ë²„íŠ¼
                    if st.button(f"ðŸ” ìƒì„¸ ë¶„ì„", key=f"detail_btn_{i}"):
                        # ì„ íƒëœ í‰ê°€ ì¸ë±ìŠ¤ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ìž¥
                        st.session_state.selected_evaluation_index = i
                        st.session_state.navigate_to = "ðŸ” Detailed Analysis"
                        st.rerun()

        # ì „ì²´ í…Œì´ë¸”ë„ í‘œì‹œ
        st.subheader("ðŸ“Š ì „ì²´ í‰ê°€ ì´ë ¥")
        st.dataframe(df, use_container_width=True)

        # ìƒì„¸ ë¹„êµ ì°¨íŠ¸
        st.subheader("ðŸ“Š í‰ê°€ ë¹„êµ")

        if len(df) > 1:
            # ì‚¬ìš©ìžê°€ ë¹„êµí•  í‰ê°€ ì„ íƒ
            col1, col2 = st.columns(2)

            with col1:
                eval1_idx = st.selectbox(
                    "ì²« ë²ˆì§¸ í‰ê°€",
                    range(len(df)),
                    format_func=lambda x: f"{df.iloc[x]['timestamp'].strftime('%Y-%m-%d %H:%M')} (#{x+1})",
                )

            with col2:
                eval2_idx = st.selectbox(
                    "ë‘ ë²ˆì§¸ í‰ê°€",
                    range(len(df)),
                    index=min(1, len(df) - 1),
                    format_func=lambda x: f"{df.iloc[x]['timestamp'].strftime('%Y-%m-%d %H:%M')} (#{x+1})",
                )

            if eval1_idx != eval2_idx:
                show_comparison_chart(df.iloc[eval1_idx], df.iloc[eval2_idx])

    else:
        st.info("ðŸ“ ì•„ì§ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")


def show_comparison_chart(eval1, eval2):
    """ë‘ í‰ê°€ ê²°ê³¼ ë¹„êµ ì°¨íŠ¸"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
        "ragas_score",
    ]

    fig = go.Figure()

    fig.add_trace(
        go.Bar(
            name=f'í‰ê°€ 1 ({eval1["timestamp"]})',
            x=metrics,
            y=[eval1.get(m, 0) for m in metrics],
            marker_color="lightblue",
        )
    )

    fig.add_trace(
        go.Bar(
            name=f'í‰ê°€ 2 ({eval2["timestamp"]})',
            x=metrics,
            y=[eval2.get(m, 0) for m in metrics],
            marker_color="darkblue",
        )
    )

    fig.update_layout(
        title="ðŸ“Š í‰ê°€ ê²°ê³¼ ë¹„êµ", barmode="group", yaxis=dict(range=[0, 1]), height=400
    )

    st.plotly_chart(fig, use_container_width=True)


def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ íŽ˜ì´ì§€"""
    show_detailed_component()


def show_metrics_guide():
    """ë©”íŠ¸ë¦­ ê°€ì´ë“œ íŽ˜ì´ì§€"""
    show_metrics_component()


def show_performance():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ íŽ˜ì´ì§€"""
    show_performance_component()


# ë°ì´í„° ì €ìž¥/ë¡œë“œ í•¨ìˆ˜ë“¤
def get_db_path():
    """ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ data/db/evaluations.dbë¡œ ê²½ë¡œ ìˆ˜ì •
    project_root = Path(__file__).parent.parent.parent.parent
    return project_root / "data" / "db" / "evaluations.db"


def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    db_path = get_db_path()
    db_path.parent.mkdir(exist_ok=True)

    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()

    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS evaluations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            faithfulness REAL,
            answer_relevancy REAL,
            context_recall REAL,
            context_precision REAL,
            ragas_score REAL,
            raw_data TEXT
        )
    """
    )

    conn.commit()
    conn.close()


def save_evaluation_result(result):
    """í‰ê°€ ê²°ê³¼ ì €ìž¥"""
    init_db()

    conn = sqlite3.connect(str(get_db_path()))
    cursor = conn.cursor()

    cursor.execute(
        """
        INSERT INTO evaluations (
            timestamp, faithfulness, answer_relevancy, 
            context_recall, context_precision, ragas_score, raw_data
        ) VALUES (?, ?, ?, ?, ?, ?, ?)
    """,
        (
            datetime.now().isoformat(),
            result.get("faithfulness", 0),
            result.get("answer_relevancy", 0),
            result.get("context_recall", 0),
            result.get("context_precision", 0),
            result.get("ragas_score", 0),
            json.dumps(result),
        ),
    )

    conn.commit()
    conn.close()


def load_latest_result():
    """ìµœì‹  í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    history = load_evaluation_history(limit=1)
    return history[0] if history else None


def load_evaluation_history(limit=None):
    """í‰ê°€ ì´ë ¥ ë¡œë“œ"""
    init_db()

    conn = sqlite3.connect(str(get_db_path()))

    query = """
        SELECT timestamp, faithfulness, answer_relevancy, 
               context_recall, context_precision, ragas_score
        FROM evaluations 
        ORDER BY timestamp DESC
    """

    if limit:
        query += f" LIMIT {limit}"

    df = pd.read_sql_query(query, conn)
    conn.close()

    return df.to_dict("records")


def get_previous_result():
    """ì´ì „ í‰ê°€ ê²°ê³¼ ë°˜í™˜"""
    history = load_evaluation_history(limit=2)
    return history[1] if len(history) > 1 else None


if __name__ == "__main__":
    main()
