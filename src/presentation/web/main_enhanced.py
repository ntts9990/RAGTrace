"""
RAGTrace Dashboard - Enhanced with Full Features

main ë¸Œëœì¹˜ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•œ ì™„ì „í•œ RAGTrace ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤.
ë¬´í•œ ë£¨í”„ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©´ì„œ ëª¨ë“  ê¸°ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.
"""

import json
import sqlite3
from datetime import datetime

import pandas as pd
import plotly.graph_objects as go
import streamlit as st

# í˜ì´ì§€ ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
st.set_page_config(
    page_title="RAGTrace ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

from src.domain.prompts import PromptType
from src.utils.paths import (
    DATABASE_PATH,
    get_available_datasets,
    get_evaluation_data_path,
)


# í˜ì´ì§€ ì •ì˜
def load_pages():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ì§€ ëª©ë¡ ë°˜í™˜"""
    return {
        "ğŸ¯ Overview": "ë©”ì¸ ëŒ€ì‹œë³´ë“œ",
        "ğŸš€ New Evaluation": "ìƒˆ í‰ê°€ ì‹¤í–‰",
        "ğŸ“ˆ Historical": "ê³¼ê±° í‰ê°€ ê²°ê³¼",
        "ğŸ“š Detailed Analysis": "ìƒì„¸ ë¶„ì„",
        "ğŸ“– Metrics Explanation": "ë©”íŠ¸ë¦­ ì„¤ëª…",
        "âš¡ Performance": "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
    }


# ì‚¬ì´ë“œë°” ë° ë„¤ë¹„ê²Œì´ì…˜
st.sidebar.title("ğŸ” RAGTrace ëŒ€ì‹œë³´ë“œ")

pages = load_pages()
page_keys = list(pages.keys())

# í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ìƒíƒœ ê´€ë¦¬
if "selected_page" not in st.session_state:
    st.session_state.selected_page = "ğŸ¯ Overview"

# ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ìœ¼ë¡œ í˜ì´ì§€ ì´ë™ ì²˜ë¦¬
if "navigate_to" in st.session_state:
    st.session_state.selected_page = st.session_state.navigate_to
    del st.session_state.navigate_to


# í˜ì´ì§€ ì„ íƒ ì½œë°± í•¨ìˆ˜
def on_page_change():
    st.session_state.selected_page = st.session_state.page_selector


# ì‚¬ì´ë“œë°”ì—ì„œ í˜ì´ì§€ ì„ íƒ
st.sidebar.selectbox(
    "í˜ì´ì§€ ì„ íƒ",
    page_keys,
    index=page_keys.index(st.session_state.selected_page),
    key="page_selector",
    on_change=on_page_change,
)

page = st.session_state.selected_page


# ë©”ì¸ í•¨ìˆ˜ë“¤
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    st.title("ğŸ” RAGTrace - RAG ì„±ëŠ¥ ì¶”ì  ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    show_overview()


def show_overview():
    """ë©”ì¸ ì˜¤ë²„ë·° ëŒ€ì‹œë³´ë“œ"""
    st.header("ğŸ“Š í‰ê°€ ê²°ê³¼ ê°œìš”")
    
    # ë°©ê¸ˆ ì™„ë£Œëœ í‰ê°€ê°€ ìˆìœ¼ë©´ ì¶•í•˜ ë©”ì‹œì§€ í‘œì‹œ
    if st.session_state.get("evaluation_completed", False):
        st.success("ğŸ‰ ìƒˆë¡œìš´ í‰ê°€ê°€ ë°©ê¸ˆ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        # í•œ ë²ˆ í‘œì‹œ í›„ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.evaluation_completed = False

    # ì•¡ì…˜ ë²„íŠ¼ë“¤
    col1, col2, col3, col4 = st.columns([2, 1, 1, 2])

    with col1:
        if st.button(
            "ğŸš€ ìƒˆ í‰ê°€ ì‹¤í–‰", type="primary", help="ìƒˆë¡œìš´ RAG í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤"
        ):
            st.session_state.navigate_to = "ğŸš€ New Evaluation"
            st.rerun()

    with col2:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", help="ìµœì‹  ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤"):
            st.rerun()

    with col3:
        if st.button("ğŸ“ˆ ì´ë ¥ë³´ê¸°", help="ê³¼ê±° í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤"):
            st.session_state.navigate_to = "ğŸ“ˆ Historical"
            st.rerun()

    with col4:
        if st.button("ğŸ“š ë©”íŠ¸ë¦­ ê°€ì´ë“œ", help="RAGAS ì ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ì•Œì•„ë³´ì„¸ìš”"):
            st.session_state.navigate_to = "ğŸ“– Metrics Explanation"
            st.rerun()

    # ìµœì‹  í‰ê°€ ê²°ê³¼ ë¡œë“œ
    latest_result = load_latest_result()

    if latest_result:
        show_metric_cards(latest_result)
        show_metric_charts(latest_result)
        show_recent_trends()
    else:
        st.info(
            "ğŸ“ ì•„ì§ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. 'ìƒˆ í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì²« í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”!"
        )
        st.markdown("---")
        st.markdown("### ğŸ¤” RAGAS ë©”íŠ¸ë¦­ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")
        st.markdown(
            "ğŸ“š ì‚¬ì´ë“œë°”ì—ì„œ **'Metrics Guide'**ë¥¼ ì„ íƒí•˜ë©´ ê° ì ìˆ˜ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì‰½ê²Œ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
        )


def show_metric_cards(result):
    """ë©”íŠ¸ë¦­ ì¹´ë“œ í‘œì‹œ"""
    st.subheader("ğŸ¯ í•µì‹¬ ì§€í‘œ")

    col1, col2, col3, col4, col5 = st.columns(5)

    metrics = [
        ("ì¢…í•© ì ìˆ˜", result.get("ragas_score", 0), "ğŸ†"),
        ("Faithfulness", result.get("faithfulness", 0), "âœ…"),
        ("Answer Relevancy", result.get("answer_relevancy", 0), "ğŸ¯"),
        ("Context Recall", result.get("context_recall", 0), "ğŸ”„"),
        ("Context Precision", result.get("context_precision", 0), "ğŸ“"),
    ]

    for i, (name, value, icon) in enumerate(metrics):
        with [col1, col2, col3, col4, col5][i]:
            # ì´ì „ í‰ê°€ì™€ì˜ ë¹„êµë¥¼ ìœ„í•œ ë¸íƒ€ ê³„ì‚°
            previous_result = get_previous_result()
            delta_value = None
            if previous_result and name.lower().replace(" ", "_") in previous_result:
                prev_value = previous_result[name.lower().replace(" ", "_")]
                delta_value = value - prev_value

            st.metric(
                label=f"{icon} {name}",
                value=f"{value:.3f}",
                delta=f"{delta_value:.3f}" if delta_value is not None else None,
            )


def show_metric_charts(result):
    """ë©”íŠ¸ë¦­ ì‹œê°í™” ì°¨íŠ¸"""
    st.subheader("ğŸ“ˆ ì‹œê°í™”")

    col1, col2 = st.columns(2)

    with col1:
        # ë ˆì´ë” ì°¨íŠ¸
        show_radar_chart(result)

    with col2:
        # ë°” ì°¨íŠ¸
        show_bar_chart(result)


def show_radar_chart(result):
    """ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    values = [result.get(metric, 0) for metric in metrics]
    labels = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]

    fig = go.Figure()

    fig.add_trace(
        go.Scatterpolar(
            r=values + [values[0]],  # ì²« ë²ˆì§¸ ê°’ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€í•˜ì—¬ ì°¨íŠ¸ë¥¼ ë‹«ìŒ
            theta=labels + [labels[0]],
            fill="toself",
            name="RAGAS ì ìˆ˜",
            line_color="rgb(32, 201, 151)",
        )
    )

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=True,
        title="ğŸ“Š ë©”íŠ¸ë¦­ ê· í˜•ë„",
        height=400,
    )

    st.plotly_chart(fig, use_container_width=True)


def show_bar_chart(result):
    """ë°” ì°¨íŠ¸ ìƒì„±"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
    ]
    values = [result.get(metric, 0) for metric in metrics]
    labels = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]

    # ìƒ‰ìƒ ë§¤í•‘
    colors = ["green" if v >= 0.8 else "orange" if v >= 0.6 else "red" for v in values]

    fig = go.Figure(data=[go.Bar(x=labels, y=values, marker_color=colors)])

    fig.update_layout(
        title="ğŸ“Š ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥",
        yaxis_title="ì ìˆ˜",
        yaxis=dict(range=[0, 1]),
        height=400,
    )

    st.plotly_chart(fig, use_container_width=True)


def show_recent_trends():
    """ìµœê·¼ íŠ¸ë Œë“œ í‘œì‹œ"""
    st.subheader("ğŸ“ˆ ìµœê·¼ íŠ¸ë Œë“œ")

    # íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
    history = load_evaluation_history(limit=10)

    if len(history) > 1:
        df = pd.DataFrame(history)
        df["timestamp"] = pd.to_datetime(df["timestamp"])

        # íŠ¸ë Œë“œ ì°¨íŠ¸
        fig = go.Figure()

        metrics = [
            "faithfulness",
            "answer_relevancy",
            "context_recall",
            "context_precision",
            "ragas_score",
        ]
        colors = ["blue", "green", "orange", "red", "purple"]

        for metric, color in zip(metrics, colors, strict=False):
            if metric in df.columns:
                fig.add_trace(
                    go.Scatter(
                        x=df["timestamp"],
                        y=df[metric],
                        mode="lines+markers",
                        name=metric.replace("_", " ").title(),
                        line=dict(color=color),
                    )
                )

        fig.update_layout(
            title="ğŸ“ˆ í‰ê°€ ì ìˆ˜ íŠ¸ë Œë“œ",
            xaxis_title="ì‹œê°„",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=400,
        )

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ğŸ“Š íŠ¸ë Œë“œ í‘œì‹œë¥¼ ìœ„í•´ ë” ë§ì€ í‰ê°€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


def show_new_evaluation_page():
    """ìƒˆ í‰ê°€ ì‹¤í–‰ í˜ì´ì§€"""
    st.title("ğŸš€ ìƒˆ í‰ê°€ ì‹¤í–‰")
    st.markdown("---")
    
    # LLM ì„ íƒ UI í‘œì‹œ
    selected_llm = show_llm_selector()
    
    st.markdown("---")
    
    # ì„ë² ë”© ì„ íƒ UI í‘œì‹œ
    selected_embedding = show_embedding_selector()
    
    st.markdown("---")
    
    # í”„ë¡¬í”„íŠ¸ ì„ íƒ UI í‘œì‹œ
    selected_prompt_type = show_prompt_selector()
    
    st.markdown("---")
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    st.markdown("### ğŸ“Š ë°ì´í„°ì…‹ ì„ íƒ")
    existing_datasets = get_available_datasets()
    if not existing_datasets:
        st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.info("data/ ë””ë ‰í† ë¦¬ì— JSON í˜•ì‹ì˜ í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    # session_stateì— ì„ íƒëœ ë°ì´í„°ì…‹ ì €ì¥
    if "selected_dataset" not in st.session_state:
        st.session_state.selected_dataset = existing_datasets[0]
    
    # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    try:
        current_index = existing_datasets.index(st.session_state.selected_dataset)
    except ValueError:
        current_index = 0
        st.session_state.selected_dataset = existing_datasets[0]
    
    # ë°ì´í„°ì…‹ ì„ íƒ UI
    selected_dataset = st.selectbox(
        "í‰ê°€í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:",
        existing_datasets,
        index=current_index,
        key="dataset_selector_box",
        help="í‰ê°€ì— ì‚¬ìš©í•  QA ë°ì´í„°ì…‹ì„ ì„ íƒí•©ë‹ˆë‹¤."
    )
    
    # ì„ íƒì´ ë³€ê²½ë˜ë©´ session_state ì—…ë°ì´íŠ¸
    st.session_state.selected_dataset = selected_dataset
    
    # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
    dataset_path = get_evaluation_data_path(selected_dataset)
    if dataset_path:
        try:
            with open(dataset_path, encoding="utf-8") as f:
                qa_data = json.load(f)
                st.info(f"ğŸ“‹ ì„ íƒëœ ë°ì´í„°ì…‹: **{selected_dataset}** ({len(qa_data)}ê°œ QA ìŒ)")
        except Exception as e:
            st.warning(f"ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    st.markdown("---")
    
    # í‰ê°€ ì„¤ì • ìš”ì•½
    st.markdown("### ğŸ“‹ í‰ê°€ ì„¤ì • ìš”ì•½")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.write(f"**ğŸ¤– LLM ëª¨ë¸:** {selected_llm}")
    with col2:
        st.write(f"**ğŸ” ì„ë² ë”© ëª¨ë¸:** {selected_embedding}")
    with col3:
        st.write(f"**ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…:** {selected_prompt_type.value}")
    with col4:
        st.write(f"**ğŸ“Š ë°ì´í„°ì…‹:** {selected_dataset}")
    
    st.markdown("---")
    
    # í‰ê°€ ì‹¤í–‰ ë²„íŠ¼ë“¤
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        if st.button("â† ë’¤ë¡œê°€ê¸°"):
            st.session_state.navigate_to = "ğŸ¯ Overview"
            st.rerun()
    
    with col2:
        if st.button("ğŸš€ í‰ê°€ ì‹œì‘", type="primary", use_container_width=True):
            execute_evaluation(selected_prompt_type, selected_dataset, selected_llm, selected_embedding)
    
    with col3:
        st.write("")  # ë¹ˆ ê³µê°„


# ì»´í¬ë„ŒíŠ¸ í•¨ìˆ˜ë“¤ (ì§€ì—° ë¡œë”©)
def show_llm_selector():
    """LLM ì„ íƒê¸° (ì§€ì—° ë¡œë”©)"""
    try:
        from src.presentation.web.components.llm_selector import show_llm_selector as _show_llm_selector
        return _show_llm_selector()
    except ImportError:
        # ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ UI
        return st.selectbox("LLM ëª¨ë¸ ì„ íƒ", ["gemini", "hcx"], key="llm_selector")


def show_embedding_selector():
    """ì„ë² ë”© ì„ íƒê¸° (ì§€ì—° ë¡œë”©)"""
    try:
        from src.presentation.web.components.embedding_selector import show_embedding_selector as _show_embedding_selector
        return _show_embedding_selector()
    except ImportError:
        # ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ UI
        return st.selectbox("ì„ë² ë”© ëª¨ë¸ ì„ íƒ", ["gemini", "bge_m3", "hcx"], key="embedding_selector")


def show_prompt_selector():
    """í”„ë¡¬í”„íŠ¸ ì„ íƒê¸° (ì§€ì—° ë¡œë”©)"""
    try:
        from src.presentation.web.components.prompt_selector import show_prompt_selector as _show_prompt_selector
        return _show_prompt_selector()
    except ImportError:
        # ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ UI
        prompt_options = [PromptType.DEFAULT, PromptType.KOREAN_FORMAL, PromptType.NUCLEAR_HYDRO_TECH]
        selected = st.selectbox("í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„ íƒ", 
                               [p.value for p in prompt_options], 
                               key="prompt_selector")
        return next(p for p in prompt_options if p.value == selected)


def execute_evaluation(prompt_type: PromptType, dataset_name: str, llm_type: str, embedding_type: str):
    """í‰ê°€ ì‹¤í–‰ ë¡œì§ (ì§€ì—° ë¡œë”© ì ìš©)"""
    with st.spinner("ğŸ”„ í‰ê°€ë¥¼ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            # í‰ê°€ ì„¤ì • ì •ë³´ í‘œì‹œ
            st.markdown("### ğŸ”§ í‰ê°€ ì„¤ì •")
            col1, col2 = st.columns(2)
            
            with col1:
                st.info(f"ğŸ¤– **LLM ëª¨ë¸**: {llm_type}")
                st.info(f"ğŸ“Š **ë°ì´í„°ì…‹**: {dataset_name}")
            
            with col2:
                st.info(f"ğŸ” **ì„ë² ë”© ëª¨ë¸**: {embedding_type}")
                st.info(f"ğŸ¯ **í”„ë¡¬í”„íŠ¸ íƒ€ì…**: {prompt_type.value}")
            
            # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ëª… ì¶”ê°€
            if prompt_type == PromptType.DEFAULT:
                st.success("ğŸ“ **ê¸°ë³¸ RAGAS í”„ë¡¬í”„íŠ¸ (ì˜ì–´)** - ë²”ìš©ì ì´ê³  ì•ˆì •ì ì¸ í‰ê°€")
            elif prompt_type == PromptType.NUCLEAR_HYDRO_TECH:
                st.success("âš›ï¸ **ì›ìë ¥/ìˆ˜ë ¥ ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™” í”„ë¡¬í”„íŠ¸** - ê¸°ìˆ  ì •í™•ì„±ê³¼ ì•ˆì „ ê·œì •ì— ìµœì í™”")
            elif prompt_type == PromptType.KOREAN_FORMAL:
                st.success("ğŸ“‹ **í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ íŠ¹í™” í”„ë¡¬í”„íŠ¸** - ì •ì±… ë¬¸ì„œì™€ ë²•ê·œ í•´ì„ì— ìµœì í™”")
            
            st.markdown("---")

            # HCX ì„ íƒ ì‹œ API í‚¤ í™•ì¸
            if llm_type == "hcx" or embedding_type == "hcx":
                from src.config import settings
                if not settings.CLOVA_STUDIO_API_KEY:
                    st.error("âŒ HCX ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— CLOVA_STUDIO_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
                    return

            # ì»¨í…Œì´ë„ˆ ë¡œë”© (ì§€ì—° ë¡œë”©)
            st.info("ğŸ”§ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆ ë°©ì‹ ì‚¬ìš©
            from src.container import container
            from src.container.factories.evaluation_use_case_factory import EvaluationRequest
            
            request = EvaluationRequest(
                llm_type=llm_type,
                embedding_type=embedding_type,
                prompt_type=prompt_type
            )
            evaluation_use_case, llm_adapter, embedding_adapter = container.create_evaluation_use_case(request)
            
            st.info("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ë° ê²€ì¦ ì¤‘...")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_placeholder = st.empty()
            
            with progress_placeholder.container():
                st.info("âš¡ í‰ê°€ ì‹¤í–‰ ì¤‘... (ìµœëŒ€ 30ì´ˆ ì†Œìš”)")
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                # í‰ê°€ ì‹¤í–‰
                progress_text.text("í‰ê°€ ì‹œì‘...")
                progress_bar.progress(25)
                
                evaluation_result = evaluation_use_case.execute(
                    dataset_name=dataset_name
                )
                
                progress_bar.progress(100)
                progress_text.text("í‰ê°€ ì™„ë£Œ!")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
            progress_placeholder.empty()

            # ê²°ê³¼ ì €ì¥
            result_dict = evaluation_result.to_dict()
            if "metadata" not in result_dict:
                result_dict["metadata"] = {}
            result_dict["metadata"]["llm_type"] = llm_type
            result_dict["metadata"]["embedding_type"] = embedding_type
            result_dict["metadata"]["dataset"] = dataset_name
            result_dict["metadata"]["prompt_type"] = prompt_type.value

            dataset_path = get_evaluation_data_path(dataset_name)
            if dataset_path:
                try:
                    with open(dataset_path, encoding="utf-8") as f:
                        qa_data = json.load(f)
                        qa_count = len(result_dict.get("individual_scores", []))
                        result_dict["qa_data"] = qa_data[:qa_count]
                except Exception as e:
                    st.warning(f"QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

            save_evaluation_result(result_dict)

            st.success("âœ… í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.balloons()
            
            # í‰ê°€ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
            st.markdown("### ğŸ“Š í‰ê°€ ê²°ê³¼")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ† RAGAS Score", f"{result_dict.get('ragas_score', 0):.3f}")
            with col2:
                st.metric("âœ… Faithfulness", f"{result_dict.get('faithfulness', 0):.3f}")
            with col3:
                st.metric("ğŸ¯ Answer Relevancy", f"{result_dict.get('answer_relevancy', 0):.3f}")
            with col4:
                st.metric("ğŸ”„ Context Recall", f"{result_dict.get('context_recall', 0):.3f}")
            
            # ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™
            st.markdown("---")
            
            # í‰ê°€ ì™„ë£Œ ìƒíƒœ ì €ì¥
            st.session_state.evaluation_completed = True
            st.session_state.latest_evaluation_result = result_dict
            
            st.info("ğŸ’¡ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                if st.button("ğŸ“Š Overview í˜ì´ì§€ë¡œ ì´ë™", type="primary", use_container_width=True, key="goto_overview"):
                    st.session_state.navigate_to = "ğŸ¯ Overview"
                    st.rerun()
            
            with col2:
                if st.button("ğŸ“ˆ Historical í˜ì´ì§€ë¡œ ì´ë™", type="secondary", use_container_width=True, key="goto_historical"):
                    st.session_state.navigate_to = "ğŸ“ˆ Historical"
                    st.rerun()

        except Exception as e:
            st.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.exception(e)


def show_historical():
    """íˆìŠ¤í† ë¦¬ í˜ì´ì§€"""
    st.header("ğŸ“ˆ í‰ê°€ ì´ë ¥")

    history = load_evaluation_history()

    if history:
        df = pd.DataFrame(history)
        df["timestamp"] = pd.to_datetime(df["timestamp"])

        # í…Œì´ë¸” í‘œì‹œ
        st.subheader("ğŸ“‹ í‰ê°€ ì´ë ¥ í…Œì´ë¸”")

        # ê° í‰ê°€ì— ëŒ€í•œ ìƒì„¸ ì •ë³´
        for i, row in df.iterrows():
            with st.expander(
                f"í‰ê°€ #{i+1} - {row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}"
            ):
                col1, col2, col3 = st.columns([2, 2, 1])

                with col1:
                    st.metric("RAGAS ì ìˆ˜", f"{row.get('ragas_score', 0):.3f}")
                    st.metric("Faithfulness", f"{row.get('faithfulness', 0):.3f}")

                with col2:
                    st.metric(
                        "Answer Relevancy", f"{row.get('answer_relevancy', 0):.3f}"
                    )
                    st.metric("Context Recall", f"{row.get('context_recall', 0):.3f}")

                with col3:
                    st.metric(
                        "Context Precision", f"{row.get('context_precision', 0):.3f}"
                    )

                    # ìƒì„¸ ë¶„ì„ í˜ì´ì§€ë¡œ ì´ë™ ë²„íŠ¼
                    if st.button("ğŸ” ìƒì„¸ ë¶„ì„", key=f"detail_btn_{i}"):
                        st.session_state.selected_evaluation_index = i
                        st.session_state.navigate_to = "ğŸ“š Detailed Analysis"
                        st.rerun()

        # ì „ì²´ í…Œì´ë¸” í‘œì‹œ
        st.subheader("ğŸ“Š ì „ì²´ í‰ê°€ ì´ë ¥")
        st.dataframe(df, use_container_width=True)

        # í‰ê°€ ë¹„êµ
        st.subheader("ğŸ“Š í‰ê°€ ë¹„êµ")

        if len(df) > 1:
            col1, col2 = st.columns(2)

            with col1:
                eval1_idx = st.selectbox(
                    "ì²« ë²ˆì§¸ í‰ê°€",
                    range(len(df)),
                    format_func=lambda x: f"{df.iloc[x]['timestamp'].strftime('%Y-%m-%d %H:%M')} (#{x+1})",
                )

            with col2:
                eval2_idx = st.selectbox(
                    "ë‘ ë²ˆì§¸ í‰ê°€",
                    range(len(df)),
                    index=min(1, len(df) - 1),
                    format_func=lambda x: f"{df.iloc[x]['timestamp'].strftime('%Y-%m-%d %H:%M')} (#{x+1})",
                )

            if eval1_idx != eval2_idx:
                show_comparison_chart(df.iloc[eval1_idx], df.iloc[eval2_idx])

    else:
        st.info("ğŸ“ ì•„ì§ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")


def show_comparison_chart(eval1, eval2):
    """ë‘ í‰ê°€ ê²°ê³¼ ë¹„êµ ì°¨íŠ¸"""
    metrics = [
        "faithfulness",
        "answer_relevancy",
        "context_recall",
        "context_precision",
        "ragas_score",
    ]

    fig = go.Figure()

    fig.add_trace(
        go.Bar(
            name=f'í‰ê°€ 1 ({eval1["timestamp"]})',
            x=metrics,
            y=[eval1.get(m, 0) for m in metrics],
            marker_color="lightblue",
        )
    )

    fig.add_trace(
        go.Bar(
            name=f'í‰ê°€ 2 ({eval2["timestamp"]})',
            x=metrics,
            y=[eval2.get(m, 0) for m in metrics],
            marker_color="darkblue",
        )
    )

    fig.update_layout(
        title="ğŸ“Š í‰ê°€ ê²°ê³¼ ë¹„êµ", barmode="group", yaxis=dict(range=[0, 1]), height=400
    )

    st.plotly_chart(fig, use_container_width=True)


def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ í˜ì´ì§€"""
    try:
        from src.presentation.web.components.detailed_analysis import (
            show_detailed_analysis as show_detailed_component,
        )
        show_detailed_component()
    except ImportError:
        st.header("ğŸ“š ìƒì„¸ ë¶„ì„")
        st.info("ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        st.write("ì´ ê¸°ëŠ¥ì€ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")


def show_metrics_guide():
    """ë©”íŠ¸ë¦­ ê°€ì´ë“œ í˜ì´ì§€"""
    try:
        from src.presentation.web.components.metrics_explanation import (
            show_metrics_explanation as show_metrics_component,
        )
        show_metrics_component()
    except ImportError:
        st.header("ğŸ“– ë©”íŠ¸ë¦­ ì„¤ëª…")
        st.markdown("""
        ### RAGAS ë©”íŠ¸ë¦­ ì„¤ëª…
        
        **ğŸ† RAGAS Score**: ì „ì²´ ì¢…í•© ì ìˆ˜
        - ëª¨ë“  ë©”íŠ¸ë¦­ì˜ ì¡°í™” í‰ê· 
        - 0.0 ~ 1.0 ë²”ìœ„
        
        **âœ… Faithfulness**: ë‹µë³€ì˜ ì‚¬ì‹¤ ì •í™•ì„±
        - ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •
        
        **ğŸ¯ Answer Relevancy**: ë‹µë³€ì˜ ê´€ë ¨ì„±
        - ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •
        
        **ğŸ”„ Context Recall**: ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨
        - ê´€ë ¨ ì •ë³´ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¸¡ì •
        
        **ğŸ“ Context Precision**: ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„
        - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ ì¸¡ì •
        """)


def show_performance():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í˜ì´ì§€"""
    try:
        from src.presentation.web.components.performance_monitor import (
            show_performance_monitor as show_performance_component,
        )
        show_performance_component()
    except ImportError:
        st.header("âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        st.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        st.write("ì´ ê¸°ëŠ¥ì€ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")


# ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ë“¤
def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    db_path = DATABASE_PATH
    db_path.parent.mkdir(exist_ok=True)

    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()

    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS evaluations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            faithfulness REAL,
            answer_relevancy REAL,
            context_recall REAL,
            context_precision REAL,
            ragas_score REAL,
            raw_data TEXT
        )
    """
    )

    conn.commit()
    conn.close()


def save_evaluation_result(result):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    init_db()

    conn = sqlite3.connect(str(DATABASE_PATH))
    cursor = conn.cursor()

    cursor.execute(
        """
        INSERT INTO evaluations (
            timestamp, faithfulness, answer_relevancy, 
            context_recall, context_precision, ragas_score, raw_data
        ) VALUES (?, ?, ?, ?, ?, ?, ?)
    """,
        (
            datetime.now().isoformat(),
            result.get("faithfulness", 0),
            result.get("answer_relevancy", 0),
            result.get("context_recall", 0),
            result.get("context_precision", 0),
            result.get("ragas_score", 0),
            json.dumps(result),
        ),
    )

    conn.commit()
    conn.close()


def load_latest_result():
    """ìµœì‹  í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    history = load_evaluation_history(limit=1)
    return history[0] if history else None


def load_evaluation_history(limit=None):
    """í‰ê°€ ì´ë ¥ ë¡œë“œ"""
    init_db()

    conn = sqlite3.connect(str(DATABASE_PATH))

    query = """
        SELECT timestamp, faithfulness, answer_relevancy, 
               context_recall, context_precision, ragas_score
        FROM evaluations 
        ORDER BY timestamp DESC
    """

    if limit:
        query += f" LIMIT {limit}"

    df = pd.read_sql_query(query, conn)
    conn.close()

    return df.to_dict("records")


def get_previous_result():
    """ì´ì „ í‰ê°€ ê²°ê³¼ ë°˜í™˜"""
    history = load_evaluation_history(limit=2)
    return history[1] if len(history) > 1 else None


# í˜ì´ì§€ ë¼ìš°íŒ…
if page == "ğŸ¯ Overview":
    main()
elif page == "ğŸš€ New Evaluation":
    show_new_evaluation_page()
elif page == "ğŸ“ˆ Historical":
    show_historical()
elif page == "ğŸ“š Detailed Analysis":
    show_detailed_analysis()
elif page == "ğŸ“– Metrics Explanation":
    show_metrics_guide()
elif page == "âš¡ Performance":
    show_performance()
else:
    main()


if __name__ == "__main__":
    main() 