"""
Evaluation View

ìƒˆ í‰ê°€ ì‹¤í–‰ ë·°ì…ë‹ˆë‹¤.
"""

import json
import streamlit as st
from typing import List

from .base_view import BaseView
from ..models.evaluation_model import EvaluationConfig, EvaluationModel
from ..services import EvaluationService, DatabaseService
from src.domain.prompts import PromptType
from src.utils.paths import get_available_datasets, get_evaluation_data_path
from src.presentation.web.components.llm_selector import show_llm_selector
from src.presentation.web.components.embedding_selector import show_embedding_selector
from src.presentation.web.components.prompt_selector import show_prompt_selector
from ..components import llm_selector, embedding_selector, prompt_selector


class EvaluationView(BaseView):
    """ìƒˆ í‰ê°€ ì‹¤í–‰ ë·°"""
    
    def __init__(self, session_manager):
        super().__init__(session_manager)
        self.evaluation_service = EvaluationService()
        self.db_service = DatabaseService()

    def render(self) -> None:
        """ìƒˆ í‰ê°€ ì‹¤í–‰ í˜ì´ì§€ ë Œë”ë§"""
        st.title("ğŸš€ Run New Evaluation")
        st.markdown("---")
        
        # ë°ì´í„°ì…‹ ì„ íƒ
        available_datasets = self.db_service.get_available_datasets()
        self.state.selected_dataset = st.selectbox(
            "1. Select Dataset", available_datasets,
            index=available_datasets.index(self.state.selected_dataset) if self.state.selected_dataset in available_datasets else 0
        )
        
        # ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸ ì„ íƒ
        llm_type = llm_selector()
        embedding_type = embedding_selector()
        prompt_type = prompt_selector()

        if st.button("Start Evaluation", type="primary"):
            if self.state.selected_dataset:
                with st.spinner("Evaluation in progress... Please wait."):
                    result = self.evaluation_service.run_evaluation(
                        dataset_name=self.state.selected_dataset,
                        llm=llm_type,
                        embedding=embedding_type,
                        prompt_type=prompt_type.value
                    )
                    # í‰ê°€ ê²°ê³¼ì™€ ì™„ë£Œ ìƒíƒœë¥¼ AppStateì— ì €ì¥
                    self.state.evaluation.is_completed = True
                    self.state.evaluation.result = result
                    
                    # í‰ê°€ ì´ë ¥ì—ë„ ì¶”ê°€ (DB ì €ì¥ í›„)
                    self.db_service.save_evaluation_result(result)
                    
                    st.success("Evaluation completed!")
                    st.rerun() # UIë¥¼ ì¦‰ì‹œ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ê²°ê³¼ í‘œì‹œ
            else:
                st.error("Please select a dataset first.")

        # í‰ê°€ ì™„ë£Œ í›„ ê²°ê³¼ í‘œì‹œ
        if self.state.evaluation.is_completed and self.state.evaluation.result:
            st.subheader("Latest Evaluation Result")
            st.json(self.state.evaluation.result)
            
            if st.button("View Detailed Analysis"):
                # ìƒì„¸ ë¶„ì„ í˜ì´ì§€ë¡œ ë„¤ë¹„ê²Œì´ì…˜
                self.session_manager.state.selected_page = "ğŸ“ˆ Historical Analysis"
                st.rerun()
    
    def _collect_configuration(self) -> EvaluationConfig:
        """í‰ê°€ ì„¤ì • ìˆ˜ì§‘"""
        # LLM ì„ íƒ
        selected_llm = show_llm_selector()
        st.markdown("---")
        
        # ì„ë² ë”© ì„ íƒ
        selected_embedding = show_embedding_selector()
        st.markdown("---")
        
        # í”„ë¡¬í”„íŠ¸ ì„ íƒ
        selected_prompt_type = show_prompt_selector()
        st.markdown("---")
        
        # ë°ì´í„°ì…‹ ì„ íƒ
        selected_dataset = self._render_dataset_selector()
        
        if not selected_dataset:
            return None
        
        return EvaluationModel.create_config(
            llm_type=selected_llm,
            embedding_type=selected_embedding,
            prompt_type=selected_prompt_type,
            dataset_name=selected_dataset
        )
    
    def _render_dataset_selector(self) -> str:
        """ë°ì´í„°ì…‹ ì„ íƒ UI"""
        st.markdown("### ğŸ“Š ë°ì´í„°ì…‹ ì„ íƒ")
        
        existing_datasets = get_available_datasets()
        if not existing_datasets:
            self.show_error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
            self.show_info("data/ ë””ë ‰í† ë¦¬ì— JSON í˜•ì‹ì˜ í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            return None
        
        # ì„¸ì…˜ ìƒíƒœì—ì„œ ì„ íƒëœ ë°ì´í„°ì…‹ ê´€ë¦¬
        if not self.session.get_selected_dataset() or self.session.get_selected_dataset() not in existing_datasets:
            self.session.set_selected_dataset(existing_datasets[0])
        
        current_index = existing_datasets.index(self.session.get_selected_dataset())
        
        selected_dataset = st.selectbox(
            "í‰ê°€í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:",
            existing_datasets,
            index=current_index,
            key="dataset_selector_box",
            help="í‰ê°€ì— ì‚¬ìš©í•  QA ë°ì´í„°ì…‹ì„ ì„ íƒí•©ë‹ˆë‹¤."
        )
        
        self.session.set_selected_dataset(selected_dataset)
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        self._show_dataset_info(selected_dataset)
        
        return selected_dataset
    
    def _show_dataset_info(self, dataset_name: str) -> None:
        """ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ"""
        dataset_path = get_evaluation_data_path(dataset_name)
        if dataset_path:
            try:
                with open(dataset_path, encoding="utf-8") as f:
                    qa_data = json.load(f)
                    self.show_info(f"ğŸ“‹ ì„ íƒëœ ë°ì´í„°ì…‹: **{dataset_name}** ({len(qa_data)}ê°œ QA ìŒ)")
            except Exception as e:
                self.show_warning(f"ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _render_configuration_summary(self, config: EvaluationConfig) -> None:
        """í‰ê°€ ì„¤ì • ìš”ì•½"""
        st.markdown("---")
        st.markdown("### ğŸ“‹ í‰ê°€ ì„¤ì • ìš”ì•½")
        
        col1, col2, col3, col4 = self.create_columns(4)
        with col1:
            st.write(f"**ğŸ¤– LLM ëª¨ë¸:** {config.llm_type}")
        with col2:
            st.write(f"**ğŸ” ì„ë² ë”© ëª¨ë¸:** {config.embedding_type}")
        with col3:
            st.write(f"**ğŸ¯ í”„ë¡¬í”„íŠ¸ íƒ€ì…:** {config.prompt_type.value}")
        with col4:
            st.write(f"**ğŸ“Š ë°ì´í„°ì…‹:** {config.dataset_name}")
        
        st.markdown("---")
    
    def _render_execution_buttons(self, config: EvaluationConfig) -> None:
        """ì‹¤í–‰ ë²„íŠ¼ë“¤"""
        col1, col2, col3 = self.create_columns([1, 2, 1])
        
        with col1:
            self.show_navigation_button("â† ë’¤ë¡œê°€ê¸°", "ğŸ¯ Overview")
        
        with col2:
            if st.button("ğŸš€ í‰ê°€ ì‹œì‘", type="primary", use_container_width=True):
                self._execute_evaluation(config)
        
        with col3:
            st.write("")  # ë¹ˆ ê³µê°„
    
    def _execute_evaluation(self, config: EvaluationConfig) -> None:
        """í‰ê°€ ì‹¤í–‰"""
        # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
        if not EvaluationService.validate_configuration(config):
            error_msg = EvaluationService.get_configuration_error_message(config)
            self.show_error(error_msg)
            return
        
        with self.show_spinner("ğŸ”„ í‰ê°€ë¥¼ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                self._show_evaluation_info(config)
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_placeholder = st.empty()
                
                with progress_placeholder.container():
                    self.show_info("âš¡ í‰ê°€ ì‹¤í–‰ ì¤‘... (ìµœëŒ€ 30ì´ˆ ì†Œìš”)")
                    self.show_warning("ğŸ’¡ **ì°¸ê³ **: í˜„ì¬ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë¡œ ì¸í•´ ì‹¤ì œ í‰ê°€ê°€ ì§„í–‰ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 30ì´ˆ í›„ ìƒ˜í”Œ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                    progress_bar = st.progress(0)
                    progress_text = st.empty()
                    
                    progress_text.text("í‰ê°€ ì‹œì‘...")
                    progress_bar.progress(25)
                    
                    # ì‹¤ì œ í‰ê°€ ì‹¤í–‰
                    result = EvaluationService.execute_evaluation(config)
                    
                    progress_bar.progress(100)
                    progress_text.text("í‰ê°€ ì™„ë£Œ!")
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
                progress_placeholder.empty()
                
                # ì„±ê³µ ì²˜ë¦¬
                self._handle_evaluation_success(result)
                
            except Exception as e:
                self.show_error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.exception(e)
    
    def _show_evaluation_info(self, config: EvaluationConfig) -> None:
        """í‰ê°€ ì •ë³´ í‘œì‹œ"""
        st.markdown("### ğŸ”§ í‰ê°€ ì„¤ì •")
        col1, col2 = self.create_columns(2)
        
        with col1:
            self.show_info(f"ğŸ¤– **LLM ëª¨ë¸**: {config.llm_type}")
            self.show_info(f"ğŸ“Š **ë°ì´í„°ì…‹**: {config.dataset_name}")
        
        with col2:
            self.show_info(f"ğŸ” **ì„ë² ë”© ëª¨ë¸**: {config.embedding_type}")
            self.show_info(f"ğŸ¯ **í”„ë¡¬í”„íŠ¸ íƒ€ì…**: {config.prompt_type.value}")
        
        # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ëª…
        if config.prompt_type == PromptType.DEFAULT:
            self.show_success("ğŸ“ **ê¸°ë³¸ RAGAS í”„ë¡¬í”„íŠ¸ (ì˜ì–´)** - ë²”ìš©ì ì´ê³  ì•ˆì •ì ì¸ í‰ê°€")
        elif config.prompt_type == PromptType.NUCLEAR_HYDRO_TECH:
            self.show_success("âš›ï¸ **ì›ìë ¥/ìˆ˜ë ¥ ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™” í”„ë¡¬í”„íŠ¸** - ê¸°ìˆ  ì •í™•ì„±ê³¼ ì•ˆì „ ê·œì •ì— ìµœì í™”")
        elif config.prompt_type == PromptType.KOREAN_FORMAL:
            self.show_success("ğŸ“‹ **í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ íŠ¹í™” í”„ë¡¬í”„íŠ¸** - ì •ì±… ë¬¸ì„œì™€ ë²•ê·œ í•´ì„ì— ìµœì í™”")
        
        st.markdown("---")
    
    def _handle_evaluation_success(self, result) -> None:
        """í‰ê°€ ì„±ê³µ ì²˜ë¦¬"""
        self.show_success("âœ… í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        self.show_balloons()
        
        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        st.markdown("### ğŸ“Š í‰ê°€ ê²°ê³¼")
        
        # ë”ë¯¸ ê²°ê³¼ í™•ì¸
        if EvaluationModel.is_dummy_result(result):
            self.show_error("ğŸš¨ **ìƒ˜í”Œ ê²°ê³¼**: ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë¡œ ì¸í•´ ì‹¤ì œ í‰ê°€ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.markdown("""
            **í•´ê²° ë°©ë²•:**
            1. ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸
            2. Google AI API í• ë‹¹ëŸ‰ í™•ì¸
            3. ë°©í™”ë²½/í”„ë¡ì‹œ ì„¤ì • í™•ì¸
            4. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„
            """)
        
        # ê²°ê³¼ ë©”íŠ¸ë¦­ í‘œì‹œ
        col1, col2, col3, col4 = self.create_columns(4)
        with col1:
            st.metric("ğŸ† RAGAS Score", f"{result.ragas_score:.3f}")
        with col2:
            st.metric("âœ… Faithfulness", f"{result.faithfulness:.3f}")
        with col3:
            st.metric("ğŸ¯ Answer Relevancy", f"{result.answer_relevancy:.3f}")
        with col4:
            st.metric("ğŸ”„ Context Recall", f"{result.context_recall:.3f}")
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.session.set_evaluation_completed(True)
        self.session.set_latest_evaluation_result(result.raw_data)
        
        # ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼
        self._render_post_evaluation_navigation()
    
    def _render_post_evaluation_navigation(self) -> None:
        """í‰ê°€ í›„ ë„¤ë¹„ê²Œì´ì…˜"""
        st.markdown("---")
        self.show_info("ğŸ’¡ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        col1, col2 = self.create_columns([1, 1])
        
        with col1:
            if st.button("ğŸ“Š Overview í˜ì´ì§€ë¡œ ì´ë™", type="primary", use_container_width=True, key="goto_overview"):
                self.session.navigate_to("ğŸ¯ Overview")
                st.rerun()
        
        with col2:
            if st.button("ğŸ“ˆ Historical í˜ì´ì§€ë¡œ ì´ë™", type="secondary", use_container_width=True, key="goto_historical"):
                self.session.navigate_to("ğŸ“ˆ Historical")
                st.rerun()
        
        st.markdown("**ë˜ëŠ”** ì‚¬ì´ë“œë°”ì—ì„œ ë‹¤ë¥¸ í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”.")