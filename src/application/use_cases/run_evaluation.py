"""Run evaluation use case"""

from typing import Any, Optional, TYPE_CHECKING

from datasets import Dataset

from src.application.ports import (
    EvaluationRepositoryPort,
    EvaluationRunnerPort,
    LlmPort,
)
from src.application.services.data_validator import DataContentValidator
from src.application.services.generation_service import GenerationService
from src.application.services.result_conversion_service import ResultConversionService
from src.domain import EvaluationData, EvaluationError, EvaluationResult
from src.domain.exceptions import DataValidationError
from src.domain.prompts import PromptType

if TYPE_CHECKING:
    from src.infrastructure.evaluation import RagasEvalAdapterFactory
    from src.infrastructure.repository import FileRepositoryFactory


class RunEvaluationUseCase:
    """í‰ê°€ ì‹¤í–‰ ìœ ìŠ¤ì¼€ì´ìŠ¤"""

    def __init__(
        self,
        llm_port: LlmPort,
        evaluation_runner_factory: "RagasEvalAdapterFactory",
        repository_factory: "FileRepositoryFactory",
    ):
        self.llm_port = llm_port
        self.evaluation_runner_factory = evaluation_runner_factory
        self.repository_factory = repository_factory
        self.data_validator = DataContentValidator()
        self.generation_service = GenerationService(answer_generator=llm_port)
        self.result_conversion_service = ResultConversionService()

    def execute(
        self, 
        dataset_name: str, 
        prompt_type: Optional[PromptType] = None
    ) -> EvaluationResult:
        """
        í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            dataset_name: í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„
            prompt_type: ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì… (ì„ íƒì‚¬í•­)

        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼ ì—”í‹°í‹°

        Raises:
            EvaluationError: í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        try:
            # 1. í•„ìš”í•œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            repository_port = self.repository_factory.create_repository(dataset_name)
            evaluation_runner = self.evaluation_runner_factory.create_evaluator(prompt_type)

            # 2. ë°ì´í„° ë¡œë“œ
            evaluation_data_list = repository_port.load_data()
            if not evaluation_data_list:
                raise EvaluationError("í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            print(f"í‰ê°€í•  ë°ì´í„° ê°œìˆ˜: {len(evaluation_data_list)}ê°œ")

            # 3. ë°ì´í„° ë‚´ìš© ì‚¬ì „ ê²€ì¦
            print("ë°ì´í„° ë‚´ìš©ì„ ê²€ì¦í•˜ëŠ” ì¤‘...")
            validation_report = self.data_validator.validate_data_list(evaluation_data_list)
            
            if validation_report.has_errors or validation_report.has_warnings:
                validation_message = self.data_validator.create_user_friendly_report(validation_report)
                print(f"\n{validation_message}")
                
                if validation_report.has_errors:
                    # ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš° ì‚¬ìš©ìì—ê²Œ ì„ íƒê¶Œ ì œê³µ (ì‹¤ì œ UIì—ì„œëŠ” ë²„íŠ¼ìœ¼ë¡œ ì²˜ë¦¬)
                    print("\nâ“ ì˜¤ë¥˜ê°€ ìˆëŠ” ë°ì´í„°ë¡œ í‰ê°€ë¥¼ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                    print("   í’ˆì§ˆì´ ë‚®ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    # í˜„ì¬ëŠ” ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
                elif validation_report.has_warnings:
                    print("\nğŸ’¡ ê²½ê³ ì‚¬í•­ì´ ìˆì§€ë§Œ í‰ê°€ë¥¼ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

            # 4. ë‹µë³€ ìƒì„± (Generation ë‹¨ê³„)
            print("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
            generation_result = self.generation_service.generate_missing_answers(evaluation_data_list)

            # 5. Ragas ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë‹µë³€ì´ í¬í•¨ëœ ìƒíƒœ)
            dataset = self._convert_to_dataset(evaluation_data_list)

            # 6. LLM ê°ì²´ ê°€ì ¸ì˜¤ê¸° (í‰ê°€ë¥¼ ìœ„í•´ì„œë§Œ ì‚¬ìš©)
            llm = self.llm_port.get_llm()

            # 7. í‰ê°€ ì‹¤í–‰ (Evaluation ë‹¨ê³„)
            print("í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘...")
            result_dict = evaluation_runner.evaluate(dataset=dataset, llm=llm)

            # 8. ê²°ê³¼ ê²€ì¦ ë° ë³€í™˜
            return self.result_conversion_service.validate_and_convert_result(
                result_dict, 
                generation_result.failures, 
                generation_result.successes, 
                generation_result.failure_details
            )

        except Exception as e:
            if isinstance(e, EvaluationError):
                raise
            raise EvaluationError(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}") from e

    def _convert_to_dataset(
        self, evaluation_data_list: list[EvaluationData]
    ) -> Dataset:
        """í‰ê°€ ë°ì´í„°ë¥¼ Ragas Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        data_dict = {
            "question": [d.question for d in evaluation_data_list],
            "contexts": [d.contexts for d in evaluation_data_list],
            "answer": [d.answer for d in evaluation_data_list],
            "ground_truth": [d.ground_truth for d in evaluation_data_list],
        }
        return Dataset.from_dict(data_dict)

