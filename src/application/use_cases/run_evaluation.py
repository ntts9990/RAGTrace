"""Run evaluation use case"""

from typing import Any, Optional, TYPE_CHECKING

from datasets import Dataset

from src.application.ports import (
    EvaluationRepositoryPort,
    EvaluationRunnerPort,
    LlmPort,
)
from src.application.services.data_validator import DataContentValidator
from src.domain import EvaluationData, EvaluationError, EvaluationResult
from src.domain.exceptions import DataValidationError
from src.domain.prompts import PromptType

if TYPE_CHECKING:
    from src.factories import FileRepositoryFactory, RagasEvalAdapterFactory


class RunEvaluationUseCase:
    """í‰ê°€ ì‹¤í–‰ ìœ ìŠ¤ì¼€ì´ìŠ¤"""

    def __init__(
        self,
        llm_port: LlmPort,
        evaluation_runner_factory: "RagasEvalAdapterFactory",
        repository_factory: "FileRepositoryFactory",
    ):
        self.llm_port = llm_port
        self.evaluation_runner_factory = evaluation_runner_factory
        self.repository_factory = repository_factory
        self.data_validator = DataContentValidator()

    def execute(
        self, 
        dataset_name: str, 
        prompt_type: Optional[PromptType] = None
    ) -> EvaluationResult:
        """
        í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            dataset_name: í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„
            prompt_type: ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì… (ì„ íƒì‚¬í•­)

        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼ ì—”í‹°í‹°

        Raises:
            EvaluationError: í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        try:
            # 1. í•„ìš”í•œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            repository_port = self.repository_factory.create_repository(dataset_name)
            evaluation_runner = self.evaluation_runner_factory.create_evaluator(prompt_type)

            # 2. ë°ì´í„° ë¡œë“œ
            evaluation_data_list = repository_port.load_data()
            if not evaluation_data_list:
                raise EvaluationError("í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            print(f"í‰ê°€í•  ë°ì´í„° ê°œìˆ˜: {len(evaluation_data_list)}ê°œ")

            # 3. ë°ì´í„° ë‚´ìš© ì‚¬ì „ ê²€ì¦
            print("ë°ì´í„° ë‚´ìš©ì„ ê²€ì¦í•˜ëŠ” ì¤‘...")
            validation_report = self.data_validator.validate_data_list(evaluation_data_list)
            
            if validation_report.has_errors or validation_report.has_warnings:
                validation_message = self.data_validator.create_user_friendly_report(validation_report)
                print(f"\n{validation_message}")
                
                if validation_report.has_errors:
                    # ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš° ì‚¬ìš©ìì—ê²Œ ì„ íƒê¶Œ ì œê³µ (ì‹¤ì œ UIì—ì„œëŠ” ë²„íŠ¼ìœ¼ë¡œ ì²˜ë¦¬)
                    print("\nâ“ ì˜¤ë¥˜ê°€ ìˆëŠ” ë°ì´í„°ë¡œ í‰ê°€ë¥¼ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                    print("   í’ˆì§ˆì´ ë‚®ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    # í˜„ì¬ëŠ” ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
                elif validation_report.has_warnings:
                    print("\nğŸ’¡ ê²½ê³ ì‚¬í•­ì´ ìˆì§€ë§Œ í‰ê°€ë¥¼ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

            # 4. ë‹µë³€ ìƒì„± (Generation ë‹¨ê³„)
            print("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
            generation_failures = 0
            generation_successes = 0
            api_failure_details = []
            
            for i, data in enumerate(evaluation_data_list):
                if not data.answer:  # ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìƒì„±
                    try:
                        generated_answer = self.llm_port.generate_answer(
                            question=data.question,
                            contexts=data.contexts
                        )
                        data.answer = generated_answer
                        generation_successes += 1
                        print(f"ë‹µë³€ ìƒì„± ì™„ë£Œ ({i+1}/{len(evaluation_data_list)})")
                    except Exception as e:
                        generation_failures += 1
                        # ìƒì„¸í•œ ì‹¤íŒ¨ ì •ë³´ ìˆ˜ì§‘
                        failure_detail = {
                            "item_index": i + 1,
                            "question": data.question[:100] + "..." if len(data.question) > 100 else data.question,
                            "error_type": type(e).__name__,
                            "error_message": str(e)
                        }
                        api_failure_details.append(failure_detail)
                        
                        print(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ({i+1}/{len(evaluation_data_list)}): {e}")
                        # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë‹µë³€ìœ¼ë¡œ ì„¤ì •
                        data.answer = ""
                else:
                    generation_successes += 1

            # 5. Ragas ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë‹µë³€ì´ í¬í•¨ëœ ìƒíƒœ)
            dataset = self._convert_to_dataset(evaluation_data_list)

            # 6. LLM ê°ì²´ ê°€ì ¸ì˜¤ê¸° (í‰ê°€ë¥¼ ìœ„í•´ì„œë§Œ ì‚¬ìš©)
            llm = self.llm_port.get_llm()

            # 7. í‰ê°€ ì‹¤í–‰ (Evaluation ë‹¨ê³„)
            print("í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘...")
            result_dict = evaluation_runner.evaluate(dataset=dataset, llm=llm)

            # 8. ê²°ê³¼ ê²€ì¦ ë° ë³€í™˜
            return self._validate_and_convert_result(
                result_dict, 
                generation_failures, 
                generation_successes, 
                api_failure_details
            )

        except Exception as e:
            if isinstance(e, EvaluationError):
                raise
            raise EvaluationError(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}") from e

    def _convert_to_dataset(
        self, evaluation_data_list: list[EvaluationData]
    ) -> Dataset:
        """í‰ê°€ ë°ì´í„°ë¥¼ Ragas Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        data_dict = {
            "question": [d.question for d in evaluation_data_list],
            "contexts": [d.contexts for d in evaluation_data_list],
            "answer": [d.answer for d in evaluation_data_list],
            "ground_truth": [d.ground_truth for d in evaluation_data_list],
        }
        return Dataset.from_dict(data_dict)

    def _validate_and_convert_result(
        self, 
        result_dict: dict[str, Any],
        generation_failures: int = 0,
        generation_successes: int = 0,
        api_failure_details: list[dict] = None
    ) -> EvaluationResult:
        """ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ê²€ì¦í•˜ê³  EvaluationResultë¡œ ë³€í™˜"""
        if not result_dict:
            raise EvaluationError("í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # í•„ìˆ˜ ë©”íŠ¸ë¦­ í™•ì¸
        required_metrics = [
            "faithfulness",
            "answer_relevancy",
            "context_recall",
            "context_precision",
        ]
        for metric in required_metrics:
            if metric not in result_dict:
                raise EvaluationError(f"í•„ìˆ˜ ë©”íŠ¸ë¦­ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {metric}")

        # ê²°ê³¼ê°€ ëª¨ë‘ 0ì¸ì§€ í™•ì¸ ë° ìƒì„± ì‹¤íŒ¨ì™€ ì—°ê´€ì„± ì²´í¬
        metric_values = [result_dict.get(metric, 0.0) for metric in required_metrics]
        if all(v == 0.0 for v in metric_values):
            warning_message = "\nê²½ê³ : ëª¨ë“  í‰ê°€ ì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤."
            if generation_failures > 0:
                warning_message += f"\në‹µë³€ ìƒì„± ì‹¤íŒ¨ê°€ {generation_failures}ê±´ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            warning_message += "\në‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:"
            warning_message += "\n1. API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸"
            warning_message += "\n2. Gemini API í• ë‹¹ëŸ‰ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸"
            warning_message += "\n3. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ì •ìƒì¸ì§€ í™•ì¸"
            warning_message += "\n4. í‰ê°€ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸"
            print(warning_message)

        # ìƒì„± ì‹¤íŒ¨ì— ëŒ€í•œ ê²½ê³ 
        if generation_failures > 0:
            total_attempts = generation_failures + generation_successes
            failure_rate = generation_failures / total_attempts * 100
            print(f"\nâš ï¸  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {generation_failures}/{total_attempts}ê±´ ({failure_rate:.1f}%)")
            print("   ì´ëŠ” í‰ê°€ ê²°ê³¼ì˜ ì‹ ë¢°ë„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # EvaluationResult ìƒì„±
        return EvaluationResult(
            faithfulness=result_dict["faithfulness"],
            answer_relevancy=result_dict["answer_relevancy"],
            context_recall=result_dict["context_recall"],
            context_precision=result_dict["context_precision"],
            ragas_score=result_dict.get("ragas_score", 0.0),
            individual_scores=result_dict.get("individual_scores"),
            metadata=result_dict.get("metadata", {}),
            generation_failures=generation_failures,
            generation_successes=generation_successes,
            api_failure_details=api_failure_details or [],
        )
