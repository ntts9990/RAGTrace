"""
Base Command for Evaluation Pipeline

í‰ê°€ íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ Command ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Optional, List

from datasets import Dataset
from src.domain import EvaluationData, EvaluationError, EvaluationResult
from src.domain.prompts import PromptType
from src.application.services.generation_service import GenerationResult


@dataclass
class EvaluationContext:
    """í‰ê°€ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ - ëª…ë ¹ë“¤ ê°„ ë°ì´í„° ê³µìœ """
    
    # ì…ë ¥ ë°ì´í„°
    dataset_name: str
    prompt_type: Optional[PromptType] = None
    
    # ì¤‘ê°„ ê²°ê³¼ë“¤
    raw_data: Optional[List[EvaluationData]] = None
    validation_report: Optional[Any] = None
    generation_result: Optional[GenerationResult] = None
    ragas_dataset: Optional[Dataset] = None
    evaluation_result_dict: Optional[dict] = None
    
    # ìµœì¢… ê²°ê³¼
    final_result: Optional[EvaluationResult] = None


class EvaluationCommand(ABC):
    """í‰ê°€ ëª…ë ¹ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def execute(self, context: EvaluationContext) -> None:
        """ëª…ë ¹ ì‹¤í–‰"""
        pass
    
    @abstractmethod
    def get_command_name(self) -> str:
        """ëª…ë ¹ ì´ë¦„ ë°˜í™˜"""
        pass
    
    def log_start(self):
        """ëª…ë ¹ ì‹œì‘ ë¡œê·¸"""
        print(f"ğŸ”„ {self.get_command_name()} ì‹œì‘...")
    
    def log_success(self):
        """ëª…ë ¹ ì„±ê³µ ë¡œê·¸"""
        print(f"âœ… {self.get_command_name()} ì™„ë£Œ")
    
    def log_error(self, error: Exception):
        """ëª…ë ¹ ì˜¤ë¥˜ ë¡œê·¸"""
        print(f"âŒ {self.get_command_name()} ì‹¤íŒ¨: {str(error)}")
        raise EvaluationError(f"{self.get_command_name()} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(error)}") from error