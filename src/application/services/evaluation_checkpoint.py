"""
í‰ê°€ ì²´í¬í¬ì¸íŠ¸ ì„œë¹„ìŠ¤

ëŒ€ëŸ‰ ë°ì´í„°ì…‹ í‰ê°€ ì‹œ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ë³µêµ¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import pickle
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import uuid


class EvaluationCheckpoint:
    """í‰ê°€ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.current_session_id = None
        self.current_checkpoint_file = None
        
    def start_session(self, dataset_name: str, dataset_size: int, config: Dict[str, Any]) -> str:
        """ìƒˆë¡œìš´ í‰ê°€ ì„¸ì…˜ ì‹œì‘"""
        session_id = f"{dataset_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"
        self.current_session_id = session_id
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        self.current_checkpoint_file = self.checkpoint_dir / f"{session_id}.checkpoint"
        
        # ì´ˆê¸° ì²´í¬í¬ì¸íŠ¸ ìƒì„±
        initial_checkpoint = {
            'session_id': session_id,
            'dataset_name': dataset_name,
            'dataset_size': dataset_size,
            'config': config,
            'start_time': datetime.now().isoformat(),
            'status': 'started',
            'completed_items': 0,
            'individual_results': [],
            'partial_metrics': {},
            'error_count': 0,
            'last_update': datetime.now().isoformat()
        }
        
        self.save_checkpoint(initial_checkpoint)
        print(f"ğŸ“‹ í‰ê°€ ì„¸ì…˜ ì‹œì‘: {session_id}")
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: {self.current_checkpoint_file}")
        
        return session_id
    
    def save_checkpoint(self, data: Dict[str, Any]):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        data['last_update'] = datetime.now().isoformat()
        
        # JSONìœ¼ë¡œ ì €ì¥ (ê°€ë…ì„±)
        with open(self.current_checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ë°±ì—…ìš© pickle ì €ì¥ (ì•ˆì •ì„±)
        backup_file = self.current_checkpoint_file.with_suffix('.backup')
        with open(backup_file, 'wb') as f:
            pickle.dump(data, f)
    
    def load_checkpoint(self, session_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if session_id:
            checkpoint_file = self.checkpoint_dir / f"{session_id}.checkpoint"
        else:
            checkpoint_file = self.current_checkpoint_file
        
        if not checkpoint_file or not checkpoint_file.exists():
            return None
        
        try:
            # JSON ë¡œë“œ ì‹œë„
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as json_error:
            print(f"âš ï¸ JSON ë¡œë“œ ì‹¤íŒ¨, ë°±ì—… ì‹œë„: {json_error}")
            try:
                # ë°±ì—… pickle ë¡œë“œ ì‹œë„
                backup_file = checkpoint_file.with_suffix('.backup')
                with open(backup_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as pickle_error:
                print(f"âŒ ë°±ì—… ë¡œë“œë„ ì‹¤íŒ¨: {pickle_error}")
                return None
    
    def update_progress(self, completed_items: int, new_results: List[Dict[str, Any]], 
                       partial_metrics: Dict[str, float] = None, error_count: int = 0):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        if not self.current_checkpoint_file:
            return
        
        checkpoint = self.load_checkpoint()
        if not checkpoint:
            return
        
        checkpoint['completed_items'] = completed_items
        checkpoint['individual_results'].extend(new_results)
        
        if partial_metrics:
            checkpoint['partial_metrics'] = partial_metrics
        
        checkpoint['error_count'] = error_count
        checkpoint['progress_percentage'] = (completed_items / checkpoint['dataset_size']) * 100
        
        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
        if completed_items > 0:
            start_time = datetime.fromisoformat(checkpoint['start_time'])
            elapsed = (datetime.now() - start_time).total_seconds()
            estimated_total = elapsed * (checkpoint['dataset_size'] / completed_items)
            remaining = estimated_total - elapsed
            checkpoint['estimated_completion'] = (datetime.now().timestamp() + remaining)
        
        self.save_checkpoint(checkpoint)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        progress = checkpoint['progress_percentage']
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {completed_items}/{checkpoint['dataset_size']} ({progress:.1f}%)")
    
    def complete_session(self, final_result: Dict[str, Any]):
        """ì„¸ì…˜ ì™„ë£Œ"""
        if not self.current_checkpoint_file:
            return
        
        checkpoint = self.load_checkpoint()
        if not checkpoint:
            return
        
        checkpoint['status'] = 'completed'
        checkpoint['completion_time'] = datetime.now().isoformat()
        checkpoint['final_result'] = final_result
        
        # ì™„ë£Œëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
        result_file = self.current_checkpoint_file.with_suffix('.result.json')
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        self.save_checkpoint(checkpoint)
        print(f"âœ… í‰ê°€ ì„¸ì…˜ ì™„ë£Œ: {self.current_session_id}")
        print(f"ğŸ“„ ìµœì¢… ê²°ê³¼: {result_file}")
    
    def list_sessions(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ì„¸ì…˜ ëª©ë¡"""
        sessions = []
        
        for checkpoint_file in self.checkpoint_dir.glob("*.checkpoint"):
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                sessions.append({
                    'session_id': data.get('session_id'),
                    'dataset_name': data.get('dataset_name'),
                    'dataset_size': data.get('dataset_size'),
                    'status': data.get('status'),
                    'progress': data.get('progress_percentage', 0),
                    'completed_items': data.get('completed_items', 0),
                    'start_time': data.get('start_time'),
                    'last_update': data.get('last_update'),
                    'checkpoint_file': str(checkpoint_file)
                })
            except Exception as e:
                print(f"âš ï¸ ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨ {checkpoint_file}: {e}")
        
        return sorted(sessions, key=lambda x: x.get('start_time', ''), reverse=True)
    
    def resume_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ì„¸ì…˜ ì¬ê°œ"""
        checkpoint = self.load_checkpoint(session_id)
        if not checkpoint:
            print(f"âŒ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}")
            return None
        
        if checkpoint.get('status') == 'completed':
            print(f"â„¹ï¸ ì´ë¯¸ ì™„ë£Œëœ ì„¸ì…˜ì…ë‹ˆë‹¤: {session_id}")
            return checkpoint
        
        self.current_session_id = session_id
        self.current_checkpoint_file = self.checkpoint_dir / f"{session_id}.checkpoint"
        
        print(f"ğŸ”„ ì„¸ì…˜ ì¬ê°œ: {session_id}")
        print(f"ğŸ“Š ì§„í–‰ ìƒí™©: {checkpoint.get('completed_items', 0)}/{checkpoint.get('dataset_size', 0)}")
        
        return checkpoint
    
    def cleanup_old_sessions(self, days: int = 7):
        """ì˜¤ë˜ëœ ì„¸ì…˜ ì •ë¦¬"""
        cutoff_time = time.time() - (days * 24 * 3600)
        cleaned = 0
        
        for checkpoint_file in self.checkpoint_dir.glob("*.checkpoint"):
            if checkpoint_file.stat().st_mtime < cutoff_time:
                try:
                    # ë°±ì—… íŒŒì¼ë„ í•¨ê»˜ ì‚­ì œ
                    backup_file = checkpoint_file.with_suffix('.backup')
                    if backup_file.exists():
                        backup_file.unlink()
                    
                    checkpoint_file.unlink()
                    cleaned += 1
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {checkpoint_file}: {e}")
        
        if cleaned > 0:
            print(f"ğŸ§¹ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ {cleaned}ê°œ ì •ë¦¬ ì™„ë£Œ")


class BatchEvaluationManager:
    """ë°°ì¹˜ í‰ê°€ ê´€ë¦¬ì"""
    
    def __init__(self, checkpoint_manager: EvaluationCheckpoint, batch_size: int = 5):
        self.checkpoint = checkpoint_manager
        self.batch_size = batch_size
        
    def evaluate_with_checkpoints(self, dataset, evaluation_func, config: Dict[str, Any]) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ë°°ì¹˜ í‰ê°€ ì‹¤í–‰"""
        dataset_name = config.get('dataset_name', 'unknown')
        dataset_size = len(dataset)
        
        # ì„¸ì…˜ ì‹œì‘
        session_id = self.checkpoint.start_session(dataset_name, dataset_size, config)
        
        all_results = []
        completed_count = 0
        error_count = 0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        import psutil
        import gc
        
        try:
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
            for batch_start in range(0, dataset_size, self.batch_size):
                batch_end = min(batch_start + self.batch_size, dataset_size)
                batch_data = dataset.select(range(batch_start, batch_end))
                
                print(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬: {batch_start+1}-{batch_end}/{dataset_size}")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                memory_percent = psutil.virtual_memory().percent
                if memory_percent > 85:
                    print(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ ({memory_percent:.1f}%) - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰")
                    gc.collect()
                
                try:
                    # ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
                    batch_result = evaluation_func(batch_data)
                    
                    # ê°œë³„ ê²°ê³¼ ì¶”ì¶œ
                    if hasattr(batch_result, 'to_pandas'):
                        df = batch_result.to_pandas()
                        batch_individual = df.to_dict('records')
                    elif isinstance(batch_result, dict) and 'individual_scores' in batch_result:
                        batch_individual = batch_result['individual_scores']
                    else:
                        # í´ë°±: ë°°ì¹˜ í¬ê¸°ë§Œí¼ í‰ê·  ì ìˆ˜ë¡œ ìƒì„±
                        avg_scores = self._extract_average_scores(batch_result)
                        batch_individual = [avg_scores] * len(batch_data)
                    
                    all_results.extend(batch_individual)
                    completed_count += len(batch_data)
                    
                    # ì¤‘ê°„ ë©”íŠ¸ë¦­ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
                    partial_metrics = self._calculate_partial_metrics(all_results[-50:])  # ìµœê·¼ 50ê°œë§Œ ì‚¬ìš©
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
                    self.checkpoint.update_progress(
                        completed_count, 
                        batch_individual, 
                        partial_metrics, 
                        error_count
                    )
                    
                    # ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸° (API ì œí•œ ê³ ë ¤) - í° ë°°ì¹˜ëŠ” ë” ì˜¤ë˜ ëŒ€ê¸°
                    wait_time = min(2, self.batch_size * 0.2)
                    time.sleep(wait_time)
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del batch_result, batch_individual
                    
                except Exception as batch_error:
                    error_count += 1
                    print(f"âŒ ë°°ì¹˜ {batch_start+1}-{batch_end} ì²˜ë¦¬ ì‹¤íŒ¨: {batch_error}")
                    
                    # ì‹¤íŒ¨í•œ í•­ëª©ë“¤ì„ 0ì ìœ¼ë¡œ ì²˜ë¦¬
                    failed_items = [self._create_zero_scores()] * len(batch_data)
                    all_results.extend(failed_items)
                    completed_count += len(batch_data)
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
                    self.checkpoint.update_progress(
                        completed_count, 
                        failed_items, 
                        error_count=error_count
                    )
                    
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ë” ì˜¤ë˜ ëŒ€ê¸°
                    time.sleep(3)
                
                # ì •ê¸°ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                if batch_start % (self.batch_size * 5) == 0:
                    gc.collect()
            
            # ìµœì¢… ê²°ê³¼ ê³„ì‚°
            final_result = self._compile_final_result(all_results, config, error_count)
            
            # ì„¸ì…˜ ì™„ë£Œ
            self.checkpoint.complete_session(final_result)
            
            return final_result
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}")
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ì €ì¥
            partial_result = self._compile_final_result(all_results, config, error_count, partial=True)
            self.checkpoint.save_checkpoint({
                **self.checkpoint.load_checkpoint(),
                'status': 'failed',
                'error': str(e),
                'partial_result': partial_result
            })
            raise
    
    def _extract_average_scores(self, result) -> Dict[str, float]:
        """ê²°ê³¼ì—ì„œ í‰ê·  ì ìˆ˜ ì¶”ì¶œ"""
        scores = {}
        
        if isinstance(result, dict):
            for key, value in result.items():
                if key not in ['individual_scores', 'metadata'] and isinstance(value, (int, float)):
                    scores[key] = float(value)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì„¤ì •
        default_metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness']
        for metric in default_metrics:
            if metric not in scores:
                scores[metric] = 0.0
        
        return scores
    
    def _create_zero_scores(self) -> Dict[str, float]:
        """0ì  ê²°ê³¼ ìƒì„±"""
        return {
            'faithfulness': 0.0,
            'answer_relevancy': 0.0,
            'context_recall': 0.0,
            'context_precision': 0.0,
            'answer_correctness': 0.0
        }
    
    def _calculate_partial_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """ë¶€ë¶„ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not results:
            return {}
        
        metrics = {}
        metric_names = results[0].keys()
        
        for metric in metric_names:
            values = [r.get(metric, 0) for r in results if r.get(metric) is not None]
            if values:
                metrics[metric] = sum(values) / len(values)
            else:
                metrics[metric] = 0.0
        
        # RAGAS ì¢…í•© ì ìˆ˜ ê³„ì‚°
        core_metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        if 'answer_correctness' in metrics:
            core_metrics.append('answer_correctness')
        
        core_values = [metrics.get(m, 0) for m in core_metrics if metrics.get(m, 0) > 0]
        if core_values:
            metrics['ragas_score'] = sum(core_values) / len(core_values)
        else:
            metrics['ragas_score'] = 0.0
        
        return metrics
    
    def _compile_final_result(self, individual_results: List[Dict[str, Any]], 
                            config: Dict[str, Any], error_count: int, partial: bool = False) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ ì»´íŒŒì¼"""
        import uuid
        from datetime import datetime
        
        # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
        final_metrics = self._calculate_partial_metrics(individual_results)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            'evaluation_id': str(uuid.uuid4())[:8],
            'timestamp': datetime.now().isoformat(),
            'model': config.get('llm_type', 'unknown'),
            'embedding_model': config.get('embedding_type', 'unknown'),
            'prompt_type': str(config.get('prompt_type', 'unknown')),
            'dataset_size': len(individual_results),
            'dataset_name': config.get('dataset_name', 'unknown'),
            'error_count': error_count,
            'success_rate': ((len(individual_results) - error_count) / len(individual_results)) * 100 if individual_results else 0,
            'batch_size': self.batch_size,
            'partial_result': partial
        }
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        result = {
            **final_metrics,
            'individual_scores': individual_results,
            'metadata': metadata
        }
        
        return result


def resume_evaluation_from_checkpoint(session_id: str) -> Optional[Dict[str, Any]]:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ í‰ê°€ ì¬ê°œ"""
    checkpoint_manager = EvaluationCheckpoint()
    return checkpoint_manager.resume_session(session_id)