"""
ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤

í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ê³¼ ë¶„ì„ ë³´ê³ ì„œë¡œ ë‚´ë³´ë‚´ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import csv
import json
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd


class ResultExporter:
    """í‰ê°€ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤"""
    
    def __init__(self, output_dir: str = "exports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def export_to_csv(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_evaluation_{timestamp}.csv"
        
        csv_path = self.output_dir / filename
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´
        metadata = result.get("metadata", {})
        
        # ê°œë³„ ì ìˆ˜ ë°ì´í„° ì¤€ë¹„
        individual_scores = result.get("individual_scores", [])
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            # CSV í—¤ë” ì •ì˜
            fieldnames = [
                'item_id', 'faithfulness', 'answer_relevancy', 
                'context_recall', 'context_precision'
            ]
            
            # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if individual_scores and 'answer_correctness' in individual_scores[0]:
                fieldnames.append('answer_correctness')
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            # ê°œë³„ ì ìˆ˜ ì‘ì„±
            for i, scores in enumerate(individual_scores):
                row = {'item_id': i + 1}
                row.update(scores)
                writer.writerow(row)
        
        return str(csv_path)
    
    def export_summary_csv(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ìš”ì•½ í†µê³„ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_summary_{timestamp}.csv"
        
        csv_path = self.output_dir / filename
        
        summary_data = []
        
        # ë©”íŠ¸ë¦­ë³„ í†µê³„ ê³„ì‚°
        individual_scores = result.get("individual_scores", [])
        
        if individual_scores:
            # individual_scoresê°€ ìˆëŠ” ê²½ìš°: ìƒì„¸ í†µê³„ ê³„ì‚°
            metrics = list(individual_scores[0].keys())
            
            for metric in metrics:
                scores = [item[metric] for item in individual_scores if item[metric] is not None]
                
                if scores:
                    summary_data.append({
                        'metric': metric,
                        'mean': statistics.mean(scores),
                        'median': statistics.median(scores),
                        'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0.0,
                        'min': min(scores),
                        'max': max(scores),
                        'count': len(scores),
                        'q1': statistics.quantiles(scores, n=4)[0] if len(scores) >= 4 else min(scores),
                        'q3': statistics.quantiles(scores, n=4)[2] if len(scores) >= 4 else max(scores),
                    })
        else:
            # individual_scoresê°€ ì—†ëŠ” ê²½ìš°: ì „ì²´ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë‹¨ì¼ ê°’ í†µê³„ ìƒì„±
            metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
            
            # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if 'answer_correctness' in result:
                metrics.append('answer_correctness')
            
            for metric in metrics:
                value = result.get(metric, 0)
                if value is not None:
                    summary_data.append({
                        'metric': metric,
                        'mean': value,
                        'median': value,
                        'std_dev': 0.0,
                        'min': value,
                        'max': value,
                        'count': 1,
                        'q1': value,
                        'q3': value,
                    })
        
        # ì „ì²´ ì ìˆ˜ ì¶”ê°€
        ragas_score = result.get('ragas_score', 0)
        if ragas_score is not None:
            summary_data.append({
                'metric': 'ragas_score',
                'mean': ragas_score,
                'median': ragas_score,
                'std_dev': 0.0,
                'min': ragas_score,
                'max': ragas_score,
                'count': 1,
                'q1': ragas_score,
                'q3': ragas_score,
            })
        
        # CSV ì‘ì„±
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['metric', 'mean', 'median', 'std_dev', 'min', 'max', 'count', 'q1', 'q3']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(summary_data)
        
        return str(csv_path)
    
    def generate_analysis_report(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_analysis_report_{timestamp}.md"
        
        report_path = self.output_dir / filename
        
        metadata = result.get("metadata", {})
        individual_scores = result.get("individual_scores", [])
        
        # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
        report_content = self._generate_report_content(result, metadata, individual_scores)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return str(report_path)
    
    def _generate_report_content(self, result: Dict[str, Any], metadata: Dict[str, Any], individual_scores: List[Dict]) -> str:
        """ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""
        
        # ê¸°ì´ˆ í†µê³„ ê³„ì‚°
        stats = self._calculate_statistics(individual_scores)
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        performance_grades = self._calculate_performance_grades(result)
        
        # ë³´ê³ ì„œ í…œí”Œë¦¿
        content = f"""# RAGTrace í‰ê°€ ê²°ê³¼ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“Š í‰ê°€ ê°œìš”

**í‰ê°€ ID**: {metadata.get('evaluation_id', 'N/A')}  
**í‰ê°€ ì¼ì‹œ**: {metadata.get('timestamp', 'N/A')}  
**LLM ëª¨ë¸**: {metadata.get('model', 'N/A')}  
**ë°ì´í„°ì…‹ í¬ê¸°**: {metadata.get('dataset_size', 'N/A')}ê°œ ë¬¸í•­  
**ì´ í‰ê°€ ì‹œê°„**: {metadata.get('total_duration_minutes', 0):.1f}ë¶„  
**í‰ê·  ë¬¸í•­ë‹¹ ì‹œê°„**: {metadata.get('avg_time_per_item_seconds', 0):.1f}ì´ˆ  

## ğŸ¯ ì „ì²´ ì„±ëŠ¥ ìš”ì•½

**RAGAS ì¢…í•© ì ìˆ˜**: {result.get('ragas_score', 0):.3f} / 1.000

### ë©”íŠ¸ë¦­ë³„ ì ìˆ˜
{self._format_metric_scores(result)}

### ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
{self._format_performance_grades(performance_grades)}

## ğŸ“ˆ ê¸°ì´ˆ í†µê³„ ë¶„ì„

{self._format_statistics_table(stats)}

## ğŸ” ìƒì„¸ ë¶„ì„

### 1. Faithfulness (ë‹µë³€ ì¶©ì‹¤ë„)
- **í˜„ì¬ ì ìˆ˜**: {result.get('faithfulness', 0):.3f}
- **í•´ì„**: {self._interpret_faithfulness(result.get('faithfulness', 0))}
{self._format_metric_analysis('faithfulness', stats.get('faithfulness', {}))}

### 2. Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)
- **í˜„ì¬ ì ìˆ˜**: {result.get('answer_relevancy', 0):.3f}
- **í•´ì„**: {self._interpret_answer_relevancy(result.get('answer_relevancy', 0))}
{self._format_metric_analysis('answer_relevancy', stats.get('answer_relevancy', {}))}

### 3. Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨)
- **í˜„ì¬ ì ìˆ˜**: {result.get('context_recall', 0):.3f}
- **í•´ì„**: {self._interpret_context_recall(result.get('context_recall', 0))}
{self._format_metric_analysis('context_recall', stats.get('context_recall', {}))}

### 4. Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„)
- **í˜„ì¬ ì ìˆ˜**: {result.get('context_precision', 0):.3f}
- **í•´ì„**: {self._interpret_context_precision(result.get('context_precision', 0))}
{self._format_metric_analysis('context_precision', stats.get('context_precision', {}))}

{self._format_answer_correctness_section(result, stats)}

## ğŸ“‹ ê°œì„  ê¶Œì¥ì‚¬í•­

{self._generate_recommendations(result, stats)}

## ğŸ“Š ë°ì´í„° í’ˆì§ˆ í‰ê°€

{self._evaluate_data_quality(individual_scores)}

---

*ì´ ë³´ê³ ì„œëŠ” RAGTraceì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*  
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return content
    
    def _calculate_statistics(self, individual_scores: List[Dict]) -> Dict[str, Dict]:
        """ë©”íŠ¸ë¦­ë³„ ê¸°ì´ˆ í†µê³„ ê³„ì‚°"""
        if not individual_scores:
            return {}
        
        metrics = list(individual_scores[0].keys())
        stats = {}
        
        for metric in metrics:
            scores = [item[metric] for item in individual_scores if item[metric] is not None]
            
            if scores:
                stats[metric] = {
                    'mean': statistics.mean(scores),
                    'median': statistics.median(scores),
                    'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0.0,
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores),
                    'range': max(scores) - min(scores),
                    'cv': (statistics.stdev(scores) / statistics.mean(scores)) if len(scores) > 1 and statistics.mean(scores) > 0 else 0.0,
                }
                
                # ì‚¬ë¶„ìœ„ìˆ˜ ê³„ì‚°
                if len(scores) >= 4:
                    quantiles = statistics.quantiles(scores, n=4)
                    stats[metric]['q1'] = quantiles[0]
                    stats[metric]['q3'] = quantiles[2]
                    stats[metric]['iqr'] = quantiles[2] - quantiles[0]
                else:
                    stats[metric]['q1'] = min(scores)
                    stats[metric]['q3'] = max(scores)
                    stats[metric]['iqr'] = stats[metric]['range']
        
        return stats
    
    def _calculate_performance_grades(self, result: Dict[str, Any]) -> Dict[str, str]:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        grades = {}
        
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        if 'answer_correctness' in result:
            metrics.append('answer_correctness')
        
        for metric in metrics:
            score = result.get(metric, 0)
            if score >= 0.9:
                grades[metric] = "A+ (ìš°ìˆ˜)"
            elif score >= 0.8:
                grades[metric] = "A (ì¢‹ìŒ)"
            elif score >= 0.7:
                grades[metric] = "B (ë³´í†µ)"
            elif score >= 0.6:
                grades[metric] = "C (ê°œì„  í•„ìš”)"
            else:
                grades[metric] = "D (í¬ê²Œ ê°œì„  í•„ìš”)"
        
        # ì „ì²´ ì ìˆ˜ ë“±ê¸‰
        ragas_score = result.get('ragas_score', 0)
        if ragas_score >= 0.9:
            grades['overall'] = "A+ (ìš°ìˆ˜)"
        elif ragas_score >= 0.8:
            grades['overall'] = "A (ì¢‹ìŒ)"
        elif ragas_score >= 0.7:
            grades['overall'] = "B (ë³´í†µ)"
        elif ragas_score >= 0.6:
            grades['overall'] = "C (ê°œì„  í•„ìš”)"
        else:
            grades['overall'] = "D (í¬ê²Œ ê°œì„  í•„ìš”)"
        
        return grades
    
    def _format_metric_scores(self, result: Dict[str, Any]) -> str:
        """ë©”íŠ¸ë¦­ ì ìˆ˜ í¬ë§·íŒ…"""
        lines = []
        lines.append(f"- **Faithfulness**: {result.get('faithfulness', 0):.3f}")
        lines.append(f"- **Answer Relevancy**: {result.get('answer_relevancy', 0):.3f}")
        lines.append(f"- **Context Recall**: {result.get('context_recall', 0):.3f}")
        lines.append(f"- **Context Precision**: {result.get('context_precision', 0):.3f}")
        
        if 'answer_correctness' in result:
            lines.append(f"- **Answer Correctness**: {result.get('answer_correctness', 0):.3f}")
        
        return "\n".join(lines)
    
    def _format_performance_grades(self, grades: Dict[str, str]) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ í¬ë§·íŒ…"""
        lines = []
        lines.append(f"- **ì „ì²´ ë“±ê¸‰**: {grades.get('overall', 'N/A')}")
        lines.append(f"- **Faithfulness**: {grades.get('faithfulness', 'N/A')}")
        lines.append(f"- **Answer Relevancy**: {grades.get('answer_relevancy', 'N/A')}")
        lines.append(f"- **Context Recall**: {grades.get('context_recall', 'N/A')}")
        lines.append(f"- **Context Precision**: {grades.get('context_precision', 'N/A')}")
        
        if 'answer_correctness' in grades:
            lines.append(f"- **Answer Correctness**: {grades.get('answer_correctness', 'N/A')}")
        
        return "\n".join(lines)
    
    def _format_statistics_table(self, stats: Dict[str, Dict]) -> str:
        """í†µê³„ í…Œì´ë¸” í¬ë§·íŒ…"""
        if not stats:
            return "í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append("| ë©”íŠ¸ë¦­ | í‰ê·  | ì¤‘ì•™ê°’ | í‘œì¤€í¸ì°¨ | ìµœì†Œê°’ | ìµœëŒ€ê°’ | ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„ |")
        lines.append("|--------|------|--------|----------|--------|--------|---------------|")
        
        for metric, stat in stats.items():
            lines.append(f"| {metric} | {stat['mean']:.3f} | {stat['median']:.3f} | {stat['std_dev']:.3f} | {stat['min']:.3f} | {stat['max']:.3f} | {stat.get('iqr', 0):.3f} |")
        
        return "\n".join(lines)
    
    def _format_metric_analysis(self, metric: str, stat: Dict) -> str:
        """ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„ í¬ë§·íŒ…"""
        if not stat:
            return "- ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append(f"- **ë³€ë™ì„±**: í‘œì¤€í¸ì°¨ {stat['std_dev']:.3f} (ë³€ë™ê³„ìˆ˜: {stat['cv']:.2f})")
        lines.append(f"- **ë¶„í¬**: ìµœì†Œ {stat['min']:.3f} ~ ìµœëŒ€ {stat['max']:.3f}")
        lines.append(f"- **ì¤‘ì•™ê°’**: {stat['median']:.3f}")
        
        if stat.get('iqr', 0) > 0.1:
            lines.append("- **ì£¼ì˜**: ì ìˆ˜ ë¶„ì‚°ì´ í¬ë¯€ë¡œ ì¼ê´€ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif stat.get('iqr', 0) < 0.05:
            lines.append("- **ì–‘í˜¸**: ì ìˆ˜ê°€ ì¼ê´€ë˜ê²Œ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(lines)
    
    def _format_answer_correctness_section(self, result: Dict[str, Any], stats: Dict[str, Dict]) -> str:
        """Answer Correctness ì„¹ì…˜ í¬ë§·íŒ…"""
        if 'answer_correctness' not in result:
            return ""
        
        return f"""
### 5. Answer Correctness (ë‹µë³€ ì •í™•ë„)
- **í˜„ì¬ ì ìˆ˜**: {result.get('answer_correctness', 0):.3f}
- **í•´ì„**: {self._interpret_answer_correctness(result.get('answer_correctness', 0))}
{self._format_metric_analysis('answer_correctness', stats.get('answer_correctness', {}))}
"""
    
    def _interpret_faithfulness(self, score: float) -> str:
        """Faithfulness ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë§¤ìš° ì¶©ì‹¤í•˜ë©°, í™˜ê°(hallucination)ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤."
        elif score >= 0.8:
            return "ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€ì²´ë¡œ ì¶©ì‹¤í•˜ì§€ë§Œ, ì¼ë¶€ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.7:
            return "ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì–´ëŠ ì •ë„ ì¶©ì‹¤í•˜ì§€ë§Œ, ì •í™•ì„± í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ë‹µë³€ì— ì»¨í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì–´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ë‹µë³€ì— í™˜ê°ì´ë‚˜ ë¶€ì •í™•í•œ ë‚´ìš©ì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_answer_relevancy(self, score: float) -> str:
        """Answer Relevancy ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ê³  ì ì ˆí•©ë‹ˆë‹¤."
        elif score >= 0.8:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë†’ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–´ëŠ ì •ë„ ê´€ë ¨ì´ ìˆì§€ë§Œ ë” êµ¬ì²´ì ì¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.6:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ë ¨ì´ ìˆì–´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_context_recall(self, score: float) -> str:
        """Context Recall ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ê´€ë ¨ ì •ë³´ê°€ ê±°ì˜ ì™„ë²½í•˜ê²Œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        elif score >= 0.8:
            return "ê´€ë ¨ ì •ë³´ê°€ ì˜ ê²€ìƒ‰ë˜ì—ˆì§€ë§Œ ì¼ë¶€ ëˆ„ë½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.7:
            return "ê´€ë ¨ ì •ë³´ê°€ ì–´ëŠ ì •ë„ ê²€ìƒ‰ë˜ì—ˆì§€ë§Œ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ì— ìƒë‹¹í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì„±ëŠ¥ì´ í¬ê²Œ ë¶€ì¡±í•˜ì—¬ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_context_precision(self, score: float) -> str:
        """Context Precision ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ë§¤ìš° ì •í™•í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        elif score >= 0.8:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ëŒ€ì²´ë¡œ ì •í™•í•˜ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ì„±ì´ ë–¨ì–´ì§€ëŠ” ë‚´ìš©ì´ ì¼ë¶€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.6:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ì„± í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ì„±ì´ ë‚®ì€ ë‚´ìš©ì´ ë§ì•„ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_answer_correctness(self, score: float) -> str:
        """Answer Correctness ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ë‹µë³€ì´ ì •ë‹µê³¼ ë§¤ìš° ì¼ì¹˜í•˜ë©° ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì…ë‹ˆë‹¤."
        elif score >= 0.8:
            return "ë‹µë³€ì´ ì •ë‹µê³¼ ëŒ€ì²´ë¡œ ì¼ì¹˜í•˜ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ë‹µë³€ì´ ì •ë‹µê³¼ ì–´ëŠ ì •ë„ ì¼ì¹˜í•˜ì§€ë§Œ ì •í™•ë„ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ë‹µë³€ê³¼ ì •ë‹µ ê°„ì˜ ì¼ì¹˜ë„ê°€ ë‚®ì•„ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ë‹µë³€ì˜ ì •í™•ë„ê°€ í¬ê²Œ ë¶€ì¡±í•˜ì—¬ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _generate_recommendations(self, result: Dict[str, Any], stats: Dict[str, Dict]) -> str:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if result.get('faithfulness', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Faithfulness ê°œì„ **: ë‹µë³€ ìƒì„± ì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë” ì¶©ì‹¤í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
        
        if result.get('answer_relevancy', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Answer Relevancy ê°œì„ **: ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ë” ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.")
        
        if result.get('context_recall', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Context Recall ê°œì„ **: ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê°œì„ í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ë” ì™„ì „íˆ ê²€ìƒ‰í•˜ì„¸ìš”.")
        
        if result.get('context_precision', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Context Precision ê°œì„ **: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê³  ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ì„¸ìš”.")
        
        if 'answer_correctness' in result and result.get('answer_correctness', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Answer Correctness ê°œì„ **: ëª¨ë¸ í›ˆë ¨ ë°ì´í„°ë‚˜ fine-tuningì„ í†µí•´ ë‹µë³€ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ì„¸ìš”.")
        
        # ë³€ë™ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        for metric, stat in stats.items():
            if stat.get('cv', 0) > 0.3:  # ë³€ë™ê³„ìˆ˜ê°€ 30% ì´ìƒ
                recommendations.append(f"ğŸ“Š **{metric} ì¼ê´€ì„± ê°œì„ **: ì ìˆ˜ ë³€ë™ì´ í¬ë¯€ë¡œ ë” ì¼ê´€ëœ ì„±ëŠ¥ì„ ìœ„í•œ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("ğŸ‰ **ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥**: í˜„ì¬ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©° ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return "\n".join(recommendations)
    
    def _evaluate_data_quality(self, individual_scores: List[Dict]) -> str:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        if not individual_scores:
            return "í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append(f"- **ì´ í‰ê°€ í•­ëª© ìˆ˜**: {len(individual_scores)}ê°œ")
        
        # ì™„ì „í•œ ì ìˆ˜ë¥¼ ê°€ì§„ í•­ëª© ìˆ˜ í™•ì¸
        complete_items = sum(1 for item in individual_scores if all(v is not None for v in item.values()))
        lines.append(f"- **ì™„ì „í•œ í‰ê°€ í•­ëª©**: {complete_items}ê°œ ({complete_items/len(individual_scores)*100:.1f}%)")
        
        # ëˆ„ë½ëœ ì ìˆ˜ í™•ì¸
        missing_counts = {}
        for item in individual_scores:
            for metric, score in item.items():
                if score is None:
                    missing_counts[metric] = missing_counts.get(metric, 0) + 1
        
        if missing_counts:
            lines.append("- **ëˆ„ë½ëœ ë©”íŠ¸ë¦­**:")
            for metric, count in missing_counts.items():
                lines.append(f"  - {metric}: {count}ê°œ í•­ëª© ({count/len(individual_scores)*100:.1f}%)")
        else:
            lines.append("- **ë°ì´í„° ì™„ì„±ë„**: ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì™„ì „íˆ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(lines)
    
    def export_full_package(self, result: Dict[str, Any], base_filename: Optional[str] = None) -> Dict[str, str]:
        """ì „ì²´ íŒ¨í‚¤ì§€ ë‚´ë³´ë‚´ê¸° (CSV + ìš”ì•½ + ë³´ê³ ì„œ)"""
        if base_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"ragas_evaluation_{timestamp}"
        
        files = {}
        
        # ìƒì„¸ CSV
        files['detailed_csv'] = self.export_to_csv(result, f"{base_filename}_detailed.csv")
        
        # ìš”ì•½ CSV  
        files['summary_csv'] = self.export_summary_csv(result, f"{base_filename}_summary.csv")
        
        # ë¶„ì„ ë³´ê³ ì„œ
        files['analysis_report'] = self.generate_analysis_report(result, f"{base_filename}_analysis.md")
        
        return files