"""
ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤

í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ê³¼ ë¶„ì„ ë³´ê³ ì„œë¡œ ë‚´ë³´ë‚´ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import csv
import json
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd


class ResultExporter:
    """í‰ê°€ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤"""
    
    def __init__(self, output_dir: str = "exports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def export_to_csv(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_evaluation_{timestamp}.csv"
        
        csv_path = self.output_dir / filename
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´
        metadata = result.get("metadata", {})
        
        # ê°œë³„ ì ìˆ˜ ë°ì´í„° ì¤€ë¹„
        individual_scores = result.get("individual_scores", [])
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            # CSV í—¤ë” ì •ì˜
            fieldnames = [
                'item_id', 'faithfulness', 'answer_relevancy', 
                'context_recall', 'context_precision'
            ]
            
            # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if individual_scores and 'answer_correctness' in individual_scores[0]:
                fieldnames.append('answer_correctness')
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            # ê°œë³„ ì ìˆ˜ ì‘ì„±
            for i, scores in enumerate(individual_scores):
                row = {'item_id': i + 1}
                row.update(scores)
                writer.writerow(row)
        
        return str(csv_path)
    
    def export_summary_csv(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ìš”ì•½ í†µê³„ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_summary_{timestamp}.csv"
        
        csv_path = self.output_dir / filename
        
        summary_data = []
        
        # ë©”íŠ¸ë¦­ë³„ í†µê³„ ê³„ì‚°
        individual_scores = result.get("individual_scores", [])
        
        if individual_scores:
            # individual_scoresê°€ ìˆëŠ” ê²½ìš°: ìƒì„¸ í†µê³„ ê³„ì‚°
            metrics = list(individual_scores[0].keys())
            
            for metric in metrics:
                scores = [item[metric] for item in individual_scores if item[metric] is not None]
                
                if scores:
                    summary_data.append({
                        'metric': metric,
                        'mean': statistics.mean(scores),
                        'median': statistics.median(scores),
                        'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0.0,
                        'min': min(scores),
                        'max': max(scores),
                        'count': len(scores),
                        'q1': statistics.quantiles(scores, n=4)[0] if len(scores) >= 4 else min(scores),
                        'q3': statistics.quantiles(scores, n=4)[2] if len(scores) >= 4 else max(scores),
                    })
        else:
            # individual_scoresê°€ ì—†ëŠ” ê²½ìš°: ì „ì²´ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë‹¨ì¼ ê°’ í†µê³„ ìƒì„±
            metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
            
            # answer_correctnessê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if 'answer_correctness' in result:
                metrics.append('answer_correctness')
            
            for metric in metrics:
                value = result.get(metric, 0)
                if value is not None:
                    summary_data.append({
                        'metric': metric,
                        'mean': value,
                        'median': value,
                        'std_dev': 0.0,
                        'min': value,
                        'max': value,
                        'count': 1,
                        'q1': value,
                        'q3': value,
                    })
        
        # ì „ì²´ ì ìˆ˜ ì¶”ê°€
        ragas_score = result.get('ragas_score', 0)
        if ragas_score is not None:
            summary_data.append({
                'metric': 'ragas_score',
                'mean': ragas_score,
                'median': ragas_score,
                'std_dev': 0.0,
                'min': ragas_score,
                'max': ragas_score,
                'count': 1,
                'q1': ragas_score,
                'q3': ragas_score,
            })
        
        # CSV ì‘ì„±
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['metric', 'mean', 'median', 'std_dev', 'min', 'max', 'count', 'q1', 'q3']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(summary_data)
        
        return str(csv_path)
    
    def generate_analysis_report(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_analysis_report_{timestamp}.md"
        
        report_path = self.output_dir / filename
        
        metadata = result.get("metadata", {})
        individual_scores = result.get("individual_scores", [])
        
        # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
        report_content = self._generate_report_content(result, metadata, individual_scores)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return str(report_path)
    
    def _generate_report_content(self, result: Dict[str, Any], metadata: Dict[str, Any], individual_scores: List[Dict]) -> str:
        """ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""
        
        # ê¸°ì´ˆ í†µê³„ ê³„ì‚°
        stats = self._calculate_statistics(individual_scores)
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        performance_grades = self._calculate_performance_grades(result)
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ì¶œ ë° ì •ë¦¬
        evaluation_id = self._extract_evaluation_id(metadata)
        timestamp = self._extract_timestamp(metadata)
        llm_model = self._extract_llm_model(metadata)
        dataset_size = self._extract_dataset_size(metadata, individual_scores)
        duration_info = self._extract_duration_info(metadata, individual_scores)
        
        # ë³´ê³ ì„œ í…œí”Œë¦¿
        content = f"""# RAGTrace í‰ê°€ ê²°ê³¼ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“Š í‰ê°€ ê°œìš”

**í‰ê°€ ID**: {evaluation_id}  
**í‰ê°€ ì¼ì‹œ**: {timestamp}  
**LLM ëª¨ë¸**: {llm_model}  
**ì„ë² ë”© ëª¨ë¸**: {self._extract_embedding_model(metadata)}  
**ë°ì´í„°ì…‹**: {metadata.get('dataset', 'N/A')}  
**ë°ì´í„°ì…‹ í¬ê¸°**: {dataset_size}ê°œ ë¬¸í•­  
**ì´ í‰ê°€ ì‹œê°„**: {duration_info['total_minutes']:.1f}ë¶„  
**í‰ê·  ë¬¸í•­ë‹¹ ì‹œê°„**: {duration_info['avg_seconds']:.1f}ì´ˆ  

## ğŸ¯ ì „ì²´ ì„±ëŠ¥ ìš”ì•½

**RAGAS ì¢…í•© ì ìˆ˜**: {result.get('ragas_score', 0):.3f} / 1.000

### ë©”íŠ¸ë¦­ë³„ ì ìˆ˜
{self._format_metric_scores(result)}

### ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
{self._format_performance_grades(performance_grades)}

## ğŸ“ˆ ê¸°ì´ˆ í†µê³„ ë¶„ì„

{self._format_statistics_table(stats)}

## ğŸ” ìƒì„¸ ë¶„ì„

### 1. Faithfulness (ë‹µë³€ ì¶©ì‹¤ë„)
- **í˜„ì¬ ì ìˆ˜**: {result.get('faithfulness', 0):.3f}
- **í•´ì„**: {self._interpret_faithfulness(result.get('faithfulness', 0))}
{self._format_metric_analysis('faithfulness', stats.get('faithfulness', {}))}

### 2. Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)
- **í˜„ì¬ ì ìˆ˜**: {result.get('answer_relevancy', 0):.3f}
- **í•´ì„**: {self._interpret_answer_relevancy(result.get('answer_relevancy', 0))}
{self._format_metric_analysis('answer_relevancy', stats.get('answer_relevancy', {}))}

### 3. Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨)
- **í˜„ì¬ ì ìˆ˜**: {result.get('context_recall', 0):.3f}
- **í•´ì„**: {self._interpret_context_recall(result.get('context_recall', 0))}
{self._format_metric_analysis('context_recall', stats.get('context_recall', {}))}

### 4. Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„)
- **í˜„ì¬ ì ìˆ˜**: {result.get('context_precision', 0):.3f}
- **í•´ì„**: {self._interpret_context_precision(result.get('context_precision', 0))}
{self._format_metric_analysis('context_precision', stats.get('context_precision', {}))}

{self._format_answer_correctness_section(result, stats)}

## ğŸ“‹ ê°œì„  ê¶Œì¥ì‚¬í•­

{self._generate_recommendations(result, stats)}

## ğŸ“Š ë°ì´í„° í’ˆì§ˆ í‰ê°€

{self._evaluate_data_quality(individual_scores)}

---

*ì´ ë³´ê³ ì„œëŠ” RAGTraceì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*  
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return content
    
    def _calculate_statistics(self, individual_scores: List[Dict]) -> Dict[str, Dict]:
        """ë©”íŠ¸ë¦­ë³„ ê¸°ì´ˆ í†µê³„ ê³„ì‚°"""
        if not individual_scores:
            return {}
        
        metrics = list(individual_scores[0].keys())
        stats = {}
        
        for metric in metrics:
            scores = [item[metric] for item in individual_scores if item[metric] is not None]
            
            if scores:
                stats[metric] = {
                    'mean': statistics.mean(scores),
                    'median': statistics.median(scores),
                    'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0.0,
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores),
                    'range': max(scores) - min(scores),
                    'cv': (statistics.stdev(scores) / statistics.mean(scores)) if len(scores) > 1 and statistics.mean(scores) > 0 else 0.0,
                }
                
                # ì‚¬ë¶„ìœ„ìˆ˜ ê³„ì‚°
                if len(scores) >= 4:
                    quantiles = statistics.quantiles(scores, n=4)
                    stats[metric]['q1'] = quantiles[0]
                    stats[metric]['q3'] = quantiles[2]
                    stats[metric]['iqr'] = quantiles[2] - quantiles[0]
                else:
                    stats[metric]['q1'] = min(scores)
                    stats[metric]['q3'] = max(scores)
                    stats[metric]['iqr'] = stats[metric]['range']
        
        return stats
    
    def _calculate_performance_grades(self, result: Dict[str, Any]) -> Dict[str, str]:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        grades = {}
        
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        if 'answer_correctness' in result:
            metrics.append('answer_correctness')
        
        for metric in metrics:
            score = result.get(metric, 0)
            if score >= 0.9:
                grades[metric] = "A+ (ìš°ìˆ˜)"
            elif score >= 0.8:
                grades[metric] = "A (ì¢‹ìŒ)"
            elif score >= 0.7:
                grades[metric] = "B (ë³´í†µ)"
            elif score >= 0.6:
                grades[metric] = "C (ê°œì„  í•„ìš”)"
            else:
                grades[metric] = "D (í¬ê²Œ ê°œì„  í•„ìš”)"
        
        # ì „ì²´ ì ìˆ˜ ë“±ê¸‰
        ragas_score = result.get('ragas_score', 0)
        if ragas_score >= 0.9:
            grades['overall'] = "A+ (ìš°ìˆ˜)"
        elif ragas_score >= 0.8:
            grades['overall'] = "A (ì¢‹ìŒ)"
        elif ragas_score >= 0.7:
            grades['overall'] = "B (ë³´í†µ)"
        elif ragas_score >= 0.6:
            grades['overall'] = "C (ê°œì„  í•„ìš”)"
        else:
            grades['overall'] = "D (í¬ê²Œ ê°œì„  í•„ìš”)"
        
        return grades
    
    def _format_metric_scores(self, result: Dict[str, Any]) -> str:
        """ë©”íŠ¸ë¦­ ì ìˆ˜ í¬ë§·íŒ…"""
        lines = []
        lines.append(f"- **Faithfulness**: {result.get('faithfulness', 0):.3f}")
        lines.append(f"- **Answer Relevancy**: {result.get('answer_relevancy', 0):.3f}")
        lines.append(f"- **Context Recall**: {result.get('context_recall', 0):.3f}")
        lines.append(f"- **Context Precision**: {result.get('context_precision', 0):.3f}")
        
        if 'answer_correctness' in result:
            lines.append(f"- **Answer Correctness**: {result.get('answer_correctness', 0):.3f}")
        
        return "\n".join(lines)
    
    def _format_performance_grades(self, grades: Dict[str, str]) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ í¬ë§·íŒ…"""
        lines = []
        lines.append(f"- **ì „ì²´ ë“±ê¸‰**: {grades.get('overall', 'N/A')}")
        lines.append(f"- **Faithfulness**: {grades.get('faithfulness', 'N/A')}")
        lines.append(f"- **Answer Relevancy**: {grades.get('answer_relevancy', 'N/A')}")
        lines.append(f"- **Context Recall**: {grades.get('context_recall', 'N/A')}")
        lines.append(f"- **Context Precision**: {grades.get('context_precision', 'N/A')}")
        
        if 'answer_correctness' in grades:
            lines.append(f"- **Answer Correctness**: {grades.get('answer_correctness', 'N/A')}")
        
        return "\n".join(lines)
    
    def _format_statistics_table(self, stats: Dict[str, Dict]) -> str:
        """í†µê³„ í…Œì´ë¸” í¬ë§·íŒ…"""
        if not stats:
            return "í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append("| ë©”íŠ¸ë¦­ | í‰ê·  | ì¤‘ì•™ê°’ | í‘œì¤€í¸ì°¨ | ìµœì†Œê°’ | ìµœëŒ€ê°’ | ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„ |")
        lines.append("|--------|------|--------|----------|--------|--------|---------------|")
        
        for metric, stat in stats.items():
            lines.append(f"| {metric} | {stat['mean']:.3f} | {stat['median']:.3f} | {stat['std_dev']:.3f} | {stat['min']:.3f} | {stat['max']:.3f} | {stat.get('iqr', 0):.3f} |")
        
        return "\n".join(lines)
    
    def _format_metric_analysis(self, metric: str, stat: Dict) -> str:
        """ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„ í¬ë§·íŒ…"""
        if not stat:
            return "- ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append(f"- **ë³€ë™ì„±**: í‘œì¤€í¸ì°¨ {stat['std_dev']:.3f} (ë³€ë™ê³„ìˆ˜: {stat['cv']:.2f})")
        lines.append(f"- **ë¶„í¬**: ìµœì†Œ {stat['min']:.3f} ~ ìµœëŒ€ {stat['max']:.3f}")
        lines.append(f"- **ì¤‘ì•™ê°’**: {stat['median']:.3f}")
        
        if stat.get('iqr', 0) > 0.1:
            lines.append("- **ì£¼ì˜**: ì ìˆ˜ ë¶„ì‚°ì´ í¬ë¯€ë¡œ ì¼ê´€ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif stat.get('iqr', 0) < 0.05:
            lines.append("- **ì–‘í˜¸**: ì ìˆ˜ê°€ ì¼ê´€ë˜ê²Œ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(lines)
    
    def _format_answer_correctness_section(self, result: Dict[str, Any], stats: Dict[str, Dict]) -> str:
        """Answer Correctness ì„¹ì…˜ í¬ë§·íŒ…"""
        if 'answer_correctness' not in result:
            return ""
        
        return f"""
### 5. Answer Correctness (ë‹µë³€ ì •í™•ë„)
- **í˜„ì¬ ì ìˆ˜**: {result.get('answer_correctness', 0):.3f}
- **í•´ì„**: {self._interpret_answer_correctness(result.get('answer_correctness', 0))}
{self._format_metric_analysis('answer_correctness', stats.get('answer_correctness', {}))}
"""
    
    def _interpret_faithfulness(self, score: float) -> str:
        """Faithfulness ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë§¤ìš° ì¶©ì‹¤í•˜ë©°, í™˜ê°(hallucination)ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤."
        elif score >= 0.8:
            return "ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€ì²´ë¡œ ì¶©ì‹¤í•˜ì§€ë§Œ, ì¼ë¶€ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.7:
            return "ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì–´ëŠ ì •ë„ ì¶©ì‹¤í•˜ì§€ë§Œ, ì •í™•ì„± í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ë‹µë³€ì— ì»¨í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì–´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ë‹µë³€ì— í™˜ê°ì´ë‚˜ ë¶€ì •í™•í•œ ë‚´ìš©ì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_answer_relevancy(self, score: float) -> str:
        """Answer Relevancy ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ê³  ì ì ˆí•©ë‹ˆë‹¤."
        elif score >= 0.8:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë†’ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–´ëŠ ì •ë„ ê´€ë ¨ì´ ìˆì§€ë§Œ ë” êµ¬ì²´ì ì¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.6:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ë ¨ì´ ìˆì–´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_context_recall(self, score: float) -> str:
        """Context Recall ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ê´€ë ¨ ì •ë³´ê°€ ê±°ì˜ ì™„ë²½í•˜ê²Œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        elif score >= 0.8:
            return "ê´€ë ¨ ì •ë³´ê°€ ì˜ ê²€ìƒ‰ë˜ì—ˆì§€ë§Œ ì¼ë¶€ ëˆ„ë½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.7:
            return "ê´€ë ¨ ì •ë³´ê°€ ì–´ëŠ ì •ë„ ê²€ìƒ‰ë˜ì—ˆì§€ë§Œ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ì— ìƒë‹¹í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì„±ëŠ¥ì´ í¬ê²Œ ë¶€ì¡±í•˜ì—¬ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_context_precision(self, score: float) -> str:
        """Context Precision ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ë§¤ìš° ì •í™•í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        elif score >= 0.8:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ëŒ€ì²´ë¡œ ì •í™•í•˜ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ì„±ì´ ë–¨ì–´ì§€ëŠ” ë‚´ìš©ì´ ì¼ë¶€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        elif score >= 0.6:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ì„± í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ì„±ì´ ë‚®ì€ ë‚´ìš©ì´ ë§ì•„ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _interpret_answer_correctness(self, score: float) -> str:
        """Answer Correctness ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ë‹µë³€ì´ ì •ë‹µê³¼ ë§¤ìš° ì¼ì¹˜í•˜ë©° ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì…ë‹ˆë‹¤."
        elif score >= 0.8:
            return "ë‹µë³€ì´ ì •ë‹µê³¼ ëŒ€ì²´ë¡œ ì¼ì¹˜í•˜ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ë‹µë³€ì´ ì •ë‹µê³¼ ì–´ëŠ ì •ë„ ì¼ì¹˜í•˜ì§€ë§Œ ì •í™•ë„ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ë‹µë³€ê³¼ ì •ë‹µ ê°„ì˜ ì¼ì¹˜ë„ê°€ ë‚®ì•„ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "ë‹µë³€ì˜ ì •í™•ë„ê°€ í¬ê²Œ ë¶€ì¡±í•˜ì—¬ ì‹œê¸‰í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    def _generate_recommendations(self, result: Dict[str, Any], stats: Dict[str, Dict]) -> str:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if result.get('faithfulness', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Faithfulness ê°œì„ **: ë‹µë³€ ìƒì„± ì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë” ì¶©ì‹¤í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
        
        if result.get('answer_relevancy', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Answer Relevancy ê°œì„ **: ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ë” ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.")
        
        if result.get('context_recall', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Context Recall ê°œì„ **: ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê°œì„ í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ë” ì™„ì „íˆ ê²€ìƒ‰í•˜ì„¸ìš”.")
        
        if result.get('context_precision', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Context Precision ê°œì„ **: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê³  ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ì„¸ìš”.")
        
        if 'answer_correctness' in result and result.get('answer_correctness', 0) < 0.7:
            recommendations.append("ğŸ“Œ **Answer Correctness ê°œì„ **: ëª¨ë¸ í›ˆë ¨ ë°ì´í„°ë‚˜ fine-tuningì„ í†µí•´ ë‹µë³€ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ì„¸ìš”.")
        
        # ë³€ë™ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        for metric, stat in stats.items():
            if stat.get('cv', 0) > 0.3:  # ë³€ë™ê³„ìˆ˜ê°€ 30% ì´ìƒ
                recommendations.append(f"ğŸ“Š **{metric} ì¼ê´€ì„± ê°œì„ **: ì ìˆ˜ ë³€ë™ì´ í¬ë¯€ë¡œ ë” ì¼ê´€ëœ ì„±ëŠ¥ì„ ìœ„í•œ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("ğŸ‰ **ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥**: í˜„ì¬ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©° ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return "\n".join(recommendations)
    
    def _evaluate_data_quality(self, individual_scores: List[Dict]) -> str:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        if not individual_scores:
            return "í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append(f"- **ì´ í‰ê°€ í•­ëª© ìˆ˜**: {len(individual_scores)}ê°œ")
        
        # ì™„ì „í•œ ì ìˆ˜ë¥¼ ê°€ì§„ í•­ëª© ìˆ˜ í™•ì¸
        complete_items = sum(1 for item in individual_scores if all(v is not None for v in item.values()))
        lines.append(f"- **ì™„ì „í•œ í‰ê°€ í•­ëª©**: {complete_items}ê°œ ({complete_items/len(individual_scores)*100:.1f}%)")
        
        # ëˆ„ë½ëœ ì ìˆ˜ í™•ì¸
        missing_counts = {}
        for item in individual_scores:
            for metric, score in item.items():
                if score is None:
                    missing_counts[metric] = missing_counts.get(metric, 0) + 1
        
        if missing_counts:
            lines.append("- **ëˆ„ë½ëœ ë©”íŠ¸ë¦­**:")
            for metric, count in missing_counts.items():
                lines.append(f"  - {metric}: {count}ê°œ í•­ëª© ({count/len(individual_scores)*100:.1f}%)")
        else:
            lines.append("- **ë°ì´í„° ì™„ì„±ë„**: ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì™„ì „íˆ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(lines)
    
    def _extract_evaluation_id(self, metadata: Dict[str, Any]) -> str:
        """í‰ê°€ ID ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ì—ì„œ í‰ê°€ ID ì°¾ê¸°
        possible_keys = ['evaluation_id', 'id', 'eval_id']
        for key in possible_keys:
            if key in metadata and metadata[key]:
                return str(metadata[key])
        
        # íŒŒì¼ëª…ì—ì„œ ID ì¶”ì¶œ ì‹œë„
        if 'filename' in metadata:
            filename = metadata['filename']
            if '_' in filename:
                parts = filename.split('_')
                if len(parts) >= 2:
                    return parts[1]  # eval_xxxxx_... í˜•íƒœì—ì„œ ì¶”ì¶œ
        
        # ìƒì„±ì¼ì‹œ ê¸°ë°˜ ID ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"eval_{timestamp}"
    
    def _extract_timestamp(self, metadata: Dict[str, Any]) -> str:
        """í‰ê°€ ì¼ì‹œ ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì°¾ê¸°
        possible_keys = ['timestamp', 'created_at', 'evaluation_time', 'start_time']
        for key in possible_keys:
            if key in metadata and metadata[key]:
                # ì´ë¯¸ í¬ë§·ëœ ë¬¸ìì—´ì¸ ê²½ìš°
                if isinstance(metadata[key], str):
                    return metadata[key]
                # datetime ê°ì²´ì¸ ê²½ìš°
                elif hasattr(metadata[key], 'strftime'):
                    return metadata[key].strftime('%Y-%m-%d %H:%M:%S')
        
        # í˜„ì¬ ì‹œê°„ ë°˜í™˜
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def _extract_llm_model(self, metadata: Dict[str, Any]) -> str:
        """LLM ëª¨ë¸ ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ì—ì„œ LLM ëª¨ë¸ ì°¾ê¸°
        possible_keys = ['llm_type', 'model', 'llm_model', 'llm']
        for key in possible_keys:
            if key in metadata and metadata[key]:
                model_name = str(metadata[key]).upper()
                # ëª¨ë¸ ì´ë¦„ ì •ê·œí™”
                if model_name in ['GEMINI', 'GEMINI-2.5-FLASH', 'GEMINI-2.5-FLASH-PREVIEW-05-20']:
                    return 'Google Gemini 2.5 Flash'
                elif model_name in ['HCX', 'HCX-005']:
                    return 'Naver HCX-005'
                return model_name
        
        return 'N/A'
    
    def _extract_embedding_model(self, metadata: Dict[str, Any]) -> str:
        """ì„ë² ë”© ëª¨ë¸ ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ì—ì„œ ì„ë² ë”© ëª¨ë¸ ì°¾ê¸°
        possible_keys = ['embedding_type', 'embedding_model', 'embedding']
        for key in possible_keys:
            if key in metadata and metadata[key]:
                model_name = str(metadata[key]).upper()
                # ëª¨ë¸ ì´ë¦„ ì •ê·œí™”
                if model_name in ['GEMINI', 'GEMINI-EMBEDDING-EXP-03-07']:
                    return 'Google Gemini Embedding'
                elif model_name in ['HCX', 'HCX-EMBEDDING']:
                    return 'Naver HCX Embedding'
                elif model_name in ['BGE_M3', 'BGE-M3']:
                    return 'BGE-M3 (ë¡œì»¬)'
                return model_name
        
        return 'N/A'
    
    def _extract_dataset_size(self, metadata: Dict[str, Any], individual_scores: List[Dict]) -> int:
        """ë°ì´í„°ì…‹ í¬ê¸° ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í¬ê¸° ì •ë³´ ì°¾ê¸°
        possible_keys = ['dataset_size', 'total_items', 'item_count', 'size']
        for key in possible_keys:
            if key in metadata and isinstance(metadata[key], (int, float)):
                return int(metadata[key])
        
        # individual_scoresì—ì„œ í¬ê¸° ê³„ì‚°
        if individual_scores:
            return len(individual_scores)
        
        # QA ë°ì´í„°ì—ì„œ í¬ê¸° ê³„ì‚°
        if 'qa_data' in metadata:
            qa_data = metadata['qa_data']
            if isinstance(qa_data, list):
                return len(qa_data)
        
        return 0
    
    def _extract_duration_info(self, metadata: Dict[str, Any], individual_scores: List[Dict]) -> Dict[str, float]:
        """í‰ê°€ ì‹œê°„ ì •ë³´ ì¶”ì¶œ"""
        # ê¸°ë³¸ê°’
        duration_info = {
            'total_minutes': 0.0,
            'avg_seconds': 0.0
        }
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹œê°„ ì •ë³´ ì°¾ê¸°
        total_duration = None
        possible_duration_keys = ['total_duration_minutes', 'duration_minutes', 'elapsed_time']
        for key in possible_duration_keys:
            if key in metadata and isinstance(metadata[key], (int, float)):
                total_duration = float(metadata[key])
                break
        
        # start_timeê³¼ end_timeì—ì„œ ê³„ì‚°
        if total_duration is None:
            start_time = metadata.get('start_time')
            end_time = metadata.get('end_time')
            if start_time and end_time:
                try:
                    if isinstance(start_time, str):
                        start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                    if isinstance(end_time, str):
                        end_time = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                    
                    duration = end_time - start_time
                    total_duration = duration.total_seconds() / 60.0  # ë¶„ ë‹¨ìœ„
                except:
                    pass
        
        # ê°œë³„ í•­ëª© ì‹œê°„ì—ì„œ ê³„ì‚°
        if total_duration is None and individual_scores:
            # í‰ê·  ì¶”ì • ì‹œê°„ ì‚¬ìš© (í•­ëª©ë‹¹ 8ì´ˆ ê°€ì •)
            estimated_seconds_per_item = 8.0
            total_duration = len(individual_scores) * estimated_seconds_per_item / 60.0
        
        if total_duration is not None:
            duration_info['total_minutes'] = total_duration
            dataset_size = self._extract_dataset_size(metadata, individual_scores)
            if dataset_size > 0:
                duration_info['avg_seconds'] = (total_duration * 60.0) / dataset_size
        
        return duration_info
    
    def export_full_package(self, result: Dict[str, Any], base_filename: Optional[str] = None) -> Dict[str, str]:
        """ì „ì²´ íŒ¨í‚¤ì§€ ë‚´ë³´ë‚´ê¸° (CSV + ìš”ì•½ + ë³´ê³ ì„œ)"""
        if base_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"ragas_evaluation_{timestamp}"
        
        files = {}
        
        # ìƒì„¸ CSV
        files['detailed_csv'] = self.export_to_csv(result, f"{base_filename}_detailed.csv")
        
        # ìš”ì•½ CSV  
        files['summary_csv'] = self.export_summary_csv(result, f"{base_filename}_summary.csv")
        
        # ë¶„ì„ ë³´ê³ ì„œ
        files['analysis_report'] = self.generate_analysis_report(result, f"{base_filename}_analysis.md")
        
        return files