{
  "faithfulness": 0.8732500000000001,
  "answer_relevancy": 0.8254999999999999,
  "context_recall": 0.8135,
  "context_precision": 0.861375,
  "ragas_score": 0.8434062499999999,
  "generation_failures": 0,
  "generation_successes": 8,
  "individual_scores": [
    {
      "faithfulness": 0.897,
      "answer_relevancy": 0.839,
      "context_recall": 0.781,
      "context_precision": 0.905
    },
    {
      "faithfulness": 0.823,
      "answer_relevancy": 0.784,
      "context_recall": 0.832,
      "context_precision": 0.837
    },
    {
      "faithfulness": 0.853,
      "answer_relevancy": 0.811,
      "context_recall": 0.863,
      "context_precision": 0.81
    },
    {
      "faithfulness": 0.847,
      "answer_relevancy": 0.851,
      "context_recall": 0.751,
      "context_precision": 0.811
    },
    {
      "faithfulness": 0.908,
      "answer_relevancy": 0.784,
      "context_recall": 0.863,
      "context_precision": 0.893
    },
    {
      "faithfulness": 0.901,
      "answer_relevancy": 0.808,
      "context_recall": 0.848,
      "context_precision": 0.866
    },
    {
      "faithfulness": 0.927,
      "answer_relevancy": 0.871,
      "context_recall": 0.798,
      "context_precision": 0.889
    },
    {
      "faithfulness": 0.83,
      "answer_relevancy": 0.856,
      "context_recall": 0.772,
      "context_precision": 0.88
    }
  ],
  "metadata": {
    "evaluation_id": "b464bc49",
    "timestamp": "2025-06-21T16:12:35.202715",
    "model": "models/gemini-2.5-flash-preview-05-20",
    "temperature": 0.0,
    "dataset_size": 8
  }
}