"""
ì›¹ ëŒ€ì‹œë³´ë“œ ë©”ì¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
Streamlit ì•±ì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸
"""

import pytest
import streamlit as st
from unittest.mock import Mock, patch, MagicMock
import pandas as pd
import sys
from pathlib import Path

# ì„ íƒì  ì˜ì¡´ì„± ì²´í¬
try:
    import plotly.graph_objects as go
    HAS_PLOTLY = True
except ImportError:
    HAS_PLOTLY = False
    go = None

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ sys.pathì— ìˆëŠ”ì§€ í™•ì¸
try:
    from src.domain.entities.evaluation_result import EvaluationResult
    from src.domain.entities.evaluation_data import EvaluationData
    HAS_SRC = True
except ImportError:
    HAS_SRC = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ì•„ ê²½ë¡œ ì¶”ê°€
def add_project_root_to_path():
    current_path = Path(__file__).resolve()
    while current_path != current_path.parent:
        if (current_path / 'pyproject.toml').exists():
            sys.path.insert(0, str(current_path))
            return current_path
        current_path = current_path.parent
    raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

add_project_root_to_path()

# ë‹¤ì‹œ ì‹œë„í•´ì„œ ì„í¬íŠ¸
if not HAS_SRC:
    try:
        from src.domain.entities.evaluation_result import EvaluationResult
        from src.domain.entities.evaluation_data import EvaluationData
        HAS_SRC = True
    except ImportError:
        EvaluationResult = None
        EvaluationData = None


@pytest.fixture
def sample_evaluation_result():
    """í…ŒìŠ¤íŠ¸ìš© í‰ê°€ ê²°ê³¼ ìƒ˜í”Œ"""
    if not HAS_SRC or EvaluationResult is None:
        pytest.skip("src modules not available")
    return EvaluationResult(
        faithfulness=0.85,
        answer_relevancy=0.78,
        context_recall=0.82,
        context_precision=0.91,
        ragas_score=0.84
    )


@pytest.fixture  
def sample_evaluation_data():
    """í…ŒìŠ¤íŠ¸ìš© í‰ê°€ ë°ì´í„° ìƒ˜í”Œ"""
    if not HAS_SRC or EvaluationData is None:
        pytest.skip("src modules not available")
    return [
        EvaluationData(
            question="í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
            contexts=["ì„œìš¸ì€ í•œêµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤."],
            answer="í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.",
            ground_truth="ì„œìš¸"
        )
    ]


class TestWebDashboardHelpers:
    """ì›¹ ëŒ€ì‹œë³´ë“œ í—¬í¼ í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸"""
    
    def test_evaluation_result_to_dict(self, sample_evaluation_result):
        """í‰ê°€ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # ì‹¤ì œ êµ¬í˜„ì´ ìˆë‹¤ë©´ í…ŒìŠ¤íŠ¸
        result_dict = {
            'faithfulness': sample_evaluation_result.faithfulness,
            'answer_relevancy': sample_evaluation_result.answer_relevancy,
            'context_recall': sample_evaluation_result.context_recall,
            'context_precision': sample_evaluation_result.context_precision,
            'ragas_score': sample_evaluation_result.ragas_score
        }
        
        assert result_dict['faithfulness'] == 0.85
        assert result_dict['ragas_score'] == 0.84
    
    def test_score_interpretation(self):
        """ì ìˆ˜ í•´ì„ ë¡œì§ í…ŒìŠ¤íŠ¸"""
        # ì ìˆ˜ ë“±ê¸‰ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸
        def get_score_grade(score):
            if score >= 0.8:
                return "ìš°ìˆ˜"
            elif score >= 0.6:
                return "ì–‘í˜¸"
            elif score >= 0.4:
                return "ë³´í†µ"
            else:
                return "ê°œì„ í•„ìš”"
        
        assert get_score_grade(0.85) == "ìš°ìˆ˜"
        assert get_score_grade(0.75) == "ì–‘í˜¸"
        assert get_score_grade(0.55) == "ë³´í†µ"
        assert get_score_grade(0.35) == "ê°œì„ í•„ìš”"


class TestDataVisualization:
    """ë°ì´í„° ì‹œê°í™” ê´€ë ¨ í…ŒìŠ¤íŠ¸"""
    
    def test_radar_chart_data_preparation(self, sample_evaluation_result):
        """ë ˆì´ë” ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„ í…ŒìŠ¤íŠ¸"""
        metrics = ['Faithfulness', 'Answer Relevancy', 'Context Recall', 'Context Precision']
        values = [
            sample_evaluation_result.faithfulness,
            sample_evaluation_result.answer_relevancy,
            sample_evaluation_result.context_recall,
            sample_evaluation_result.context_precision
        ]
        
        assert len(metrics) == len(values)
        assert all(0 <= v <= 1 for v in values)
        assert values[0] == 0.85  # faithfulness
    
    def test_metrics_dataframe_creation(self, sample_evaluation_result):
        """ë©”íŠ¸ë¦­ ë°ì´í„°í”„ë ˆì„ ìƒì„± í…ŒìŠ¤íŠ¸"""
        df = pd.DataFrame({
            'Metric': ['Faithfulness', 'Answer Relevancy', 'Context Recall', 'Context Precision'],
            'Score': [
                sample_evaluation_result.faithfulness,
                sample_evaluation_result.answer_relevancy,
                sample_evaluation_result.context_recall,
                sample_evaluation_result.context_precision
            ]
        })
        
        assert len(df) == 4
        assert 'Metric' in df.columns
        assert 'Score' in df.columns
        assert df['Score'].max() <= 1.0
        assert df['Score'].min() >= 0.0


class TestDatabaseOperations:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ í…ŒìŠ¤íŠ¸ (Streamlit ì•±ì—ì„œ ì‚¬ìš©)"""
    
    @patch('sqlite3.connect')
    def test_save_evaluation_result(self, mock_connect, sample_evaluation_result):
        """í‰ê°€ ê²°ê³¼ ì €ì¥ í…ŒìŠ¤íŠ¸"""
        mock_conn = Mock()
        mock_cursor = Mock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor
        
        # ì‹¤ì œ êµ¬í˜„ì´ ìˆë‹¤ë©´ í…ŒìŠ¤íŠ¸
        def save_evaluation_result(result, db_path="test.db"):
            import sqlite3
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO evaluations (faithfulness, answer_relevancy, context_recall, context_precision, ragas_score, timestamp)
                VALUES (?, ?, ?, ?, ?, datetime('now'))
            """, (result.faithfulness, result.answer_relevancy, result.context_recall, result.context_precision, result.ragas_score))
            conn.commit()
            conn.close()
        
        # í•¨ìˆ˜ ì‹¤í–‰
        save_evaluation_result(sample_evaluation_result)
        
        # í˜¸ì¶œ í™•ì¸
        mock_connect.assert_called_once()
        mock_conn.cursor.assert_called_once()
    
    @patch('sqlite3.connect')
    def test_load_evaluation_history(self, mock_connect):
        """í‰ê°€ ì´ë ¥ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        mock_conn = Mock()
        mock_cursor = Mock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor
        
        # ê°€ì§œ ë°ì´í„° ì„¤ì •
        mock_cursor.fetchall.return_value = [
            (1, 0.85, 0.78, 0.82, 0.91, 0.84, '2024-01-01 12:00:00'),
            (2, 0.88, 0.82, 0.85, 0.93, 0.87, '2024-01-02 12:00:00')
        ]
        
        def load_evaluation_history(db_path="test.db"):
            import sqlite3
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM evaluations ORDER BY timestamp DESC")
            results = cursor.fetchall()
            conn.close()
            return results
        
        # í•¨ìˆ˜ ì‹¤í–‰
        history = load_evaluation_history()
        
        # ê²°ê³¼ í™•ì¸
        assert len(history) == 2
        assert history[0][1] == 0.85  # faithfulness
        mock_connect.assert_called_once()


class TestErrorHandling:
    """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    def test_invalid_data_handling(self):
        """ì˜ëª»ëœ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        with pytest.raises(ValueError):
            # ì˜ëª»ëœ ì ìˆ˜ ë²”ìœ„
            EvaluationResult(
                faithfulness=1.5,  # ë²”ìœ„ ì´ˆê³¼
                answer_relevancy=0.8,
                context_recall=0.7,
                context_precision=0.9,
                ragas_score=0.85
            )
    
    def test_empty_data_handling(self):
        """ë¹ˆ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        empty_data = []
        
        # ë¹ˆ ë°ì´í„°ì— ëŒ€í•œ ì²˜ë¦¬ ë¡œì§ í…ŒìŠ¤íŠ¸
        def process_empty_data(data):
            if not data:
                return {"message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", "data": []}
            return {"message": "ì •ìƒ", "data": data}
        
        result = process_empty_data(empty_data)
        assert result["message"] == "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
        assert result["data"] == []


class TestUIComponents:
    """UI ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ (ëª¨í‚¹ ê¸°ë°˜)"""
    
    @patch('streamlit.metric')
    def test_metrics_display(self, mock_metric, sample_evaluation_result):
        """ë©”íŠ¸ë¦­ í‘œì‹œ í…ŒìŠ¤íŠ¸"""
        def display_metrics(result):
            st.metric("Faithfulness", f"{result.faithfulness:.3f}")
            st.metric("Answer Relevancy", f"{result.answer_relevancy:.3f}")
            st.metric("Context Recall", f"{result.context_recall:.3f}")
            st.metric("Context Precision", f"{result.context_precision:.3f}")
        
        display_metrics(sample_evaluation_result)
        
        # ë©”íŠ¸ë¦­ í˜¸ì¶œ í™•ì¸
        assert mock_metric.call_count == 4
    
    @patch('streamlit.success')
    @patch('streamlit.warning')
    @patch('streamlit.error')
    def test_status_messages(self, mock_error, mock_warning, mock_success):
        """ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œ í…ŒìŠ¤íŠ¸"""
        def show_status_message(score):
            if score >= 0.8:
                st.success(f"ìš°ìˆ˜í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤! ({score:.3f})")
            elif score >= 0.6:
                st.warning(f"ì–‘í˜¸í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤. ({score:.3f})")
            else:
                st.error(f"ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ({score:.3f})")
        
        # ê° ë²”ìœ„ë³„ í…ŒìŠ¤íŠ¸
        show_status_message(0.85)
        mock_success.assert_called_once()
        
        show_status_message(0.75)
        mock_warning.assert_called_once()
        
        show_status_message(0.45)
        mock_error.assert_called_once()


def test_module_imports():
    """ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    try:
        import src.presentation.web.main
        assert True
    except ImportError as e:
        pytest.skip(f"ì›¹ ëŒ€ì‹œë³´ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")


def test_streamlit_configuration():
    """Streamlit ì„¤ì • í…ŒìŠ¤íŠ¸"""
    # í˜ì´ì§€ ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ëŠ” ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    page_config = {
        "page_title": "RAGAS í‰ê°€ ëŒ€ì‹œë³´ë“œ",
        "page_icon": "ğŸ§ª",
        "layout": "wide"
    }
    
    assert page_config["page_title"] == "RAGAS í‰ê°€ ëŒ€ì‹œë³´ë“œ"
    assert page_config["layout"] == "wide"