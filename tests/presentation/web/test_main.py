"""
ì›¹ ëŒ€ì‹œë³´ë“œ ë©”ì¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
Streamlit ì•±ì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸
"""

import sys
from pathlib import Path
from unittest.mock import Mock, patch

import pandas as pd
import pytest
import streamlit as st

# ì„ íƒì  ì˜ì¡´ì„± ì²´í¬
try:
    import plotly.graph_objects as go

    HAS_PLOTLY = True
except ImportError:
    HAS_PLOTLY = False
    go = None

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ sys.pathì— ìˆëŠ”ì§€ í™•ì¸
try:
    from src.domain.entities.evaluation_data import EvaluationData
    from src.domain.entities.evaluation_result import EvaluationResult

    HAS_SRC = True
except ImportError:
    HAS_SRC = False


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ì•„ ê²½ë¡œ ì¶”ê°€
def add_project_root_to_path():
    current_path = Path(__file__).resolve()
    while current_path != current_path.parent:
        if (current_path / "pyproject.toml").exists():
            sys.path.insert(0, str(current_path))
            return current_path
        current_path = current_path.parent
    raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


add_project_root_to_path()

# ë‹¤ì‹œ ì‹œë„í•´ì„œ ì„í¬íŠ¸
if not HAS_SRC:
    try:
        from src.domain.entities.evaluation_data import EvaluationData
        from src.domain.entities.evaluation_result import EvaluationResult
        HAS_SRC = True
    except ImportError:
        HAS_SRC = False


@pytest.fixture
def sample_evaluation_data():
    """í…ŒìŠ¤íŠ¸ìš© í‰ê°€ ë°ì´í„°"""
    return [
        {
            "question": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
            "answer": "ì„œìš¸ì…ë‹ˆë‹¤",
            "contexts": ["ì„œìš¸ì€ í•œêµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤"],
            "ground_truth": "ì„œìš¸",
            "faithfulness": 0.95,
            "answer_relevancy": 0.90,
            "context_recall": 0.85,
            "context_precision": 0.88,
            "ragas_score": 0.895
        }
    ]


class TestWebMain:
    """ì›¹ ëŒ€ì‹œë³´ë“œ ë©”ì¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""

    def test_main_module_import(self):
        """ë©”ì¸ ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
        try:
            from src.presentation.web.main import load_pages
            assert callable(load_pages)
        except ImportError:
            pytest.skip("ì›¹ ë©”ì¸ ëª¨ë“ˆì´ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")

    def test_load_pages_function(self):
        """í˜ì´ì§€ ë¡œë“œ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
        from src.presentation.web.main import load_pages
        
        pages = load_pages()
        
        assert isinstance(pages, dict)
        assert len(pages) > 0
        
        # í•„ìˆ˜ í˜ì´ì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        expected_pages = ["Overview", "Historical", "Detailed Analysis", "Metrics Explanation", "Performance"]
        for expected in expected_pages:
            assert any(expected in key for key in pages.keys())

    @patch('streamlit.sidebar')
    @patch('streamlit.set_page_config')
    def test_streamlit_configuration(self, mock_config, mock_sidebar):
        """Streamlit ì„¤ì • í…ŒìŠ¤íŠ¸"""
        # ì‹¤ì œ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ëª¨ë“ˆì„ ì„í¬íŠ¸
        try:
            import src.presentation.web.main
            # ëª¨ë“ˆì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ë©´ ì„¤ì •ì´ ì˜¬ë°”ë¥´ê²Œ ë˜ì—ˆë‹¤ê³  ê°€ì •
            assert True
        except Exception as e:
            pytest.fail(f"Streamlit ì„¤ì • ì‹¤íŒ¨: {e}")

    def test_database_operations(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… í…ŒìŠ¤íŠ¸"""
        
        def mock_load_evaluations():
            """ëª¨ì˜ í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
            return [
                {
                    "id": 1,
                    "timestamp": "2024-01-01 10:00:00",
                    "faithfulness": 0.95,
                    "answer_relevancy": 0.90,
                    "context_recall": 0.85,
                    "context_precision": 0.88,
                    "ragas_score": 0.895
                }
            ]
        
        evaluations = mock_load_evaluations()
        
        assert len(evaluations) == 1
        assert evaluations[0]["ragas_score"] == 0.895

    def test_visualization_functions(self):
        """ì‹œê°í™” í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
        
        def create_metrics_radar_chart(metrics):
            """ë ˆì´ë” ì°¨íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
            if not HAS_PLOTLY:
                return None
                
            fig = go.Figure()
            
            categories = list(metrics.keys())
            values = list(metrics.values())
            
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=categories,
                fill='toself',
                name='Metrics'
            ))
            
            return fig
        
        test_metrics = {
            "Faithfulness": 0.9,
            "Answer Relevancy": 0.8,
            "Context Recall": 0.85,
            "Context Precision": 0.88
        }
        
        if HAS_PLOTLY:
            chart = create_metrics_radar_chart(test_metrics)
            assert chart is not None
            assert len(chart.data) == 1

    def test_data_processing_functions(self):
        """ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
        
        def process_evaluation_results(results):
            """í‰ê°€ ê²°ê³¼ ì²˜ë¦¬"""
            if not results:
                return {}
            
            processed = {
                "total_evaluations": len(results),
                "avg_ragas_score": sum(r.get("ragas_score", 0) for r in results) / len(results),
                "metrics_summary": {}
            }
            
            # ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
            metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision"]
            for metric in metrics:
                values = [r.get(metric, 0) for r in results if metric in r]
                if values:
                    processed["metrics_summary"][metric] = {
                        "mean": sum(values) / len(values),
                        "min": min(values),
                        "max": max(values)
                    }
            
            return processed
        
        test_results = [
            {"ragas_score": 0.9, "faithfulness": 0.95, "answer_relevancy": 0.85},
            {"ragas_score": 0.8, "faithfulness": 0.85, "answer_relevancy": 0.75}
        ]
        
        processed = process_evaluation_results(test_results)
        
        assert processed["total_evaluations"] == 2
        assert processed["avg_ragas_score"] == 0.85
        assert "faithfulness" in processed["metrics_summary"]

    @patch('sqlite3.connect')
    def test_database_connection_handling(self, mock_connect):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        from unittest.mock import MagicMock
        
        # ì„±ê³µì ì¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.execute.return_value = mock_cursor
        mock_cursor.fetchall.return_value = []
        
        def test_db_operation():
            """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… í…ŒìŠ¤íŠ¸"""
            conn = mock_connect("test.db")
            cursor = conn.execute("SELECT * FROM evaluations")
            results = cursor.fetchall()
            conn.close()
            return results
        
        results = test_db_operation()
        assert results == []
        mock_connect.assert_called_once()

    def test_error_handling(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        
        def safe_division(a, b):
            """ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ"""
            try:
                return a / b
            except ZeroDivisionError:
                return 0
            except Exception:
                return None
        
        # ì •ìƒ ì¼€ì´ìŠ¤
        assert safe_division(10, 2) == 5
        
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        assert safe_division(10, 0) == 0
        
        # ì˜ëª»ëœ íƒ€ì… 
        assert safe_division("10", 2) is None

    def test_session_state_management(self):
        """ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
        
        def manage_session_state():
            """ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ë¡œì§"""
            # Streamlit ì„¸ì…˜ ìƒíƒœ ëª¨í‚¹
            session_state = {}
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            if "current_page" not in session_state:
                session_state["current_page"] = "Overview"
            
            if "evaluation_history" not in session_state:
                session_state["evaluation_history"] = []
            
            return session_state
        
        state = manage_session_state()
        
        assert "current_page" in state
        assert state["current_page"] == "Overview"
        assert "evaluation_history" in state
        assert isinstance(state["evaluation_history"], list)

    def test_page_routing_logic(self):
        """í˜ì´ì§€ ë¼ìš°íŒ… ë¡œì§ í…ŒìŠ¤íŠ¸"""
        
        def route_to_page(page_name, available_pages):
            """í˜ì´ì§€ ë¼ìš°íŒ…"""
            if page_name in available_pages:
                return page_name
            else:
                # ê¸°ë³¸ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜
                return list(available_pages.keys())[0] if available_pages else None
        
        test_pages = {
            "ğŸ¯ Overview": "ë©”ì¸ ëŒ€ì‹œë³´ë“œ",
            "ğŸ“ˆ Historical": "ê³¼ê±° í‰ê°€ ê²°ê³¼"
        }
        
        # ìœ íš¨í•œ í˜ì´ì§€
        result = route_to_page("ğŸ¯ Overview", test_pages)
        assert result == "ğŸ¯ Overview"
        
        # ë¬´íš¨í•œ í˜ì´ì§€ - ê¸°ë³¸ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜
        result = route_to_page("Invalid Page", test_pages)
        assert result == "ğŸ¯ Overview"

    def test_metrics_calculation(self):
        """ë©”íŠ¸ë¦­ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        
        def calculate_overall_score(metrics):
            """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
            if not metrics:
                return 0
            
            # RAGAS ì ìˆ˜ ê³„ì‚° (4ê°œ ë©”íŠ¸ë¦­ì˜ í‰ê· )
            required_metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision"]
            
            available_metrics = [metrics.get(metric, 0) for metric in required_metrics if metric in metrics]
            
            if not available_metrics:
                return 0
            
            return sum(available_metrics) / len(available_metrics)
        
        test_metrics = {
            "faithfulness": 0.9,
            "answer_relevancy": 0.8,
            "context_recall": 0.85,
            "context_precision": 0.88
        }
        
        score = calculate_overall_score(test_metrics)
        expected = (0.9 + 0.8 + 0.85 + 0.88) / 4
        
        assert abs(score - expected) < 0.001

    def test_data_export_functionality(self):
        """ë°ì´í„° ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        def export_to_csv(data):
            """CSV ë‚´ë³´ë‚´ê¸°"""
            if not data:
                return None
            
            df = pd.DataFrame(data)
            return df.to_csv(index=False)
        
        test_data = [
            {"metric": "faithfulness", "value": 0.9},
            {"metric": "answer_relevancy", "value": 0.8}
        ]
        
        csv_output = export_to_csv(test_data)
        
        assert csv_output is not None
        assert "metric,value" in csv_output
        assert "faithfulness,0.9" in csv_output