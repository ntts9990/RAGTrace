#!/usr/bin/env python3
"""
RAGAS ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ - ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì‚¬ìš©
ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ìœ¼ë¡œ RAGAS ë©”íŠ¸ë¦­ì˜ ì¼ê´€ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import json
import time
import logging
import requests
from dotenv import load_dotenv
import uuid
from datetime import datetime
import re

# ë¡œê·¸ ì„¤ì •
log_filename = f'ragas_validation_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë‘ ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° (API ì„¤ê³„ ê´€ë ¨ - ë” ë³µì¡í•œ í‰ê°€ ì‹œë‚˜ë¦¬ì˜¤)
TEST_DATA = {
    "question": "API ì„¤ê³„ì—ì„œ RESTful ì›ì¹™ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "contexts": [
        "RESTful APIëŠ” HTTP ë©”ì„œë“œë¥¼ ì˜ë¯¸ì— ë§ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ìì›(Resource)ì€ URIë¡œ í‘œí˜„ë˜ë©°, ëª…ì‚¬ í˜•íƒœë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.",
        "ìƒíƒœ ì½”ë“œë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ìš”ì²­ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ëª…í™•íˆ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤."
    ],
    "answer": "RESTful ì›ì¹™ì€ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì•„í‚¤í…ì²˜ ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤. SOAPì™€ ë‹¬ë¦¬ ê°€ë³ê³  ë¹ ë¥¸ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, JSONê³¼ XML í˜•ì‹ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤. ë˜í•œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì™€ ì˜ ì–´ìš¸ë¦¬ëŠ” íŠ¹ì§•ì´ ìˆì–´ ë§ì€ íšŒì‚¬ì—ì„œ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "ground_truth": "RESTful ì›ì¹™ì€ HTTP ë©”ì„œë“œì˜ ì˜ë¯¸ì  ì‚¬ìš©, ìì›ì˜ URI í‘œí˜„, ì ì ˆí•œ ìƒíƒœ ì½”ë“œ ì‚¬ìš©ì„ í†µí•´ ì¼ê´€ë˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ APIë¥¼ ì„¤ê³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
}

# RAGAS ë©”íŠ¸ë¦­ë³„ í”„ë¡¬í”„íŠ¸ (ì´ì „ê³¼ ë™ì¼)
PROMPTS = {
    "faithfulness": {
        "system": "You are an expert evaluator for RAG systems. Extract statements from the given answer exactly as they appear, without interpretation or modification.",
        "user": """Given an answer, extract each sentence or claim exactly as stated in the original answer.

IMPORTANT: Do NOT interpret, rephrase, or modify the sentences. Extract them EXACTLY as they appear in the answer.

Answer: {answer}

Return a JSON object with a single key 'statements' containing an array of strings, where each string is a sentence or claim from the answer.

Example for "The sky is blue. Water is wet.":
{{"statements": ["The sky is blue.", "Water is wet."]}}"""
    },
    "answer_relevancy": {
        "system": "You are an expert evaluator. Assess if the answer is relevant to the question.",
        "user": """Question: {question}
Answer: {answer}

Evaluate if the answer addresses the question. Return a JSON object with 'reason' explaining your assessment.

Example:
{{"reason": "The answer directly addresses the question by explaining..."}}"""
    },
    "context_recall": {
        "system": "You are an expert evaluator. Assess if the contexts contain information needed for the ground truth.",
        "user": """Question: {question}
Ground Truth: {ground_truth}
Contexts: {contexts}

Evaluate if the contexts contain all information needed to derive the ground truth. Return a JSON with 'reason'.

Example:
{{"reason": "The contexts contain all necessary information..."}}"""
    },
    "context_precision": {
        "system": "You are an expert evaluator. Assess the precision and relevance of the contexts.",
        "user": """Question: {question}
Contexts: {contexts}

Evaluate if the contexts are precise and relevant without unnecessary information. Return a JSON with 'reason'.

Example:
{{"reason": "The contexts are highly relevant with minimal irrelevant information..."}}"""
    }
}


def parse_json_response(content: str) -> dict:
    """JSON ì‘ë‹µ íŒŒì‹± (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)"""
    content = content.strip()
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    if "```json" in content or "```" in content:
        match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
    
    # 2. JSON ê°ì²´ ì¶”ì¶œ
    start = content.find('{')
    end = content.rfind('}')
    if start != -1 and end != -1:
        try:
            return json.loads(content[start:end+1])
        except json.JSONDecodeError:
            pass
    
    # 3. ì „ì²´ ë¬¸ìì—´ íŒŒì‹±
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        # 4. ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í˜•ì‹
        return {"error": "Failed to parse JSON", "raw": content}


def test_gemini_metric(metric_name: str) -> dict:
    """Geminië¡œ íŠ¹ì • ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    logger.info(f"\n[Gemini] {metric_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        return {"model": "gemini", "metric": metric_name, "success": False, "error": "No API key"}
    
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent"
    
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key
    }
    
    # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    prompt_config = PROMPTS[metric_name]
    if metric_name == "faithfulness":
        user_prompt = prompt_config["user"].format(answer=TEST_DATA["answer"])
    elif metric_name == "answer_relevancy":
        user_prompt = prompt_config["user"].format(
            question=TEST_DATA["question"],
            answer=TEST_DATA["answer"]
        )
    elif metric_name == "context_recall":
        user_prompt = prompt_config["user"].format(
            question=TEST_DATA["question"],
            ground_truth=TEST_DATA["ground_truth"],
            contexts="\n".join(TEST_DATA["contexts"])
        )
    elif metric_name == "context_precision":
        user_prompt = prompt_config["user"].format(
            question=TEST_DATA["question"],
            contexts="\n".join(TEST_DATA["contexts"])
        )
    
    # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    full_prompt = f"{prompt_config['system']}\n\n{user_prompt}"
    
    payload = {
        "contents": [{
            "parts": [{
                "text": full_prompt
            }]
        }],
        "generationConfig": {
            "temperature": 0.1,
            "maxOutputTokens": 4096
        }
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        if "candidates" in data and data["candidates"]:
            content = data["candidates"][0]["content"]["parts"][0]["text"]
            logger.info(f"[Gemini] ì›ë³¸ ì‘ë‹µ:\n{content}")
            
            parsed = parse_json_response(content)
            logger.info(f"[Gemini] íŒŒì‹±ëœ JSON:\n{json.dumps(parsed, ensure_ascii=False, indent=2)}")
            
            return {
                "model": "gemini",
                "metric": metric_name,
                "success": True,
                "raw_response": content,
                "parsed_response": parsed
            }
        else:
            return {
                "model": "gemini",
                "metric": metric_name,
                "success": False,
                "error": "No candidates in response"
            }
            
    except Exception as e:
        logger.error(f"[Gemini] {metric_name} ì˜¤ë¥˜: {str(e)}")
        return {
            "model": "gemini",
            "metric": metric_name,
            "success": False,
            "error": str(e)
        }


def test_hcx_metric(metric_name: str) -> dict:
    """HCXë¡œ íŠ¹ì • ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    logger.info(f"\n[HCX] {metric_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    api_key = os.getenv("CLOVA_STUDIO_API_KEY")
    if not api_key:
        return {"model": "hcx", "metric": metric_name, "success": False, "error": "No API key"}
    
    api_url = "https://clovastudio.stream.ntruss.com/testapp/v3/chat-completions/HCX-005"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "X-NCP-CLOVASTUDIO-API-KEY": api_key,
        "X-NCP-APIGW-API-KEY": api_key,
        "X-NCP-CLOVASTUDIO-REQUEST-ID": str(uuid.uuid4()),
        "Content-Type": "application/json",
    }
    
    # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    prompt_config = PROMPTS[metric_name]
    if metric_name == "faithfulness":
        user_prompt = prompt_config["user"].format(answer=TEST_DATA["answer"])
    elif metric_name == "answer_relevancy":
        user_prompt = prompt_config["user"].format(
            question=TEST_DATA["question"],
            answer=TEST_DATA["answer"]
        )
    elif metric_name == "context_recall":
        user_prompt = prompt_config["user"].format(
            question=TEST_DATA["question"],
            ground_truth=TEST_DATA["ground_truth"],
            contexts="\n".join(TEST_DATA["contexts"])
        )
    elif metric_name == "context_precision":
        user_prompt = prompt_config["user"].format(
            question=TEST_DATA["question"],
            contexts="\n".join(TEST_DATA["contexts"])
        )
    
    body = {
        "messages": [
            {
                "role": "system",
                "content": prompt_config["system"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ],
        "maxTokens": 4096,
        "temperature": 0.1,
        "topP": 0.8,
        "topK": 0,
        "repetitionPenalty": 1.1,
        "stop": [],
        "includeAiFilters": False,
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=body, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        if "result" in data and "message" in data["result"]:
            content = data["result"]["message"]["content"]
        elif "message" in data:
            content = data["message"]["content"]
        else:
            content = ""
        
        logger.info(f"[HCX] ì›ë³¸ ì‘ë‹µ:\n{content}")
        
        parsed = parse_json_response(content)
        logger.info(f"[HCX] íŒŒì‹±ëœ JSON:\n{json.dumps(parsed, ensure_ascii=False, indent=2)}")
        
        return {
            "model": "hcx",
            "metric": metric_name,
            "success": True,
            "raw_response": content,
            "parsed_response": parsed
        }
        
    except Exception as e:
        logger.error(f"[HCX] {metric_name} ì˜¤ë¥˜: {str(e)}")
        return {
            "model": "hcx",
            "metric": metric_name,
            "success": False,
            "error": str(e)
        }


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("=" * 80)
    logger.info("RAGAS ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ - ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹")
    logger.info("=" * 80)
    
    logger.info(f"\ní…ŒìŠ¤íŠ¸ ë°ì´í„°:")
    logger.info(f"ì§ˆë¬¸: {TEST_DATA['question']}")
    logger.info(f"ë‹µë³€: {TEST_DATA['answer'][:100]}...")
    logger.info(f"ì •ë‹µ: {TEST_DATA['ground_truth'][:100]}...")
    
    # ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
    metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision"]
    
    # ê²°ê³¼ ì €ì¥
    all_results = []
    
    # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for metric in metrics:
        logger.info(f"\n{'='*60}")
        logger.info(f"{metric.upper()} ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸")
        logger.info(f"{'='*60}")
        
        # Gemini í…ŒìŠ¤íŠ¸
        gemini_result = test_gemini_metric(metric)
        all_results.append(gemini_result)
        time.sleep(2)  # API ì œí•œ ë°©ì§€
        
        # HCX í…ŒìŠ¤íŠ¸
        hcx_result = test_hcx_metric(metric)
        all_results.append(hcx_result)
        time.sleep(3)  # HCXëŠ” ë” ê¸´ ëŒ€ê¸°
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 80)
    logger.info("ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 80)
    
    # ëª¨ë¸ë³„ ì„±ê³µë¥  ê³„ì‚°
    for model in ["gemini", "hcx"]:
        model_results = [r for r in all_results if r["model"] == model]
        success_count = sum(1 for r in model_results if r["success"])
        total_count = len(model_results)
        
        logger.info(f"\n{model.upper()} ê²°ê³¼:")
        logger.info(f"- ì„±ê³µë¥ : {success_count}/{total_count} ({success_count/total_count*100:.0f}%)")
        
        for metric in metrics:
            metric_result = next((r for r in model_results if r["metric"] == metric), None)
            if metric_result and metric_result["success"]:
                parsed = metric_result.get("parsed_response", {})
                if metric == "faithfulness" and "statements" in parsed:
                    logger.info(f"- {metric}: âœ… (statements: {len(parsed['statements'])}ê°œ)")
                elif "reason" in parsed:
                    logger.info(f"- {metric}: âœ… (reason ê¸¸ì´: {len(parsed['reason'])}ì)")
                else:
                    logger.info(f"- {metric}: âœ…")
            else:
                logger.info(f"- {metric}: âŒ")
    
    # Faithfulness ë¹„êµ ë¶„ì„
    logger.info("\n" + "=" * 80)
    logger.info("Faithfulness ë¶„ì„ - ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ ë¶ˆì¼ì¹˜ ì‹œë‚˜ë¦¬ì˜¤")
    logger.info("=" * 80)
    
    logger.info(f"\nì›ë³¸ ë‹µë³€:")
    logger.info(f"{TEST_DATA['answer']}")
    
    logger.info(f"\nì œê³µëœ ì»¨í…ìŠ¤íŠ¸:")
    for i, context in enumerate(TEST_DATA['contexts'], 1):
        logger.info(f"  {i}. {context}")
    
    for model in ["gemini", "hcx"]:
        faith_result = next((r for r in all_results if r["model"] == model and r["metric"] == "faithfulness"), None)
        if faith_result and faith_result["success"]:
            parsed = faith_result.get("parsed_response", {})
            if "statements" in parsed:
                logger.info(f"\n{model.upper()} ì¶”ì¶œëœ statements:")
                for i, stmt in enumerate(parsed["statements"], 1):
                    logger.info(f"  {i}. {stmt}")
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€ í™•ì¸
    total_success = sum(1 for r in all_results if r["success"])
    total_tests = len(all_results)
    overall_success_rate = total_success / total_tests * 100
    
    logger.info(f"\nì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ : {total_success}/{total_tests} ({overall_success_rate:.0f}%)")
    
    if overall_success_rate == 100:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! RAGAS ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
        test_passed = True
    else:
        logger.info("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        test_passed = False
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    output_file = f"ragas_validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    results_summary = {
        "test_timestamp": datetime.now().isoformat(),
        "test_data": TEST_DATA,
        "overall_success_rate": overall_success_rate,
        "test_passed": test_passed,
        "detailed_results": all_results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info(f"ë¡œê·¸ íŒŒì¼: {log_filename}")
    
    return test_passed


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)