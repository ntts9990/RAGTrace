"""
ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸
ê°œë³„ QA ìŒì˜ ìƒì„¸ í‰ê°€ ê²°ê³¼ ë¶„ì„
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
from pathlib import Path

def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ë©”ì¸ í™”ë©´"""
    st.header("ğŸ” ìƒì„¸ ë¶„ì„")
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    evaluation_data = load_evaluation_data()
    
    if not evaluation_data:
        st.warning("ğŸ“ ë¶„ì„í•  í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š QA ê°œë³„ ë¶„ì„", "ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„í¬", "ğŸ¯ íŒ¨í„´ ë¶„ì„"])
    
    with tab1:
        show_qa_analysis(evaluation_data)
    
    with tab2:
        show_metric_distribution(evaluation_data)
    
    with tab3:
        show_pattern_analysis(evaluation_data)

def show_qa_analysis(evaluation_data):
    """ê°œë³„ QA ë¶„ì„"""
    st.subheader("ğŸ“‹ ì§ˆë¬¸-ë‹µë³€ ìŒë³„ ìƒì„¸ ë¶„ì„")
    
    # QA ìŒ ì„ íƒ
    qa_options = [f"Q{i+1}: {qa['question'][:50]}..." for i, qa in enumerate(evaluation_data)]
    selected_qa_idx = st.selectbox("ë¶„ì„í•  QA ì„ íƒ", range(len(qa_options)), format_func=lambda x: qa_options[x])
    
    if selected_qa_idx is not None:
        qa_data = evaluation_data[selected_qa_idx]
        show_individual_qa_details(qa_data, selected_qa_idx + 1)

def show_individual_qa_details(qa_data, qa_number):
    """ê°œë³„ QA ìƒì„¸ ì •ë³´ í‘œì‹œ"""
    st.markdown(f"### ğŸ“ QA {qa_number} ìƒì„¸ ë¶„ì„")
    
    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ¤” ì§ˆë¬¸")
        st.info(qa_data['question'])
        
        st.markdown("#### ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
        st.success(qa_data['answer'])
    
    with col2:
        st.markdown("#### ğŸ“š ì œê³µëœ ì»¨í…ìŠ¤íŠ¸")
        for i, context in enumerate(qa_data['contexts'], 1):
            st.markdown(f"**ì»¨í…ìŠ¤íŠ¸ {i}:**")
            st.text_area(f"context_{i}", context, height=80, disabled=True, key=f"context_{qa_number}_{i}")
        
        st.markdown("#### âœ… ì •ë‹µ (Ground Truth)")
        st.info(qa_data['ground_truth'])
    
    # ê°€ìƒì˜ ê°œë³„ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ìµœì‹  í‰ê°€ ê²°ê³¼ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
    st.markdown("#### ğŸ“Š ì´ QAì˜ í‰ê°€ ì ìˆ˜")
    
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì €ì¥ëœ ìƒì„¸ ê²°ê³¼ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
    mock_scores = {
        'faithfulness': 1.0 if qa_number == 1 else 1.0,
        'answer_relevancy': 0.861 if qa_number == 1 else 0.741,
        'context_recall': 1.0,
        'context_precision': 0.5 if qa_number == 1 else 1.0
    }
    
    score_cols = st.columns(4)
    for i, (metric, score) in enumerate(mock_scores.items()):
        with score_cols[i]:
            color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
            st.metric(
                label=metric.replace('_', ' ').title(),
                value=f"{score:.3f}",
                delta=f"{score - 0.5:.3f}"
            )
    
    # ì ìˆ˜ ì‹œê°í™”
    show_qa_score_chart(mock_scores, qa_number)
    
    # í‰ê°€ ê·¼ê±° (ì‹¤ì œ í…ìŠ¤íŠ¸ í¬í•¨)
    show_evaluation_reasoning(qa_data, qa_number)

def show_qa_score_chart(scores, qa_number):
    """ê°œë³„ QA ì ìˆ˜ ì°¨íŠ¸"""
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ì‹œê°í™”")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°” ì°¨íŠ¸
        metrics = list(scores.keys())
        values = list(scores.values())
        
        fig = go.Figure(data=[
            go.Bar(x=metrics, y=values, 
                  marker_color=['green' if v >= 0.8 else 'orange' if v >= 0.6 else 'red' for v in values])
        ])
        
        fig.update_layout(
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ì ìˆ˜",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ê²Œì´ì§€ ì°¨íŠ¸ (í‰ê·  ì ìˆ˜)
        avg_score = sum(values) / len(values)
        
        fig = go.Figure(go.Indicator(
            mode = "gauge+number+delta",
            value = avg_score,
            domain = {'x': [0, 1], 'y': [0, 1]},
            title = {'text': f"QA {qa_number} ì¢…í•© ì ìˆ˜"},
            delta = {'reference': 0.5},
            gauge = {
                'axis': {'range': [None, 1]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 0.5], 'color': "lightgray"},
                    {'range': [0.5, 0.8], 'color': "yellow"},
                    {'range': [0.8, 1], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 0.9
                }
            }
        ))
        
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

def show_evaluation_reasoning(qa_data, qa_number):
    """í‰ê°€ ê·¼ê±° í‘œì‹œ (ì‹¤ì œ í…ìŠ¤íŠ¸ í¬í•¨)"""
    st.markdown("#### ğŸ§  í‰ê°€ ê·¼ê±°")
    
    # ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°
    question = qa_data['question']
    answer = qa_data['answer']
    contexts = qa_data['contexts']
    ground_truth = qa_data['ground_truth']
    
    # í–¥ìƒëœ í‰ê°€ ê·¼ê±° (ì‹¤ì œ í…ìŠ¤íŠ¸ í¬í•¨)
    detailed_reasoning = get_detailed_reasoning(qa_data, qa_number)
    
    for metric, analysis in detailed_reasoning.items():
        with st.expander(f"ğŸ“ {metric.replace('_', ' ').title()} í‰ê°€ ê·¼ê±°"):
            st.markdown(analysis['explanation'])
            
            if 'text_analysis' in analysis:
                st.markdown("##### ğŸ“„ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¶„ì„:")
                for item in analysis['text_analysis']:
                    if item['type'] == 'highlight':
                        st.markdown(f"**{item['label']}:**")
                        st.code(item['text'], language=None)
                    elif item['type'] == 'comparison':
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown(f"**{item['label1']}:**")
                            st.info(item['text1'])
                        with col2:
                            st.markdown(f"**{item['label2']}:**")
                            st.info(item['text2'])

def get_detailed_reasoning(qa_data, qa_number):
    """ìƒì„¸í•œ í‰ê°€ ê·¼ê±° ìƒì„±"""
    question = qa_data['question']
    answer = qa_data['answer']
    contexts = qa_data['contexts']
    ground_truth = qa_data['ground_truth']
    
    if qa_number == 1:
        return {
            'faithfulness': {
                'explanation': """ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤. 
                ì˜ì¡´ì„± ê·œì¹™ì— ëŒ€í•œ ì„¤ëª…ì´ ì •í™•í•˜ë©°, ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'highlight',
                        'label': 'ë‹µë³€ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ë¬¸ì¥',
                        'text': 'í´ë¦° ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì›ì¹™ì€ ì˜ì¡´ì„± ê·œì¹™ì…ë‹ˆë‹¤.'
                    },
                    {
                        'type': 'highlight',
                        'label': 'ì´ë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸',
                        'text': "ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ì€ 'ì˜ì¡´ì„± ê·œì¹™'ìœ¼ë¡œ, ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œ ì˜ì¡´ì„±ì€ ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ..."
                    }
                ]
            },
            'answer_relevancy': {
                'explanation': """ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ê³  ìˆìœ¼ë‚˜, ì¼ë¶€ ì¶”ê°€ì ì¸ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ê´€ë ¨ì„±ì´ ì•½ê°„ ë‚®ìŠµë‹ˆë‹¤. 
                ì§ˆë¬¸ì€ 'í•µì‹¬ ì›ì¹™'ë§Œ ë¬»ê³  ìˆëŠ”ë°, ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ë°©í–¥ì„±ê¹Œì§€ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'comparison',
                        'label1': 'ì§ˆë¬¸ (í•µì‹¬ë§Œ ìš”êµ¬)',
                        'text1': 'í´ë¦° ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?',
                        'label2': 'ë‹µë³€ (ì¶”ê°€ ì„¤ëª… í¬í•¨)',
                        'text2': 'í´ë¦° ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì›ì¹™ì€ ì˜ì¡´ì„± ê·œì¹™ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë“  ì†ŒìŠ¤ì½”ë“œ ì˜ì¡´ì„±ì´ ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ í–¥í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.'
                    },
                    {
                        'type': 'highlight',
                        'label': 'ì¶”ê°€ëœ ì„¤ëª… ë¶€ë¶„ (ê´€ë ¨ì„± ì €í•˜ ìš”ì¸)',
                        'text': 'ì´ëŠ” ëª¨ë“  ì†ŒìŠ¤ì½”ë“œ ì˜ì¡´ì„±ì´ ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ í–¥í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.'
                    }
                ]
            },
            'context_recall': {
                'explanation': """Ground truthì˜ ëª¨ë“  í•µì‹¬ ì •ë³´ê°€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤. 
                ì˜ì¡´ì„± ê·œì¹™ê³¼ ê·¸ ë°©í–¥ì„±ì— ëŒ€í•œ ëª¨ë“  ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'comparison',
                        'label1': 'Ground Truth í•µì‹¬ ë‚´ìš©',
                        'text1': 'ì˜ì¡´ì„± ê·œì¹™ìœ¼ë¡œ, ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œ ì˜ì¡´ì„±ì€ ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ, ì €ìˆ˜ì¤€ ì •ì±…ì—ì„œ ê³ ìˆ˜ì¤€ ì •ì±…ìœ¼ë¡œ',
                        'label2': 'ë§¤ì¹­ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸',
                        'text2': "ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ì€ 'ì˜ì¡´ì„± ê·œì¹™'ìœ¼ë¡œ, ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œ ì˜ì¡´ì„±ì€ ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ, ì¦‰ ì €ìˆ˜ì¤€ì˜ êµ¬ì²´ì ì¸ ì •ì±…ì—ì„œ ê³ ìˆ˜ì¤€ì˜ ì¶”ìƒì ì¸ ì •ì±…ìœ¼ë¡œë§Œ í–¥í•´ì•¼ í•©ë‹ˆë‹¤."
                    }
                ]
            },
            'context_precision': {
                'explanation': """ì²« ë²ˆì§¸ ì»¨í…ìŠ¤íŠ¸ëŠ” ì¼ë°˜ì ì¸ ë°°ê²½ ì„¤ëª…ì´ê³ , ë‘ ë²ˆì§¸ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ í¬í•¨í•©ë‹ˆë‹¤. 
                ì„¸ ë²ˆì§¸ ì»¨í…ìŠ¤íŠ¸ëŠ” í´ë¦° ì•„í‚¤í…ì²˜ì˜ ì¥ì ì— ëŒ€í•œ ë‚´ìš©ìœ¼ë¡œ ì§ì ‘ì ì¸ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'highlight',
                        'label': 'ë†’ì€ ì •í™•ë„ - í•µì‹¬ ë‹µë³€ ì»¨í…ìŠ¤íŠ¸',
                        'text': "ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ì€ 'ì˜ì¡´ì„± ê·œì¹™'ìœ¼ë¡œ..."
                    },
                    {
                        'type': 'highlight',
                        'label': 'ì¤‘ê°„ ì •í™•ë„ - ë°°ê²½ ì„¤ëª… ì»¨í…ìŠ¤íŠ¸',
                        'text': 'í´ë¦° ì•„í‚¤í…ì²˜ëŠ” ë¡œë²„íŠ¸ C. ë§ˆí‹´ì´ ì œì•ˆí•œ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ ì² í•™ì…ë‹ˆë‹¤.'
                    },
                    {
                        'type': 'highlight',
                        'label': 'ë‚®ì€ ì •í™•ë„ - ê°„ì ‘ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸',
                        'text': 'ì´ë¥¼ í†µí•´ ì‹œìŠ¤í…œì€ í”„ë ˆì„ì›Œí¬, ë°ì´í„°ë² ì´ìŠ¤, UIì™€ ë…ë¦½ì ìœ¼ë¡œ ìœ ì§€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
                    }
                ]
            }
        }
    else:  # qa_number == 2
        return {
            'faithfulness': {
                'explanation': """ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ì™€ ì™„ì „íˆ ì¼ì¹˜í•˜ë©° ì¶”ê°€ ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 
                í™˜ê° í˜„ìƒ ì—†ì´ ì œê³µëœ ì •ë³´ë§Œì„ í™œìš©í–ˆìŠµë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'comparison',
                        'label1': 'ë‹µë³€ ë‚´ìš©',
                        'text1': 'FaithfulnessëŠ” ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€, ì¦‰ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ì•Šì•˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.',
                        'label2': 'ë§¤ì¹­ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸',
                        'text2': 'FaithfulnessëŠ” ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.'
                    }
                ]
            },
            'answer_relevancy': {
                'explanation': """ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‹µë³€ì´ì§€ë§Œ ì¼ë¶€ í‘œí˜„ì´ ë‹¤ì†Œ ë³µì¡í•©ë‹ˆë‹¤. 
                'ì¦‰ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ì•Šì•˜ëŠ”ì§€ë¥¼'ë¼ëŠ” ë¶€ë¶„ì´ ì¶”ê°€ ì„¤ëª…ì— í•´ë‹¹í•©ë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'highlight',
                        'label': 'ë³µì¡í•œ í‘œí˜„ (ê´€ë ¨ì„± ì €í•˜ ìš”ì¸)',
                        'text': 'ì¦‰ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ì•Šì•˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ”'
                    },
                    {
                        'type': 'highlight',
                        'label': 'ë” ê°„ë‹¨í•œ ëŒ€ì•ˆ',
                        'text': 'FaithfulnessëŠ” ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.'
                    }
                ]
            },
            'context_recall': {
                'explanation': """Ground truthì˜ ëª¨ë“  ìš”ì†Œê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ë©ë‹ˆë‹¤. 
                í™˜ê° í˜„ìƒ ì¸¡ì •ì— ëŒ€í•œ ë‚´ìš©ë„ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'comparison',
                        'label1': 'Ground Truth ì „ì²´',
                        'text1': 'FaithfulnessëŠ” ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ë¥¼ í‰ê°€í•˜ì—¬ LLMì˜ í™˜ê° í˜„ìƒì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ë‹¤.',
                        'label2': 'ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë˜ëŠ” ëª¨ë“  ìš”ì†Œ',
                        'text2': '1) ì¶©ì‹¤ì„± í‰ê°€ + 2) í™˜ê° í˜„ìƒ ì¸¡ì •'
                    }
                ]
            },
            'context_precision': {
                'explanation': """ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ê°€ ë‹µë³€ì— ì§ì ‘ì ìœ¼ë¡œ ê¸°ì—¬í•©ë‹ˆë‹¤. 
                ì„¸ ê°œ ì»¨í…ìŠ¤íŠ¸ ëª¨ë‘ Faithfulnessì— ëŒ€í•œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.""",
                'text_analysis': [
                    {
                        'type': 'highlight',
                        'label': 'ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ê°€ ìœ ìš©í•¨',
                        'text': '1) Faithfulness ì •ì˜ + 2) í™˜ê° ì¸¡ì • ìš©ë„ + 3) ë‹¤ë¥¸ ì§€í‘œì™€ì˜ êµ¬ë¶„'
                    }
                ]
            }
        }

def show_metric_distribution(evaluation_data):
    """ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„"""
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„")
    
    # ëª¨ì˜ ì ìˆ˜ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì €ì¥ëœ ê²°ê³¼ì—ì„œ)
    mock_data = {
        'QA': [f'Q{i+1}' for i in range(len(evaluation_data))],
        'faithfulness': [1.0, 1.0],
        'answer_relevancy': [0.861, 0.741],
        'context_recall': [1.0, 1.0], 
        'context_precision': [0.5, 1.0]
    }
    
    df = pd.DataFrame(mock_data)
    
    # íˆíŠ¸ë§µ
    st.markdown("#### ğŸ”¥ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ")
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    heatmap_data = df[metrics].values
    
    # í˜¸ë²„ ì •ë³´ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ìƒì„±
    hover_text = []
    for i, qa in enumerate(df['QA']):
        row_text = []
        for j, metric in enumerate(metrics):
            score = heatmap_data[i, j]
            # ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰
            if score >= 0.9:
                grade = "ğŸŒŸ ìš°ìˆ˜"
            elif score >= 0.8:
                grade = "âœ… ì–‘í˜¸"
            elif score >= 0.6:
                grade = "âš ï¸ ë³´í†µ"
            else:
                grade = "âŒ ê°œì„ í•„ìš”"
            
            text = f"QA: {qa}<br>ë©”íŠ¸ë¦­: {metric.replace('_', ' ').title()}<br>ì ìˆ˜: {score:.3f}<br>ë“±ê¸‰: {grade}"
            row_text.append(text)
        hover_text.append(row_text)
    
    fig = go.Figure(data=go.Heatmap(
        z=heatmap_data,
        x=[m.replace('_', ' ').title() for m in metrics],
        y=df['QA'],
        colorscale='RdYlGn',
        colorbar=dict(title="ì ìˆ˜"),
        hovertemplate='%{hovertext}<extra></extra>',
        hovertext=hover_text
    ))
    
    fig.update_layout(
        title="QAë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ",
        height=400,
        xaxis_title="ë©”íŠ¸ë¦­",
        yaxis_title="ì§ˆë¬¸-ë‹µë³€"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # ë¶„í¬ ì°¨íŠ¸
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ë¶„í¬")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°•ìŠ¤í”Œë¡¯
        fig = go.Figure()
        
        for metric in metrics:
            fig.add_trace(go.Box(
                y=df[metric],
                name=metric.replace('_', ' ').title(),
                boxpoints='all'
            ))
        
        fig.update_layout(
            title="ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë¶„í¬",
            yaxis_title="ì ìˆ˜",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # íˆìŠ¤í† ê·¸ë¨
        selected_metric = st.selectbox("ë¶„í¬ë¥¼ ë³¼ ë©”íŠ¸ë¦­ ì„ íƒ", metrics)
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        metric_data = df[selected_metric].dropna()
        
        if len(metric_data) > 0 and not metric_data.isna().all():
            # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
            fig = go.Figure(data=[go.Histogram(
                x=metric_data, 
                nbinsx=min(10, len(metric_data)),  # ë°ì´í„° ê°œìˆ˜ì— ë”°ë¼ bin ìˆ˜ ì¡°ì •
                histnorm='probability density' if len(metric_data) > 1 else 'count'
            )])
            
            fig.update_layout(
                title=f"{selected_metric.replace('_', ' ').title()} ì ìˆ˜ ë¶„í¬",
                xaxis_title="ì ìˆ˜",
                yaxis_title="ë¹ˆë„",
                height=400,
                xaxis=dict(range=[0, 1])  # ì ìˆ˜ ë²”ìœ„ ê³ ì •
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€ (ì•ˆì „í•œ ê³„ì‚°)
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            with col_stat1:
                try:
                    mean_val = metric_data.mean()
                    st.metric("í‰ê· ", f"{mean_val:.3f}" if not pd.isna(mean_val) else "ê³„ì‚°ë¶ˆê°€")
                except:
                    st.metric("í‰ê· ", "ê³„ì‚°ë¶ˆê°€")
            with col_stat2:
                try:
                    std_val = metric_data.std()
                    st.metric("í‘œì¤€í¸ì°¨", f"{std_val:.3f}" if not pd.isna(std_val) else "ê³„ì‚°ë¶ˆê°€")
                except:
                    st.metric("í‘œì¤€í¸ì°¨", "ê³„ì‚°ë¶ˆê°€")
            with col_stat3:
                st.metric("ë°ì´í„° ê°œìˆ˜", len(metric_data))
        else:
            st.warning(f"{selected_metric} ë©”íŠ¸ë¦­ì— ëŒ€í•œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def show_pattern_analysis(evaluation_data):
    """íŒ¨í„´ ë¶„ì„"""
    st.subheader("ğŸ¯ RAG ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒ¨í„´ ë¶„ì„")
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë” ë§ì€ ë¶„ì„ ì œê³µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ ì§ˆë¬¸ íŠ¹ì„±", "ğŸ“š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„", "ğŸ¯ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸", "ğŸ”— ìƒê´€ê´€ê³„"])
    
    with tab1:
        show_question_analysis(evaluation_data)
    
    with tab2:
        show_context_analysis(evaluation_data)
    
    with tab3:
        show_performance_insights(evaluation_data)
    
    with tab4:
        show_correlation_analysis(evaluation_data)

def show_question_analysis(evaluation_data):
    """ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“ ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„")
        
        # ì§ˆë¬¸ ê¸¸ì´ ë¶„ì„
        question_lengths = [len(qa['question'].split()) for qa in evaluation_data]
        avg_length = sum(question_lengths) / len(question_lengths)
        
        st.metric("í‰ê·  ì§ˆë¬¸ ê¸¸ì´", f"{avg_length:.1f} ë‹¨ì–´")
        
        # ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
        complex_indicators = 0
        for qa in evaluation_data:
            question = qa['question']
            if '?' in question or 'ë¬´ì—‡' in question or 'ì–´ë–»ê²Œ' in question:
                complex_indicators += 1
        
        complexity_ratio = complex_indicators / len(evaluation_data) * 100
        st.metric("ëª…í™•í•œ ì§ˆë¬¸ ë¹„ìœ¨", f"{complexity_ratio:.1f}%")
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        question_types = []
        for qa in evaluation_data:
            question = qa['question'].lower()
            if 'ë¬´ì—‡' in question:
                question_types.append('ì •ì˜í˜•')
            elif 'ì–´ë–»ê²Œ' in question:
                question_types.append('ë°©ë²•í˜•')
            elif 'ì™œ' in question:
                question_types.append('ì´ìœ í˜•')
            else:
                question_types.append('ê¸°íƒ€')
        
        type_counts = pd.Series(question_types).value_counts()
        
        fig = go.Figure(data=[go.Pie(labels=type_counts.index, values=type_counts.values)])
        fig.update_layout(title="ì§ˆë¬¸ ìœ í˜• ë¶„í¬", height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ¯ ì§ˆë¬¸ ìµœì í™” ì œì•ˆ")
        
        # ì§ˆë¬¸ë³„ ë¬¸ì œì  ë¶„ì„
        for i, qa in enumerate(evaluation_data):
            question = qa['question']
            
            # ê°„ë‹¨í•œ ë¶„ì„
            issues = []
            if len(question.split()) > 15:
                issues.append("â“ ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹€")
            if '?' not in question and '?' not in question:
                issues.append("â“ ëª…í™•í•œ ì§ˆë¬¸ í˜•íƒœê°€ ì•„ë‹˜")
            if not any(word in question for word in ['ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ']):
                issues.append("â“ ë¶ˆëª…í™•í•œ ì˜ë„")
            
            if issues:
                with st.expander(f"Q{i+1} ê°œì„  ì œì•ˆ"):
                    st.write(f"**ì§ˆë¬¸:** {question}")
                    for issue in issues:
                        st.write(f"- {issue}")
            else:
                with st.expander(f"Q{i+1} âœ… ì¢‹ì€ ì§ˆë¬¸"):
                    st.write(f"**ì§ˆë¬¸:** {question}")
                    st.success("ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì§ˆë¬¸ì…ë‹ˆë‹¤.")

def show_context_analysis(evaluation_data):
    """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“š ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„±")
        
        # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ ë¶„ì„
        context_counts = [len(qa['contexts']) for qa in evaluation_data]
        avg_contexts = sum(context_counts) / len(context_counts)
        
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜", f"{avg_contexts:.1f} ê°œ")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
        all_context_lengths = []
        for qa in evaluation_data:
            for context in qa['contexts']:
                all_context_lengths.append(len(context.split()))
        
        avg_context_length = sum(all_context_lengths) / len(all_context_lengths)
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´", f"{avg_context_length:.1f} ë‹¨ì–´")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µë„ ë¶„ì„
        overlap_scores = []
        for qa in evaluation_data:
            contexts = qa['contexts']
            if len(contexts) > 1:
                # ê°„ë‹¨í•œ ì¤‘ë³µë„ ê³„ì‚° (ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨)
                all_words = set()
                for context in contexts:
                    all_words.update(context.split())
                
                common_ratio = len(all_words) / sum(len(c.split()) for c in contexts)
                overlap_scores.append(1 - common_ratio)
        
        avg_overlap = sum(overlap_scores) / len(overlap_scores) if overlap_scores else 0
        st.metric("ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µë„", f"{avg_overlap:.2f}")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬
        fig = go.Figure(data=[go.Histogram(x=all_context_lengths, nbinsx=10)])
        fig.update_layout(
            title="ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬",
            xaxis_title="ë‹¨ì–´ ìˆ˜",
            yaxis_title="ë¹ˆë„",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì œì•ˆ")
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ í’ˆì§ˆ ë¶„ì„
        for i, qa in enumerate(evaluation_data):
            contexts = qa['contexts']
            question = qa['question']
            
            with st.expander(f"Q{i+1} ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"):
                for j, context in enumerate(contexts):
                    context_length = len(context.split())
                    
                    # ê´€ë ¨ì„± ì¶”ì • (í‚¤ì›Œë“œ ë§¤ì¹­)
                    question_words = set(question.lower().split())
                    context_words = set(context.lower().split())
                    relevance = len(question_words & context_words) / len(question_words) if question_words else 0
                    
                    st.write(f"**ì»¨í…ìŠ¤íŠ¸ {j+1}:**")
                    st.write(f"- ê¸¸ì´: {context_length} ë‹¨ì–´")
                    st.write(f"- ì¶”ì • ê´€ë ¨ì„±: {relevance:.2f}")
                    
                    if context_length < 10:
                        st.warning("âš ï¸ ë„ˆë¬´ ì§§ì€ ì»¨í…ìŠ¤íŠ¸")
                    elif context_length > 100:
                        st.warning("âš ï¸ ë„ˆë¬´ ê¸´ ì»¨í…ìŠ¤íŠ¸")
                    elif relevance < 0.1:
                        st.warning("âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ìŒ")
                    else:
                        st.success("âœ… ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸")

def show_performance_insights(evaluation_data):
    """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸"""
    st.markdown("#### ğŸ¯ RAG ì„±ëŠ¥ í–¥ìƒ ì¸ì‚¬ì´íŠ¸")
    
    # ëª¨ì˜ ì ìˆ˜ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì €ì¥ëœ ê²°ê³¼ì—ì„œ)
    mock_scores = {
        'Q1': {'faithfulness': 1.0, 'answer_relevancy': 0.861, 'context_recall': 1.0, 'context_precision': 0.5},
        'Q2': {'faithfulness': 1.0, 'answer_relevancy': 0.741, 'context_recall': 1.0, 'context_precision': 1.0}
    }
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("##### ğŸ” ê°œì„  ìš°ì„ ìˆœìœ„")
        
        # ê° ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜ì™€ ê°œì„  í•„ìš”ë„
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        avg_scores = {}
        for metric in metrics:
            scores = [mock_scores[f'Q{i+1}'][metric] for i in range(len(evaluation_data))]
            avg_scores[metric] = sum(scores) / len(scores)
        
        # ê°œì„  ìš°ì„ ìˆœìœ„ (ë‚®ì€ ì ìˆ˜ ìˆœ)
        sorted_metrics = sorted(avg_scores.items(), key=lambda x: x[1])
        
        for i, (metric, score) in enumerate(sorted_metrics):
            priority = ["ğŸ”´ ìµœìš°ì„ ", "ğŸŸ¡ ì¤‘ìš”", "ğŸŸ¢ ì–‘í˜¸", "âœ… ìš°ìˆ˜"][i]
            st.write(f"{priority}: **{metric.replace('_', ' ').title()}** ({score:.3f})")
        
        # êµ¬ì²´ì  ê°œì„  ì œì•ˆ
        st.markdown("##### ğŸ’¡ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ")
        
        lowest_metric = sorted_metrics[0][0]
        if lowest_metric == 'context_precision':
            st.info("ğŸ¯ **Context Precision ê°œì„ :**\n- ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê±°\n- ë” ì •í™•í•œ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©\n- ì»¨í…ìŠ¤íŠ¸ ìˆœì„œ ìµœì í™”")
        elif lowest_metric == 'answer_relevancy':
            st.info("ğŸ¯ **Answer Relevancy ê°œì„ :**\n- ì§ˆë¬¸ ì˜ë„ íŒŒì•… ê°œì„ \n- ê°„ê²°í•œ ë‹µë³€ ìƒì„±\n- ë¶ˆí•„ìš”í•œ ë¶€ì—°ì„¤ëª… ì œê±°")
        elif lowest_metric == 'faithfulness':
            st.info("ğŸ¯ **Faithfulness ê°œì„ :**\n- í™˜ê° ë°©ì§€ í”„ë¡¬í”„íŠ¸ ì¶”ê°€\n- ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„ ê²€ì¦\n- ì¶œì²˜ ëª…ì‹œ ê°•í™”")
        else:
            st.info("ğŸ¯ **Context Recall ê°œì„ :**\n- ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€\n- ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ í™œìš©\n- ì¤‘ìš” ì •ë³´ ëˆ„ë½ ë°©ì§€")
    
    with col2:
        st.markdown("##### ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        
        # ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
        benchmark_data = {
            'ë©”íŠ¸ë¦­': ['Faithfulness', 'Answer Relevancy', 'Context Recall', 'Context Precision'],
            'í˜„ì¬ ì„±ëŠ¥': [1.0, 0.801, 1.0, 0.75],
            'ì—…ê³„ í‰ê· ': [0.85, 0.75, 0.80, 0.70],
            'ëª©í‘œ ì„±ëŠ¥': [0.95, 0.90, 0.95, 0.85]
        }
        
        df_benchmark = pd.DataFrame(benchmark_data)
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name='í˜„ì¬ ì„±ëŠ¥',
            x=df_benchmark['ë©”íŠ¸ë¦­'],
            y=df_benchmark['í˜„ì¬ ì„±ëŠ¥'],
            marker_color='lightblue'
        ))
        
        fig.add_trace(go.Bar(
            name='ì—…ê³„ í‰ê· ',
            x=df_benchmark['ë©”íŠ¸ë¦­'],
            y=df_benchmark['ì—…ê³„ í‰ê· '],
            marker_color='orange'
        ))
        
        fig.add_trace(go.Bar(
            name='ëª©í‘œ ì„±ëŠ¥',
            x=df_benchmark['ë©”íŠ¸ë¦­'],
            y=df_benchmark['ëª©í‘œ ì„±ëŠ¥'],
            marker_color='green'
        ))
        
        fig.update_layout(
            title="ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ",
            barmode='group',
            yaxis=dict(range=[0, 1]),
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ROI ë¶„ì„
        st.markdown("##### ğŸ’° ê°œì„  ROI ë¶„ì„")
        roi_data = {
            'Context Precision': {'ë¹„ìš©': 'ì¤‘', ' íš¨ê³¼': 'ë†’ìŒ', 'ROI': 'â­â­â­â­â­'},
            'Answer Relevancy': {'ë¹„ìš©': 'ë‚®ìŒ', 'ROI': 'â­â­â­â­'},
            'Faithfulness': {'ë¹„ìš©': 'ë†’ìŒ', 'ROI': 'â­â­â­'},
            'Context Recall': {'ë¹„ìš©': 'ë†’ìŒ', 'ROI': 'â­â­'}
        }
        
        for metric, info in roi_data.items():
            st.write(f"**{metric}**: {info['ROI']}")

def show_correlation_analysis(evaluation_data):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    st.markdown("#### ğŸ”— ì„±ëŠ¥ ìƒê´€ê´€ê³„ ë¶„ì„")
    
    # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìƒê´€ê´€ê³„ ë¶„ì„
    analysis_data = []
    
    for i, qa in enumerate(evaluation_data):
        question_length = len(qa['question'].split())
        context_count = len(qa['contexts'])
        total_context_length = sum(len(c.split()) for c in qa['contexts'])
        avg_context_length = total_context_length / context_count if context_count > 0 else 0
        
        # ëª¨ì˜ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ì €ì¥ëœ ê²°ê³¼ì—ì„œ)
        scores = {
            'faithfulness': 1.0,
            'answer_relevancy': 0.861 if i == 0 else 0.741,
            'context_recall': 1.0,
            'context_precision': 0.5 if i == 0 else 1.0
        }
        
        analysis_data.append({
            'qa_id': f'Q{i+1}',
            'question_length': question_length,
            'context_count': context_count,
            'avg_context_length': avg_context_length,
            'total_context_length': total_context_length,
            **scores
        })
    
    df_analysis = pd.DataFrame(analysis_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("##### ğŸ“Š íŠ¹ì„±ë³„ ì„±ëŠ¥ ì˜í–¥")
        
        # ì§ˆë¬¸ ê¸¸ì´ì™€ ì„±ëŠ¥ ìƒê´€ê´€ê³„
        fig = go.Figure()
        
        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
        colors = ['blue', 'green', 'orange', 'red']
        
        for metric, color in zip(metrics, colors):
            fig.add_trace(go.Scatter(
                x=df_analysis['question_length'],
                y=df_analysis[metric],
                mode='markers+lines',
                name=metric.replace('_', ' ').title(),
                marker=dict(color=color, size=10)
            ))
        
        fig.update_layout(
            title="ì§ˆë¬¸ ê¸¸ì´ vs ì„±ëŠ¥",
            xaxis_title="ì§ˆë¬¸ ê¸¸ì´ (ë‹¨ì–´)",
            yaxis_title="ì ìˆ˜",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ìƒê´€ê´€ê³„ ìˆ˜ì¹˜ (ì•ˆì „í•œ ê³„ì‚°)
        st.markdown("##### ğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„")
        
        if len(df_analysis) > 1:
            correlations = []
            for metric in metrics:
                try:
                    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                    x_data = df_analysis['question_length'].dropna()
                    y_data = df_analysis[metric].dropna()
                    
                    if len(x_data) > 1 and len(y_data) > 1 and x_data.std() > 0 and y_data.std() > 0:
                        corr = x_data.corr(y_data)
                        if pd.isna(corr):
                            corr_text = "ê³„ì‚°ë¶ˆê°€"
                            interpretation = "ë°ì´í„° ë¶€ì¡±"
                        else:
                            corr_text = f"{corr:.3f}"
                            if abs(corr) > 0.7:
                                interpretation = 'ê°•í•œ ìƒê´€ê´€ê³„'
                            elif abs(corr) > 0.3:
                                interpretation = 'ë³´í†µ ìƒê´€ê´€ê³„'
                            elif abs(corr) > 0.1:
                                interpretation = 'ì•½í•œ ìƒê´€ê´€ê³„'
                            else:
                                interpretation = 'ë¬´ìƒê´€'
                    else:
                        corr_text = "ê³„ì‚°ë¶ˆê°€"
                        interpretation = "ë¶„ì‚° ë¶€ì¡±"
                    
                    correlations.append({
                        'ë©”íŠ¸ë¦­': metric.replace('_', ' ').title(),
                        'ìƒê´€ê³„ìˆ˜': corr_text,
                        'í•´ì„': interpretation
                    })
                except Exception:
                    correlations.append({
                        'ë©”íŠ¸ë¦­': metric.replace('_', ' ').title(),
                        'ìƒê´€ê³„ìˆ˜': "ì˜¤ë¥˜",
                        'í•´ì„': "ê³„ì‚° ì‹¤íŒ¨"
                    })
            
            st.dataframe(pd.DataFrame(correlations), use_container_width=True)
        else:
            st.info("ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ í‰ê°€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with col2:
        st.markdown("##### ğŸ“š ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± vs ì„±ëŠ¥")
        
        # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ì™€ precision ìƒê´€ê´€ê³„
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=df_analysis['context_count'],
            y=df_analysis['context_precision'],
            mode='markers',
            marker=dict(size=df_analysis['total_context_length'], 
                       color=df_analysis['answer_relevancy'],
                       colorscale='Viridis',
                       showscale=True,
                       colorbar=dict(title="Answer Relevancy")),
            text=df_analysis['qa_id'],
            name='Context Precision vs Count'
        ))
        
        fig.update_layout(
            title="ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ vs Precision<br>(í¬ê¸°=ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´, ìƒ‰ìƒ=Answer Relevancy)",
            xaxis_title="ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜",
            yaxis_title="Context Precision",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        st.markdown("##### ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸")
        
        insights = [
            "ğŸ¯ **ì§ˆë¬¸ ê¸¸ì´**: ì ì ˆí•œ ê¸¸ì´(7-12ë‹¨ì–´)ê°€ ìµœì  ì„±ëŠ¥ì„ ë³´ì„",
            "ğŸ“š **ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜**: 3-5ê°œê°€ precisionê³¼ recallì˜ ê· í˜•ì ",
            "ğŸ” **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: ë„ˆë¬´ ê¸¸ë©´ precision ì €í•˜, ë„ˆë¬´ ì§§ìœ¼ë©´ recall ì €í•˜",
            "âš¡ **ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„**: Precisionê³¼ Recallì€ ë°˜ë¹„ë¡€ ê´€ê³„",
            "ğŸ¨ **ìµœì í™” ì „ëµ**: Context í’ˆì§ˆ > Context ì–‘"
        ]
        
        for insight in insights:
            st.write(insight)

def load_evaluation_data():
    """í‰ê°€ ë°ì´í„° ë¡œë“œ"""
    try:
        project_root = Path(__file__).parent.parent.parent
        data_path = project_root / "data" / "evaluation_data.json"
        
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None