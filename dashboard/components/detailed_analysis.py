"""
ìƒì„¸ ë¶„ì„ ì»´í¬ë„ŒíŠ¸
ê°œë³„ QA ìŒì˜ ìƒì„¸ í‰ê°€ ê²°ê³¼ ë¶„ì„
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
from pathlib import Path

def show_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ë©”ì¸ í™”ë©´"""
    st.header("ğŸ” ìƒì„¸ ë¶„ì„")
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    evaluation_data = load_evaluation_data()
    
    if not evaluation_data:
        st.warning("ğŸ“ ë¶„ì„í•  í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š QA ê°œë³„ ë¶„ì„", "ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„í¬", "ğŸ¯ íŒ¨í„´ ë¶„ì„"])
    
    with tab1:
        show_qa_analysis(evaluation_data)
    
    with tab2:
        show_metric_distribution(evaluation_data)
    
    with tab3:
        show_pattern_analysis(evaluation_data)

def show_qa_analysis(evaluation_data):
    """ê°œë³„ QA ë¶„ì„"""
    st.subheader("ğŸ“‹ ì§ˆë¬¸-ë‹µë³€ ìŒë³„ ìƒì„¸ ë¶„ì„")
    
    # QA ìŒ ì„ íƒ
    qa_options = [f"Q{i+1}: {qa['question'][:50]}..." for i, qa in enumerate(evaluation_data)]
    selected_qa_idx = st.selectbox("ë¶„ì„í•  QA ì„ íƒ", range(len(qa_options)), format_func=lambda x: qa_options[x])
    
    if selected_qa_idx is not None:
        qa_data = evaluation_data[selected_qa_idx]
        show_individual_qa_details(qa_data, selected_qa_idx + 1)

def show_individual_qa_details(qa_data, qa_number):
    """ê°œë³„ QA ìƒì„¸ ì •ë³´ í‘œì‹œ"""
    st.markdown(f"### ğŸ“ QA {qa_number} ìƒì„¸ ë¶„ì„")
    
    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ¤” ì§ˆë¬¸")
        st.info(qa_data['question'])
        
        st.markdown("#### ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
        st.success(qa_data['answer'])
    
    with col2:
        st.markdown("#### ğŸ“š ì œê³µëœ ì»¨í…ìŠ¤íŠ¸")
        for i, context in enumerate(qa_data['contexts'], 1):
            st.markdown(f"**ì»¨í…ìŠ¤íŠ¸ {i}:**")
            st.text_area(f"context_{i}", context, height=80, disabled=True, key=f"context_{qa_number}_{i}")
        
        st.markdown("#### âœ… ì •ë‹µ (Ground Truth)")
        st.info(qa_data['ground_truth'])
    
    # ê°€ìƒì˜ ê°œë³„ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ìµœì‹  í‰ê°€ ê²°ê³¼ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
    st.markdown("#### ğŸ“Š ì´ QAì˜ í‰ê°€ ì ìˆ˜")
    
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì €ì¥ëœ ìƒì„¸ ê²°ê³¼ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
    mock_scores = {
        'faithfulness': 1.0 if qa_number == 1 else 1.0,
        'answer_relevancy': 0.861 if qa_number == 1 else 0.741,
        'context_recall': 1.0,
        'context_precision': 0.5 if qa_number == 1 else 1.0
    }
    
    score_cols = st.columns(4)
    for i, (metric, score) in enumerate(mock_scores.items()):
        with score_cols[i]:
            color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
            st.metric(
                label=metric.replace('_', ' ').title(),
                value=f"{score:.3f}",
                delta=f"{score - 0.5:.3f}"
            )
    
    # ì ìˆ˜ ì‹œê°í™”
    show_qa_score_chart(mock_scores, qa_number)
    
    # í‰ê°€ ê·¼ê±° (ëª¨ì˜ ë°ì´í„°)
    show_evaluation_reasoning(qa_number)

def show_qa_score_chart(scores, qa_number):
    """ê°œë³„ QA ì ìˆ˜ ì°¨íŠ¸"""
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ì‹œê°í™”")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°” ì°¨íŠ¸
        metrics = list(scores.keys())
        values = list(scores.values())
        
        fig = go.Figure(data=[
            go.Bar(x=metrics, y=values, 
                  marker_color=['green' if v >= 0.8 else 'orange' if v >= 0.6 else 'red' for v in values])
        ])
        
        fig.update_layout(
            title=f"QA {qa_number} ë©”íŠ¸ë¦­ ì ìˆ˜",
            yaxis_title="ì ìˆ˜",
            yaxis=dict(range=[0, 1]),
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ê²Œì´ì§€ ì°¨íŠ¸ (í‰ê·  ì ìˆ˜)
        avg_score = sum(values) / len(values)
        
        fig = go.Figure(go.Indicator(
            mode = "gauge+number+delta",
            value = avg_score,
            domain = {'x': [0, 1], 'y': [0, 1]},
            title = {'text': f"QA {qa_number} ì¢…í•© ì ìˆ˜"},
            delta = {'reference': 0.5},
            gauge = {
                'axis': {'range': [None, 1]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 0.5], 'color': "lightgray"},
                    {'range': [0.5, 0.8], 'color': "yellow"},
                    {'range': [0.8, 1], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 0.9
                }
            }
        ))
        
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

def show_evaluation_reasoning(qa_number):
    """í‰ê°€ ê·¼ê±° í‘œì‹œ (ëª¨ì˜ ë°ì´í„°)"""
    st.markdown("#### ğŸ§  í‰ê°€ ê·¼ê±°")
    
    # ì‹¤ì œë¡œëŠ” RAGAS tracesì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
    mock_reasoning = {
        1: {
            'faithfulness': "ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤. ì˜ì¡´ì„± ê·œì¹™ì— ëŒ€í•œ ì„¤ëª…ì´ ì •í™•í•©ë‹ˆë‹¤.",
            'answer_relevancy': "ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ê³  ìˆìœ¼ë‚˜, ì¼ë¶€ ì¶”ê°€ì ì¸ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ê´€ë ¨ì„±ì´ ì•½ê°„ ë‚®ìŠµë‹ˆë‹¤.",
            'context_recall': "Ground truthì˜ ëª¨ë“  ì •ë³´ê°€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤.",
            'context_precision': "ì²« ë²ˆì§¸ ì»¨í…ìŠ¤íŠ¸ëŠ” ì¼ë°˜ì ì¸ ì„¤ëª…ì´ê³ , ë‘ ë²ˆì§¸ê°€ í•µì‹¬ ë‹µë³€ì„ í¬í•¨í•©ë‹ˆë‹¤."
        },
        2: {
            'faithfulness': "ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ì™€ ì™„ì „íˆ ì¼ì¹˜í•˜ë©° ì¶”ê°€ ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            'answer_relevancy': "ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‹µë³€ì´ì§€ë§Œ ì¼ë¶€ í‘œí˜„ì´ ë‹¤ì†Œ ë³µì¡í•©ë‹ˆë‹¤.",
            'context_recall': "Ground truthì˜ ëª¨ë“  ìš”ì†Œê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ë©ë‹ˆë‹¤.",
            'context_precision': "ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ê°€ ë‹µë³€ì— ì§ì ‘ì ìœ¼ë¡œ ê¸°ì—¬í•©ë‹ˆë‹¤."
        }
    }
    
    reasoning = mock_reasoning.get(qa_number, mock_reasoning[1])
    
    for metric, explanation in reasoning.items():
        with st.expander(f"ğŸ“ {metric.replace('_', ' ').title()} í‰ê°€ ê·¼ê±°"):
            st.write(explanation)

def show_metric_distribution(evaluation_data):
    """ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„"""
    st.subheader("ğŸ“Š ë©”íŠ¸ë¦­ ë¶„í¬ ë¶„ì„")
    
    # ëª¨ì˜ ì ìˆ˜ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì €ì¥ëœ ê²°ê³¼ì—ì„œ)
    mock_data = {
        'QA': [f'Q{i+1}' for i in range(len(evaluation_data))],
        'faithfulness': [1.0, 1.0],
        'answer_relevancy': [0.861, 0.741],
        'context_recall': [1.0, 1.0], 
        'context_precision': [0.5, 1.0]
    }
    
    df = pd.DataFrame(mock_data)
    
    # íˆíŠ¸ë§µ
    st.markdown("#### ğŸ”¥ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ")
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
    heatmap_data = df[metrics].values
    
    fig = go.Figure(data=go.Heatmap(
        z=heatmap_data,
        x=metrics,
        y=df['QA'],
        colorscale='RdYlGn',
        colorbar=dict(title="ì ìˆ˜")
    ))
    
    fig.update_layout(
        title="QAë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ",
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # ë¶„í¬ ì°¨íŠ¸
    st.markdown("#### ğŸ“ˆ ì ìˆ˜ ë¶„í¬")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ë°•ìŠ¤í”Œë¡¯
        fig = go.Figure()
        
        for metric in metrics:
            fig.add_trace(go.Box(
                y=df[metric],
                name=metric.replace('_', ' ').title(),
                boxpoints='all'
            ))
        
        fig.update_layout(
            title="ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë¶„í¬",
            yaxis_title="ì ìˆ˜",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # íˆìŠ¤í† ê·¸ë¨
        selected_metric = st.selectbox("ë¶„í¬ë¥¼ ë³¼ ë©”íŠ¸ë¦­ ì„ íƒ", metrics)
        
        fig = go.Figure(data=[go.Histogram(x=df[selected_metric], nbinsx=10)])
        
        fig.update_layout(
            title=f"{selected_metric.replace('_', ' ').title()} ì ìˆ˜ ë¶„í¬",
            xaxis_title="ì ìˆ˜",
            yaxis_title="ë¹ˆë„",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)

def show_pattern_analysis(evaluation_data):
    """íŒ¨í„´ ë¶„ì„"""
    st.subheader("ğŸ¯ íŒ¨í„´ ë¶„ì„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“ ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„")
        
        # ì§ˆë¬¸ ê¸¸ì´ ë¶„ì„
        question_lengths = [len(qa['question'].split()) for qa in evaluation_data]
        avg_length = sum(question_lengths) / len(question_lengths)
        
        st.metric("í‰ê·  ì§ˆë¬¸ ê¸¸ì´", f"{avg_length:.1f} ë‹¨ì–´")
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        question_types = []
        for qa in evaluation_data:
            question = qa['question'].lower()
            if 'ë¬´ì—‡' in question:
                question_types.append('ì •ì˜í˜•')
            elif 'ì–´ë–»ê²Œ' in question:
                question_types.append('ë°©ë²•í˜•')
            elif 'ì™œ' in question:
                question_types.append('ì´ìœ í˜•')
            else:
                question_types.append('ê¸°íƒ€')
        
        type_counts = pd.Series(question_types).value_counts()
        
        fig = go.Figure(data=[go.Pie(labels=type_counts.index, values=type_counts.values)])
        fig.update_layout(title="ì§ˆë¬¸ ìœ í˜• ë¶„í¬", height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ“š ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± ë¶„ì„")
        
        # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ ë¶„ì„
        context_counts = [len(qa['contexts']) for qa in evaluation_data]
        avg_contexts = sum(context_counts) / len(context_counts)
        
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜", f"{avg_contexts:.1f} ê°œ")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
        all_context_lengths = []
        for qa in evaluation_data:
            for context in qa['contexts']:
                all_context_lengths.append(len(context.split()))
        
        avg_context_length = sum(all_context_lengths) / len(all_context_lengths)
        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´", f"{avg_context_length:.1f} ë‹¨ì–´")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬
        fig = go.Figure(data=[go.Histogram(x=all_context_lengths, nbinsx=10)])
        fig.update_layout(
            title="ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬",
            xaxis_title="ë‹¨ì–´ ìˆ˜",
            yaxis_title="ë¹ˆë„",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ì„±ëŠ¥ ìƒê´€ê´€ê³„ ë¶„ì„
    st.markdown("#### ğŸ”— ì„±ëŠ¥ ìƒê´€ê´€ê³„")
    
    # ëª¨ì˜ ìƒê´€ê´€ê³„ ë°ì´í„°
    st.info("ğŸ“Š ì§ˆë¬¸ ê¸¸ì´ì™€ ì„±ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„, ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ì™€ precision ê°„ì˜ ê´€ê³„ ë“±ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def load_evaluation_data():
    """í‰ê°€ ë°ì´í„° ë¡œë“œ"""
    try:
        project_root = Path(__file__).parent.parent.parent
        data_path = project_root / "data" / "evaluation_data.json"
        
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None