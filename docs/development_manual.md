# ğŸ› ï¸ RAGAS í‰ê°€ ì‹œìŠ¤í…œ ê°œë°œ ë§¤ë‰´ì–¼

## ğŸ“– ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [ê°œë°œ í™˜ê²½ ì„¤ì •](#-ê°œë°œ-í™˜ê²½-ì„¤ì •)
3. [í•µì‹¬ ê¸°ëŠ¥ í™•ì¥](#-í•µì‹¬-ê¸°ëŠ¥-í™•ì¥)
4. [ë¡œì»¬ LLM í†µí•©](#-ë¡œì»¬-llm-í†µí•©)
5. [í…ŒìŠ¤íŠ¸ ì „ëµ](#-í…ŒìŠ¤íŠ¸-ì „ëµ)
6. [ë°°í¬ ê°€ì´ë“œ](#-ë°°í¬-ê°€ì´ë“œ)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

### Clean Architecture êµ¬ì¡°

```
src/
â”œâ”€â”€ domain/           # ğŸ›ï¸ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”œâ”€â”€ entities/     # EvaluationResult, EvaluationData
â”‚   â”œâ”€â”€ value_objects/# Metrics
â”‚   â””â”€â”€ exceptions/   # EvaluationError
â”œâ”€â”€ application/      # ğŸ”§ ìœ ìŠ¤ì¼€ì´ìŠ¤
â”‚   â”œâ”€â”€ use_cases/    # RunEvaluationUseCase
â”‚   â””â”€â”€ ports/        # ì¸í„°í˜ì´ìŠ¤ ì •ì˜
â”œâ”€â”€ infrastructure/   # ğŸ”Œ ì™¸ë¶€ ì‹œìŠ¤í…œ ì–´ëŒ‘í„°
â”‚   â”œâ”€â”€ evaluation/   # RagasEvalAdapter
â”‚   â”œâ”€â”€ llm/          # GeminiAdapter, LocalLLMAdapter
â”‚   â””â”€â”€ repository/   # FileRepositoryAdapter, DBAdapter
â””â”€â”€ presentation/     # ğŸ–¥ï¸ UI ë ˆì´ì–´
    â”œâ”€â”€ main.py       # CLI ì¸í„°í˜ì´ìŠ¤
    â””â”€â”€ web/          # Streamlit ëŒ€ì‹œë³´ë“œ
```

### ì˜ì¡´ì„± ê·œì¹™

**í•µì‹¬ ì›ì¹™**: ì˜ì¡´ì„±ì€ í•­ìƒ ì™¸ë¶€â†’ë‚´ë¶€ ë°©í–¥

```
Presentation â†’ Application â†’ Domain â† Infrastructure
```

- **Domain**: ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ (ìˆœìˆ˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
- **Application**: Domainë§Œ ì˜ì¡´ (ìœ ìŠ¤ì¼€ì´ìŠ¤ êµ¬í˜„)
- **Infrastructure**: Application í¬íŠ¸ êµ¬í˜„
- **Presentation**: Application ìœ ìŠ¤ì¼€ì´ìŠ¤ ì‚¬ìš©

## ğŸš€ ê°œë°œ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë„êµ¬

```bash
# Python 3.11+ ì„¤ì¹˜ í™•ì¸
python --version

# í”„ë¡œì íŠ¸ í´ë¡  ë° í™˜ê²½ ì„¤ì •
git clone <repository>
cd ragas-test
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -e ".[dev]"

# ì½”ë“œ í’ˆì§ˆ ë„êµ¬
pip install black isort flake8 mypy pre-commit
pre-commit install
```

### IDE ì„¤ì • (VS Code)

```json
// .vscode/settings.json
{
    "python.defaultInterpreterPath": "./.venv/bin/python",
    "python.testing.pytestEnabled": true,
    "python.testing.pytestArgs": ["tests"],
    "python.linting.enabled": true,
    "python.linting.flake8Enabled": true,
    "python.formatting.provider": "black"
}
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# ê°œë°œìš© ì„¤ì •
GEMINI_API_KEY=your_api_key
DEBUG_MODE=True
VERBOSE_LOGGING=True
```

## ğŸ”§ í•µì‹¬ ê¸°ëŠ¥ í™•ì¥

### 1. ìƒˆë¡œìš´ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€

#### Step 1: ë„ë©”ì¸ ì—”í‹°í‹° í™•ì¥

```python
# src/domain/entities/evaluation_result.py
@dataclass
class EvaluationResult:
    faithfulness: float
    answer_relevancy: float
    context_recall: float
    context_precision: float
    ragas_score: float
    # ìƒˆ ë©”íŠ¸ë¦­ ì¶”ê°€
    semantic_similarity: float = 0.0
    
    def __post_init__(self):
        # ê²€ì¦ ë¡œì§ì— ìƒˆ ë©”íŠ¸ë¦­ ì¶”ê°€
        self._validate_score("semantic_similarity", self.semantic_similarity)
```

#### Step 2: RAGAS ì–´ëŒ‘í„° ìˆ˜ì •

```python
# src/infrastructure/evaluation/ragas_adapter.py
from ragas.metrics import semantic_similarity

class RagasEvalAdapter:
    def __init__(self):
        self.metrics = [
            faithfulness,
            answer_relevancy, 
            context_recall,
            context_precision,
            semantic_similarity  # ìƒˆ ë©”íŠ¸ë¦­ ì¶”ê°€
        ]
    
    def _extract_scores(self, result) -> dict:
        # ìƒˆ ë©”íŠ¸ë¦­ ì ìˆ˜ ì¶”ì¶œ ë¡œì§ ì¶”ê°€
        return {
            # ê¸°ì¡´ ë©”íŠ¸ë¦­ë“¤...
            "semantic_similarity": self._get_metric_score(result, "semantic_similarity")
        }
```

#### Step 3: í”„ë ˆì  í…Œì´ì…˜ ì—…ë°ì´íŠ¸

```python
# src/presentation/web/main.py
def display_metrics(result: EvaluationResult):
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric("Faithfulness", f"{result.faithfulness:.3f}")
    # ... ê¸°ì¡´ ë©”íŠ¸ë¦­ë“¤
    with col5:
        st.metric("Semantic Similarity", f"{result.semantic_similarity:.3f}")
```

### 2. ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤ ì—°ë™

#### PostgreSQL ì–´ëŒ‘í„° ì˜ˆì‹œ

```python
# src/infrastructure/repository/postgres_adapter.py
import psycopg2
from src.application.ports.repository import EvaluationRepositoryPort
from src.domain.entities.evaluation_data import EvaluationData

class PostgresRepositoryAdapter(EvaluationRepositoryPort):
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
    
    def load_data(self) -> List[EvaluationData]:
        with psycopg2.connect(self.connection_string) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT question, contexts, answer, ground_truth 
                FROM evaluation_datasets 
                WHERE active = true
            """)
            
            return [
                EvaluationData(
                    question=row[0],
                    contexts=row[1], 
                    answer=row[2],
                    ground_truth=row[3]
                ) for row in cursor.fetchall()
            ]
```

#### ì˜ì¡´ì„± ì£¼ì… ìˆ˜ì •

```python
# src/presentation/web/main.py
def create_use_case():
    # í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì–´ëŒ‘í„° ì„ íƒ
    if os.getenv("USE_POSTGRES", "false").lower() == "true":
        repository = PostgresRepositoryAdapter(os.getenv("DATABASE_URL"))
    else:
        repository = FileRepositoryAdapter("data/evaluation_data.json")
    
    return RunEvaluationUseCase(
        repository_port=repository,
        llm_port=GeminiAdapter(),
        evaluation_runner=RagasEvalAdapter()
    )
```

## ğŸ¤– ë¡œì»¬ LLM í†µí•©

### Ollama í†µí•©

#### 1. Ollama ì–´ëŒ‘í„° êµ¬í˜„

```python
# src/infrastructure/llm/ollama_adapter.py
from langchain_community.llms import Ollama
from src.application.ports.llm import LlmPort

class OllamaAdapter(LlmPort):
    def __init__(self, model_name: str = "qwen2.5:14b", base_url: str = "http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url
    
    def get_llm(self):
        return Ollama(
            model=self.model_name,
            base_url=self.base_url,
            temperature=0.1
        )
```

#### 2. í™˜ê²½ ì„¤ì •

```bash
# .env íŒŒì¼
USE_LOCAL_LLM=True
LOCAL_LLM_TYPE=ollama
LOCAL_LLM_MODEL=qwen2.5:14b
LOCAL_LLM_BASE_URL=http://localhost:11434
```

#### 3. ì–´ëŒí„° íŒ©í† ë¦¬ íŒ¨í„´

```python
# src/infrastructure/llm/llm_factory.py
def create_llm_adapter() -> LlmPort:
    if os.getenv("USE_LOCAL_LLM", "false").lower() == "true":
        llm_type = os.getenv("LOCAL_LLM_TYPE", "ollama")
        
        if llm_type == "ollama":
            return OllamaAdapter(
                model_name=os.getenv("LOCAL_LLM_MODEL", "qwen2.5:14b"),
                base_url=os.getenv("LOCAL_LLM_BASE_URL", "http://localhost:11434")
            )
        elif llm_type == "hcx":
            return HCXAdapter(
                model_name=os.getenv("LOCAL_LLM_MODEL", "hcx-005"),
                endpoint=os.getenv("LOCAL_LLM_ENDPOINT")
            )
    else:
        return GeminiAdapter()
```

### HCX-005 í†µí•© (íì‡„ë§ í™˜ê²½)

```python
# src/infrastructure/llm/hcx_adapter.py
import requests
from langchain_core.language_models import BaseLLM

class HCXAdapter(LlmPort):
    def __init__(self, model_name: str, endpoint: str, api_key: str = None):
        self.model_name = model_name
        self.endpoint = endpoint
        self.api_key = api_key
    
    def get_llm(self):
        return HCXLanguageModel(
            model_name=self.model_name,
            endpoint=self.endpoint,
            api_key=self.api_key,
            temperature=0.1
        )

class HCXLanguageModel(BaseLLM):
    def __init__(self, model_name: str, endpoint: str, api_key: str = None, **kwargs):
        super().__init__(**kwargs)
        self.model_name = model_name
        self.endpoint = endpoint
        self.api_key = api_key
    
    def _call(self, prompt: str, stop: list = None, **kwargs) -> str:
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
            
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "temperature": self.temperature,
            "max_tokens": 4096
        }
        
        response = requests.post(f"{self.endpoint}/generate", json=payload, headers=headers)
        response.raise_for_status()
        
        return response.json()["response"]
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### í…ŒìŠ¤íŠ¸ ì•„í‚¤í…ì²˜

```
tests/
â”œâ”€â”€ domain/           # ìˆœìˆ˜ ìœ ë‹› í…ŒìŠ¤íŠ¸
â”œâ”€â”€ application/      # ëª¨í‚¹ ê¸°ë°˜ ìœ ìŠ¤ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ infrastructure/   # í†µí•© í…ŒìŠ¤íŠ¸ + ëª¨í‚¹
â””â”€â”€ presentation/     # E2E í…ŒìŠ¤íŠ¸
```

### í•µì‹¬ í…ŒìŠ¤íŠ¸ íŒ¨í„´

#### 1. ë„ë©”ì¸ í…ŒìŠ¤íŠ¸ (ìˆœìˆ˜ ìœ ë‹›)

```python
# tests/domain/test_evaluation_result.py
def test_evaluation_result_validation():
    """ì ìˆ˜ ë²”ìœ„ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    with pytest.raises(ValueError, match="ì ìˆ˜ëŠ” 0.0ê³¼ 1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤"):
        EvaluationResult(
            faithfulness=1.5,  # ì˜ëª»ëœ ê°’
            answer_relevancy=0.8,
            context_recall=0.7,
            context_precision=0.9,
            ragas_score=0.85
        )
```

#### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸ (ëª¨í‚¹)

```python
# tests/application/test_run_evaluation.py
@pytest.fixture
def mock_dependencies():
    return {
        "repository_port": Mock(spec=EvaluationRepositoryPort),
        "llm_port": Mock(spec=LlmPort), 
        "evaluation_runner": Mock(spec=EvaluationPort)
    }

def test_run_evaluation_success(mock_dependencies):
    # Given
    mock_dependencies["repository_port"].load_data.return_value = [sample_data]
    mock_dependencies["evaluation_runner"].evaluate.return_value = expected_result
    
    # When
    use_case = RunEvaluationUseCase(**mock_dependencies)
    result = use_case.execute()
    
    # Then
    assert result.ragas_score == expected_result["ragas_score"]
```

#### 3. ì¸í”„ë¼ í…ŒìŠ¤íŠ¸ (í†µí•© + ëª¨í‚¹)

```python
# tests/infrastructure/test_ragas_adapter.py
@patch('src.infrastructure.evaluation.ragas_adapter.evaluate')
def test_ragas_evaluation(mock_evaluate):
    # RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨í‚¹
    mock_result = create_mock_ragas_result()
    mock_evaluate.return_value = mock_result
    
    adapter = RagasEvalAdapter()
    result = adapter.evaluate(test_dataset, test_llm)
    
    assert result["faithfulness"] == pytest.approx(0.85, abs=0.01)
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
pytest

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest --cov=src --cov-report=html

# íŠ¹ì • ë ˆì´ì–´ í…ŒìŠ¤íŠ¸
pytest tests/domain/
pytest tests/application/
pytest tests/infrastructure/

# ìë™ ë¦¬í¬íŠ¸ ìƒì„±
python scripts/generate_test_report.py
```

## ğŸš€ ë°°í¬ ê°€ì´ë“œ

### Docker ë°°í¬

#### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/
COPY data/ ./data/
COPY run_dashboard.py .

EXPOSE 8501
CMD ["streamlit", "run", "src/presentation/web/main.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

#### docker-compose.yml

```yaml
version: '3.8'
services:
  ragas-eval:
    build: .
    ports:
      - "8501:8501"
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./reports:/app/reports
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    
volumes:
  ollama_data:
```

### í”„ë¡œë•ì…˜ ì„¤ì •

#### í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬

```bash
# .env.production
GEMINI_API_KEY=prod_api_key
DEBUG_MODE=False
VERBOSE_LOGGING=False
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0

# ë³´ì•ˆ ì„¤ì •
SSL_ENABLED=True
SSL_CERT_PATH=/etc/ssl/certs/ragas.crt
SSL_KEY_PATH=/etc/ssl/private/ragas.key
```

#### CI/CD íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/deploy.yml
name: Deploy
on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - run: pip install -e ".[dev]"
      - run: pytest --cov=src --cov-fail-under=99
      
  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Deploy to production
        run: |
          docker build -t ragas-eval:latest .
          docker push ${{ secrets.REGISTRY_URL }}/ragas-eval:latest
```

## ğŸ“š ì¶”ê°€ ì°¸ê³ ìë£Œ

### í•µì‹¬ ë¬¸ì„œ

- **[Clean Architecture ê°€ì´ë“œ](./clean_architecture_summary.md)**: ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª…
- **[RAGAS ë©”íŠ¸ë¦­ ê°€ì´ë“œ](./RAGAS_METRICS.md)**: í‰ê°€ ì§€í‘œ ì„¤ëª…
- **[í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸](../reports/)**: ìë™ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ë¶„ì„

### ìœ ìš©í•œ ìŠ¤í¬ë¦½íŠ¸

```bash
# ë°ì´í„° ê²€ì¦
python scripts/analysis/validate_dataset.py data/your_data.json

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§  
python scripts/monitor_api_usage.py

# í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
python scripts/generate_test_report.py
```

### ê°œë°œ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

1. **í…ŒìŠ¤íŠ¸ ìš°ì„  ê°œë°œ**: ìƒˆ ê¸°ëŠ¥ êµ¬í˜„ ì „ í…ŒìŠ¤íŠ¸ ì‘ì„±
2. **ì˜ì¡´ì„± ì—­ì „ ì¤€ìˆ˜**: ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëŠìŠ¨í•œ ê²°í•©
3. **ë‹¨ì¼ ì±…ì„ ì›ì¹™**: ê° í´ë˜ìŠ¤ëŠ” í•˜ë‚˜ì˜ ì±…ì„ë§Œ
4. **ì»¤ë²„ë¦¬ì§€ ìœ ì§€**: 99% ì´ìƒ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ìœ ì§€
5. **ë¬¸ì„œí™”**: ì½”ë“œ ë³€ê²½ ì‹œ ë¬¸ì„œë„ í•¨ê»˜ ì—…ë°ì´íŠ¸

---

ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ë©´ [GitHub Issues](https://github.com/your-org/ragas-test/issues)ë¥¼ í†µí•´ ë¬¸ì˜í•´ì£¼ì„¸ìš”.