# íì‡„ë§ í™˜ê²½ì—ì„œì˜ LLM ë° ì„ë² ë”© ëª¨ë¸ í†µí•© ê°€ì´ë“œ

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [íì‡„ë§ í™˜ê²½ ìš”êµ¬ì‚¬í•­](#2-íì‡„ë§-í™˜ê²½-ìš”êµ¬ì‚¬í•­)
3. [HCX-005 ëª¨ë¸ í†µí•©](#3-hcx-005-ëª¨ë¸-í†µí•©)
4. [BGE-M3 ì„ë² ë”© ëª¨ë¸ í†µí•©](#4-bge-m3-ì„ë² ë”©-ëª¨ë¸-í†µí•©)
5. [ì˜¤í”„ë¼ì¸ ì˜ì¡´ì„± ì„¤ì¹˜](#5-ì˜¤í”„ë¼ì¸-ì˜ì¡´ì„±-ì„¤ì¹˜)
6. [ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ í™˜ê²½ ì„¤ì •](#6-ë„¤íŠ¸ì›Œí¬-ê²©ë¦¬-í™˜ê²½-ì„¤ì •)
7. [ì„±ëŠ¥ ìµœì í™”](#7-ì„±ëŠ¥-ìµœì í™”)
8. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#8-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## 1. ê°œìš”

ì´ ë¬¸ì„œëŠ” ì™¸ë¶€ ì¸í„°ë„· ì ‘ì†ì´ ì œí•œëœ íì‡„ë§(Air-gapped) í™˜ê²½ì—ì„œ RAGTrace í”„ë¡œì íŠ¸ì— ë‹¤ìŒ ëª¨ë¸ë“¤ì„ í†µí•©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤:

- **LLM**: HyperCLOVA X HCX-005 ëª¨ë¸
- **ì„ë² ë”©**: BGE-M3 ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸

### 1.1 íì‡„ë§ í™˜ê²½ì˜ íŠ¹ì§•

- ì™¸ë¶€ ì¸í„°ë„· ì—°ê²° ë¶ˆê°€
- ëª¨ë¸ íŒŒì¼ ë¡œì»¬ ì €ì¥ í•„ìš”
- API í‚¤ ê¸°ë°˜ ì™¸ë¶€ ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€
- ëª¨ë“  ì˜ì¡´ì„± ì‚¬ì „ ì„¤ì¹˜ í•„ìš”

### 1.2 ì•„í‚¤í…ì²˜ ì ‘ê·¼ë²•

RAGTraceì˜ Hexagonal Architectureë¥¼ í™œìš©í•˜ì—¬:
- ê¸°ì¡´ ì½”ë“œ ë³€ê²½ ìµœì†Œí™”
- ìƒˆë¡œìš´ ì–´ëŒ‘í„° ì¶”ê°€ë¡œ ëª¨ë¸ í†µí•©
- ëŸ°íƒ€ì„ ëª¨ë¸ êµì²´ ì§€ì›

---

## 2. íì‡„ë§ í™˜ê²½ ìš”êµ¬ì‚¬í•­

### 2.1 í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

```yaml
ìµœì†Œ ì‚¬ì–‘:
  CPU: 8ì½”ì–´ ì´ìƒ
  RAM: 32GB ì´ìƒ
  Storage: 100GB ì´ìƒ (ëª¨ë¸ íŒŒì¼ ì €ì¥ìš©)
  GPU: NVIDIA RTX 3080 ì´ìƒ (ì„ íƒì‚¬í•­, ì„±ëŠ¥ í–¥ìƒìš©)

ê¶Œì¥ ì‚¬ì–‘:
  CPU: 16ì½”ì–´ ì´ìƒ
  RAM: 64GB ì´ìƒ
  Storage: 500GB ì´ìƒ SSD
  GPU: NVIDIA RTX 4090 ë˜ëŠ” A100
```

### 2.2 ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­

```bash
# ê¸°ë³¸ í™˜ê²½
Python 3.11+
CUDA 11.8+ (NVIDIA GPU ì‚¬ìš© ì‹œ)
Docker 20.10+ (ì»¨í…Œì´ë„ˆ í™˜ê²½ ì‹œ)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì˜¤í”„ë¼ì¸ ì„¤ì¹˜ í•„ìš”)
torch>=2.0.0
transformers>=4.36.0
sentence-transformers>=2.2.2

# GPU ê°€ì† ì§€ì›
CUDA: NVIDIA GPU (RTX ì‹œë¦¬ì¦ˆ, Tesla, A100 ë“±)
MPS: Apple Silicon (M1, M2, M3 Pro/Max/Ultra)
CPU: ëª¨ë“  x86_64, ARM64 í”„ë¡œì„¸ì„œ
```

### 2.3 ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì¸í„°ë„· ì—°ê²° í™˜ê²½ì—ì„œ ì‚¬ì „ ì¤€ë¹„)

```bash
# ë³„ë„ ì¸í„°ë„· ì—°ê²° ì‹œìŠ¤í…œì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
mkdir -p /models/bge-m3
mkdir -p /models/hcx-005

# BGE-M3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python -c "
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id='BAAI/bge-m3',
    local_dir='/models/bge-m3',
    local_dir_use_symlinks=False
)
"

# HCX-005 ëª¨ë¸ íŒŒì¼ (NAVER Cloud Platformì—ì„œ ë‹¤ìš´ë¡œë“œ)
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” NAVERì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ
```

---

## 3. HCX-005 ëª¨ë¸ í†µí•©

### 3.1 HCX-005 ì–´ëŒ‘í„° êµ¬í˜„

`src/infrastructure/llm/hcx_offline_adapter.py` íŒŒì¼ ìƒì„±:

```python
"""
íì‡„ë§ í™˜ê²½ì—ì„œì˜ HCX-005 ëª¨ë¸ ì–´ëŒ‘í„°
ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ìˆ˜í–‰
"""

import os
import json
import torch
from typing import List, Optional, Dict, Any
from pathlib import Path

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TextGenerationPipeline,
    GenerationConfig
)
from langchain_core.language_models import BaseLLM
from langchain_core.outputs import LLMResult, Generation

from src.application.ports.llm import LlmPort


class HCXOfflineAdapter(LlmPort):
    """íì‡„ë§ í™˜ê²½ì—ì„œ HCX-005 ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì–´ëŒ‘í„°"""
    
    def __init__(
        self,
        model_path: str,
        device: str = "auto",
        max_length: int = 4096,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ):
        """
        Args:
            model_path: ë¡œì»¬ì— ì €ì¥ëœ HCX-005 ëª¨ë¸ ê²½ë¡œ
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("cpu", "cuda", "auto")
            max_length: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„ (0.0-1.0)
            top_p: Nucleus sampling íŒŒë¼ë¯¸í„°
        """
        self.model_path = Path(model_path)
        self.device = self._get_device(device)
        self.max_length = max_length
        self.temperature = temperature
        self.top_p = top_p
        
        # ëª¨ë¸ ë¡œë”©
        print(f"ğŸ”„ HCX-005 ëª¨ë¸ ë¡œë”© ì¤‘... ({self.model_path})")
        self._load_model()
        print(f"âœ… HCX-005 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    def _get_device(self, device: str) -> str:
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _load_model(self):
        """ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ë¡œë”©"""
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                local_files_only=True,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë”©
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                local_files_only=True,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map=self.device if self.device == "cuda" else None
            )
            
            # ìƒì„± ì„¤ì •
            self.generation_config = GenerationConfig(
                max_length=self.max_length,
                temperature=self.temperature,
                top_p=self.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            self.pipeline = TextGenerationPipeline(
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1
            )
            
        except Exception as e:
            raise RuntimeError(f"HCX-005 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def generate_answer(self, question: str, contexts: List[str]) -> str:
        """
        ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            contexts: ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        prompt = self._create_prompt(question, contexts)
        
        try:
            # í† í°í™”
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            if self.device == "cuda":
                inputs = inputs.to("cuda")
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    generation_config=self.generation_config,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ì¶”ì¶œ
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.shape[1]:], 
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            print(f"âŒ HCX-005 ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_llm(self) -> BaseLLM:
        """LangChain í˜¸í™˜ LLM ê°ì²´ ë°˜í™˜"""
        return HCXLangChainLLM(self)
    
    def _create_prompt(self, question: str, contexts: List[str]) -> str:
        """HCX-005ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        context_text = "\n".join(f"ì°¸ê³ ìë£Œ {i+1}: {ctx}" for i, ctx in enumerate(contexts))
        
        prompt = f"""ë‹¤ìŒ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì°¸ê³ ìë£Œ:
{context_text}

ì§ˆë¬¸: {question}

ë‹µë³€: """
        return prompt
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": "HCX-005",
            "model_path": str(self.model_path),
            "device": self.device,
            "max_length": self.max_length,
            "temperature": self.temperature,
            "top_p": self.top_p
        }


class HCXLangChainLLM(BaseLLM):
    """LangChain í˜¸í™˜ì„ ìœ„í•œ HCX-005 ë˜í¼"""
    
    def __init__(self, hcx_adapter: HCXOfflineAdapter):
        super().__init__()
        self.hcx_adapter = hcx_adapter
    
    @property
    def _llm_type(self) -> str:
        return "hcx-005-offline"
    
    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ìƒì„± ê²°ê³¼ ë°˜í™˜"""
        generations = []
        
        for prompt in prompts:
            try:
                # HCX ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ì—†ì´)
                result = self._generate_single(prompt)
                generations.append([Generation(text=result)])
            except Exception as e:
                generations.append([Generation(text=f"ìƒì„± ì‹¤íŒ¨: {str(e)}")])
        
        return LLMResult(generations=generations)
    
    def _generate_single(self, prompt: str) -> str:
        """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ìƒì„±"""
        inputs = self.hcx_adapter.tokenizer.encode(prompt, return_tensors="pt")
        if self.hcx_adapter.device == "cuda":
            inputs = inputs.to("cuda")
        
        with torch.no_grad():
            outputs = self.hcx_adapter.model.generate(
                inputs,
                generation_config=self.hcx_adapter.generation_config
            )
        
        generated_text = self.hcx_adapter.tokenizer.decode(
            outputs[0][inputs.shape[1]:], 
            skip_special_tokens=True
        )
        
        return generated_text.strip()
```

### 3.2 HCX-005 ì„¤ì • íŒŒì¼

`config/hcx_config.yaml` ìƒì„±:

```yaml
# HCX-005 ì˜¤í”„ë¼ì¸ ëª¨ë¸ ì„¤ì •
hcx_offline:
  model_path: "/models/hcx-005"  # ë¡œì»¬ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
  device: "auto"                 # "cpu", "cuda", "auto"
  generation:
    max_length: 4096
    temperature: 0.7
    top_p: 0.9
    repetition_penalty: 1.1
  
  # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
  optimization:
    use_flash_attention: true
    gradient_checkpointing: true
    load_in_8bit: false          # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ trueë¡œ ì„¤ì •
    load_in_4bit: false          # ê·¹ë„ì˜ ë©”ëª¨ë¦¬ ì ˆì•½ í•„ìš” ì‹œ
  
  # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
  batch_processing:
    max_batch_size: 4
    enable_batching: true
```

---

## 4. BGE-M3 ì„ë² ë”© ëª¨ë¸ í†µí•©

### 4.1 BGE-M3 ì–´ëŒ‘í„° êµ¬í˜„

`src/infrastructure/embeddings/bge_m3_offline_adapter.py` íŒŒì¼ ìƒì„±:

```python
"""
íì‡„ë§ í™˜ê²½ì—ì„œì˜ BGE-M3 ì„ë² ë”© ëª¨ë¸ ì–´ëŒ‘í„°
ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
"""

import os
import torch
import numpy as np
from typing import List, Dict, Any, Optional, Union
from pathlib import Path

from FlagEmbedding import BGEM3FlagModel
from langchain_core.embeddings import Embeddings

from src.application.ports.embedding import EmbeddingPort


class BGEM3OfflineAdapter(EmbeddingPort, Embeddings):
    """íì‡„ë§ í™˜ê²½ì—ì„œ BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì–´ëŒ‘í„°"""
    
    def __init__(
        self,
        model_path: str,
        device: str = "auto",
        use_fp16: bool = True,
        max_length: int = 8192,
        batch_size: int = 12,
        normalize_embeddings: bool = True,
        **kwargs
    ):
        """
        Args:
            model_path: ë¡œì»¬ì— ì €ì¥ëœ BGE-M3 ëª¨ë¸ ê²½ë¡œ
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("cpu", "cuda", "auto")
            use_fp16: FP16 ì •ë°€ë„ ì‚¬ìš© ì—¬ë¶€ (GPUì—ì„œ ì„±ëŠ¥ í–¥ìƒ)
            max_length: ìµœëŒ€ í† í° ê¸¸ì´ (BGE-M3ëŠ” ìµœëŒ€ 8192 ì§€ì›)
            batch_size: ë°°ì¹˜ í¬ê¸°
            normalize_embeddings: ì„ë² ë”© ì •ê·œí™” ì—¬ë¶€
        """
        self.model_path = Path(model_path)
        self.device = self._get_device(device)
        self.use_fp16 = use_fp16 and self.device == "cuda"
        self.max_length = max_length
        self.batch_size = batch_size
        self.normalize_embeddings = normalize_embeddings
        
        # ëª¨ë¸ ë¡œë”©
        print(f"ğŸ”„ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... ({self.model_path})")
        self._load_model()
        print(f"âœ… BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    def _get_device(self, device: str) -> str:
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _load_model(self):
        """ë¡œì»¬ BGE-M3 ëª¨ë¸ ë¡œë”©"""
        try:
            self.model = BGEM3FlagModel(
                str(self.model_path),
                use_fp16=self.use_fp16,
                device=self.device,
                max_length=self.max_length,
                normalize_embeddings=self.normalize_embeddings
            )
            
            # ëª¨ë¸ ì •ë³´ í™•ì¸
            print(f"ğŸ“Š BGE-M3 ëª¨ë¸ ì •ë³´:")
            print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
            print(f"   - FP16 ì‚¬ìš©: {self.use_fp16}")
            print(f"   - ìµœëŒ€ ê¸¸ì´: {self.max_length}")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
            
        except Exception as e:
            raise RuntimeError(f"BGE-M3 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            # BGE-M3ì˜ dense ì„ë² ë”© ì‚¬ìš©
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                max_length=self.max_length
            )['dense_vecs']
            
            return embeddings.tolist()
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ ì˜ë²¡í„° ë°˜í™˜
            return [[0.0] * 1024 for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        try:
            # BGE-M3ëŠ” ì¿¼ë¦¬ì™€ ë¬¸ì„œì— ë™ì¼í•œ ë°©ì‹ ì‚¬ìš© (instruction ë¶ˆí•„ìš”)
            embedding = self.model.encode(
                [text],
                batch_size=1,
                max_length=self.max_length
            )['dense_vecs'][0]
            
            return embedding.tolist()
            
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return [0.0] * 1024
    
    def get_multi_embeddings(
        self, 
        texts: List[str], 
        return_dense: bool = True,
        return_sparse: bool = False,
        return_colbert_vecs: bool = False
    ) -> Dict[str, Any]:
        """
        BGE-M3ì˜ ë‹¤ì¤‘ ì„ë² ë”© ê¸°ëŠ¥ ì‚¬ìš©
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            return_dense: Dense ì„ë² ë”© ë°˜í™˜ ì—¬ë¶€
            return_sparse: Sparse ì„ë² ë”© ë°˜í™˜ ì—¬ë¶€
            return_colbert_vecs: ColBERT ìŠ¤íƒ€ì¼ ë²¡í„° ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            ë‹¤ì–‘í•œ íƒ€ì…ì˜ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        """
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                max_length=self.max_length,
                return_dense=return_dense,
                return_sparse=return_sparse,
                return_colbert_vecs=return_colbert_vecs
            )
            
            return embeddings
            
        except Exception as e:
            print(f"âŒ ë‹¤ì¤‘ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return {"dense_vecs": np.zeros((len(texts), 1024))}
    
    def compute_similarity(
        self, 
        query_embedding: List[float], 
        doc_embeddings: List[List[float]]
    ) -> List[float]:
        """
        ì¿¼ë¦¬ì™€ ë¬¸ì„œ ì„ë² ë”© ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            doc_embeddings: ë¬¸ì„œ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            query_vec = np.array(query_embedding)
            doc_vecs = np.array(doc_embeddings)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = np.dot(doc_vecs, query_vec) / (
                np.linalg.norm(doc_vecs, axis=1) * np.linalg.norm(query_vec)
            )
            
            return similarities.tolist()
            
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [0.0] * len(doc_embeddings)
    
    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        return 1024  # BGE-M3 ê¸°ë³¸ ì°¨ì›
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": "BGE-M3",
            "model_path": str(self.model_path),
            "device": self.device,
            "use_fp16": self.use_fp16,
            "max_length": self.max_length,
            "batch_size": self.batch_size,
            "embedding_dimension": self.get_embedding_dimension(),
            "features": [
                "Multi-lingual (100+ languages)",
                "Multi-granularity (up to 8192 tokens)",
                "Multi-functionality (dense, sparse, colbert)"
            ]
        }
```

### 4.2 BGE-M3 ì„¤ì • íŒŒì¼

`config/bge_m3_config.yaml` ìƒì„±:

```yaml
# BGE-M3 ì˜¤í”„ë¼ì¸ ì„ë² ë”© ì„¤ì •
bge_m3_offline:
  model_path: "/models/bge-m3"   # ë¡œì»¬ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
  device: "auto"                 # "cpu", "cuda", "auto"
  
  # ì„±ëŠ¥ ì„¤ì •
  performance:
    use_fp16: true               # GPUì—ì„œ FP16 ì‚¬ìš©
    batch_size: 12               # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
    max_length: 8192             # ìµœëŒ€ í† í° ê¸¸ì´
    normalize_embeddings: true   # ì„ë² ë”© ì •ê·œí™”
  
  # ë‹¤ì¤‘ ê¸°ëŠ¥ ì„¤ì •
  multi_functionality:
    return_dense: true           # Dense ì„ë² ë”© ë°˜í™˜
    return_sparse: false         # Sparse ì„ë² ë”© ë°˜í™˜ (í•„ìš”ì‹œ true)
    return_colbert_vecs: false   # ColBERT ë²¡í„° ë°˜í™˜ (í•„ìš”ì‹œ true)
  
  # ë©”ëª¨ë¦¬ ìµœì í™”
  optimization:
    enable_cache: true           # ì„ë² ë”© ìºì‹±
    cache_size: 10000           # ìºì‹œ ìµœëŒ€ í•­ëª© ìˆ˜
    clear_cache_interval: 1000   # ìºì‹œ ì •ë¦¬ ì£¼ê¸°
```

---

## 5. ì˜¤í”„ë¼ì¸ ì˜ì¡´ì„± ì„¤ì¹˜

### 5.1 ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ (ì¸í„°ë„· ì—°ê²° í™˜ê²½)

```bash
#!/bin/bash
# download_dependencies.sh - ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

# íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p offline_packages

# í•µì‹¬ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ
pip download -d offline_packages \
    torch==2.1.0 \
    transformers==4.36.0 \
    sentence-transformers==2.2.2 \
    FlagEmbedding==1.2.0 \
    numpy==1.24.3 \
    scipy==1.11.3 \
    scikit-learn==1.3.0 \
    huggingface_hub==0.19.0 \
    tokenizers==0.15.0 \
    accelerate==0.25.0 \
    safetensors==0.4.0

# GPU ì§€ì› íŒ¨í‚¤ì§€ (CUDA 11.8)
pip download -d offline_packages \
    torch==2.1.0+cu118 \
    torchvision==0.16.0+cu118 \
    torchaudio==2.1.0+cu118 \
    --extra-index-url https://download.pytorch.org/whl/cu118

# ì••ì¶•
tar -czf offline_packages.tar.gz offline_packages/
```

### 5.2 íì‡„ë§ì—ì„œì˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
#!/bin/bash
# install_offline.sh - íì‡„ë§ì—ì„œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

# íŒ¨í‚¤ì§€ ì••ì¶• í•´ì œ
tar -xzf offline_packages.tar.gz

# ì˜¤í”„ë¼ì¸ ì„¤ì¹˜
pip install --no-index --find-links offline_packages/ \
    torch transformers sentence-transformers \
    FlagEmbedding numpy scipy scikit-learn \
    huggingface_hub tokenizers accelerate safetensors

# ì„¤ì¹˜ í™•ì¸
python -c "
import torch
import transformers
from FlagEmbedding import BGEM3FlagModel
print('âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ')
print(f'PyTorch ë²„ì „: {torch.__version__}')
print(f'Transformers ë²„ì „: {transformers.__version__}')
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
"
```

### 5.3 UVë¥¼ ì´ìš©í•œ ì˜¤í”„ë¼ì¸ ì„¤ì¹˜

```bash
# pyproject.tomlì— ë¡œì»¬ íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
[tool.uv]
index-url = "file:///path/to/offline_packages"
extra-index-urls = []

# ì˜¤í”„ë¼ì¸ ì„¤ì¹˜
uv sync --offline --no-network
```

---

## 6. ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ í™˜ê²½ ì„¤ì •

### 6.1 í™˜ê²½ ì„¤ì • íŒŒì¼

`.env.offline` íŒŒì¼ ìƒì„±:

```bash
# íì‡„ë§ í™˜ê²½ ì„¤ì •
ENVIRONMENT=offline
NETWORK_MODE=isolated

# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
HCX_MODEL_PATH=/models/hcx-005
BGE_M3_MODEL_PATH=/models/bge-m3

# ë””ë°”ì´ìŠ¤ ì„¤ì •
DEVICE=auto  # auto, cpu, cuda
USE_GPU=true

# ìºì‹± ì„¤ì •
ENABLE_MODEL_CACHE=true
CACHE_DIR=/cache/ragtrace

# ë¡œê¹… ì„¤ì •
LOG_LEVEL=INFO
LOG_FILE=/logs/ragtrace.log

# API ì„¤ì • (ì˜¤í”„ë¼ì¸ì—ì„œëŠ” ë¹„í™œì„±í™”)
ENABLE_EXTERNAL_API=false
GEMINI_API_KEY=""
CLOVA_STUDIO_API_KEY=""
```

### 6.2 ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸

`scripts/init_offline_container.py`:

```python
#!/usr/bin/env python3
"""
íì‡„ë§ í™˜ê²½ì—ì„œ RAGTrace ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import logging
from pathlib import Path

def check_offline_requirements():
    """ì˜¤í”„ë¼ì¸ í™˜ê²½ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    print("ğŸ” íì‡„ë§ í™˜ê²½ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
    
    # ëª¨ë¸ íŒŒì¼ í™•ì¸
    hcx_path = Path(os.getenv("HCX_MODEL_PATH", "/models/hcx-005"))
    bge_path = Path(os.getenv("BGE_M3_MODEL_PATH", "/models/bge-m3"))
    
    if not hcx_path.exists():
        print(f"âŒ HCX-005 ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {hcx_path}")
        return False
    
    if not bge_path.exists():
        print(f"âŒ BGE-M3 ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {bge_path}")
        return False
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    required_files = {
        "HCX-005": ["config.json", "pytorch_model.bin", "tokenizer.json"],
        "BGE-M3": ["config.json", "pytorch_model.bin", "tokenizer.json"]
    }
    
    for model_name, model_path in [("HCX-005", hcx_path), ("BGE-M3", bge_path)]:
        for file_name in required_files[model_name]:
            file_path = model_path / file_name
            if not file_path.exists():
                print(f"âŒ {model_name} í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {file_path}")
                return False
    
    print("âœ… ëª¨ë“  ëª¨ë¸ íŒŒì¼ í™•ì¸ ì™„ë£Œ")
    return True

def setup_offline_models():
    """ì˜¤í”„ë¼ì¸ ëª¨ë¸ ì„¤ì •"""
    print("ğŸ”§ ì˜¤í”„ë¼ì¸ ëª¨ë¸ ì„¤ì • ì¤‘...")
    
    try:
        # HCX-005 ëª¨ë¸ í…ŒìŠ¤íŠ¸
        from src.infrastructure.llm.hcx_offline_adapter import HCXOfflineAdapter
        hcx_path = os.getenv("HCX_MODEL_PATH", "/models/hcx-005")
        hcx_adapter = HCXOfflineAdapter(hcx_path)
        print("âœ… HCX-005 ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        
        # BGE-M3 ëª¨ë¸ í…ŒìŠ¤íŠ¸
        from src.infrastructure.embeddings.bge_m3_offline_adapter import BGEM3OfflineAdapter
        bge_path = os.getenv("BGE_M3_MODEL_PATH", "/models/bge-m3")
        bge_adapter = BGEM3OfflineAdapter(bge_path)
        print("âœ… BGE-M3 ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ RAGTrace íì‡„ë§ í™˜ê²½ ì´ˆê¸°í™” ì‹œì‘")
    print("=" * 50)
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    if os.path.exists(".env.offline"):
        from dotenv import load_dotenv
        load_dotenv(".env.offline")
        print("âœ… ì˜¤í”„ë¼ì¸ í™˜ê²½ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    
    # ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not check_offline_requirements():
        print("âŒ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ëª¨ë¸ ì„¤ì •
    if not setup_offline_models():
        print("âŒ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨")
        sys.exit(1)
    
    print("ğŸ‰ íì‡„ë§ í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ!")
    print("RAGTrace ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
```

### 6.3 Docker ì˜¤í”„ë¼ì¸ ì„¤ì •

`Dockerfile.offline`:

```dockerfile
# RAGTrace íì‡„ë§ í™˜ê²½ìš© Dockerfile
FROM python:3.11-slim

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì˜¤í”„ë¼ì¸ íŒ¨í‚¤ì§€ ë³µì‚¬ ë° ì„¤ì¹˜
COPY offline_packages/ /tmp/offline_packages/
RUN pip install --no-index --find-links /tmp/offline_packages/ \
    torch transformers sentence-transformers \
    FlagEmbedding numpy scipy scikit-learn \
    huggingface_hub tokenizers accelerate safetensors

# ëª¨ë¸ íŒŒì¼ ë³µì‚¬
COPY models/ /models/

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY src/ /app/src/
COPY config/ /app/config/
COPY scripts/ /app/scripts/
COPY .env.offline /app/.env

# ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
RUN python scripts/init_offline_container.py

# ë¹„ë£¨íŠ¸ ì‚¬ìš©ì ìƒì„±
RUN useradd -m -u 1000 ragtrace && \
    chown -R ragtrace:ragtrace /app /models

USER ragtrace

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONPATH=/app
ENV ENVIRONMENT=offline

EXPOSE 8501

CMD ["python", "src/presentation/main.py"]
```

---

## 7. ì„±ëŠ¥ ìµœì í™”

### 7.1 ë©”ëª¨ë¦¬ ìµœì í™”

```python
# scripts/optimize_memory.py
"""ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸"""

import torch
import gc
from typing import Dict, Any

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” ê´€ë¦¬ì"""
    
    @staticmethod
    def optimize_torch_settings():
        """PyTorch ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”"""
        if torch.cuda.is_available():
            # CUDA ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
        # CPU ìŠ¤ë ˆë“œ ìµœì í™”
        torch.set_num_threads(min(8, torch.get_num_threads()))
    
    @staticmethod
    def cleanup_memory():
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    @staticmethod
    def get_memory_info() -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        info = {
            "cpu_memory_gb": psutil.virtual_memory().used / (1024**3),
            "cpu_memory_percent": psutil.virtual_memory().percent
        }
        
        if torch.cuda.is_available():
            info.update({
                "gpu_memory_allocated_gb": torch.cuda.memory_allocated() / (1024**3),
                "gpu_memory_reserved_gb": torch.cuda.memory_reserved() / (1024**3),
                "gpu_memory_total_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3)
            })
        
        return info
```

### 7.2 ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
# src/utils/batch_processor.py
"""ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""

from typing import List, Generator, TypeVar, Callable
import math

T = TypeVar('T')

class BatchProcessor:
    """íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def create_batches(items: List[T], batch_size: int) -> Generator[List[T], None, None]:
        """ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• """
        for i in range(0, len(items), batch_size):
            yield items[i:i + batch_size]
    
    @staticmethod
    def process_with_batches(
        items: List[T],
        processor: Callable[[List[T]], List[Any]],
        batch_size: int = 32,
        show_progress: bool = True
    ) -> List[Any]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ í•­ëª© ì²˜ë¦¬"""
        results = []
        total_batches = math.ceil(len(items) / batch_size)
        
        for i, batch in enumerate(BatchProcessor.create_batches(items, batch_size)):
            if show_progress:
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}/{total_batches}")
            
            batch_results = processor(batch)
            results.extend(batch_results)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % 10 == 0:  # 10ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                MemoryOptimizer.cleanup_memory()
        
        return results
```

### 7.3 ìºì‹± ì‹œìŠ¤í…œ

```python
# src/utils/embedding_cache.py
"""ì„ë² ë”© ìºì‹± ì‹œìŠ¤í…œ"""

import pickle
import hashlib
from pathlib import Path
from typing import List, Optional, Dict, Any
import sqlite3

class EmbeddingCache:
    """ì„ë² ë”© ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = "/cache/embeddings", max_entries: int = 10000):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_entries = max_entries
        self.db_path = self.cache_dir / "embeddings.db"
        self._init_db()
    
    def _init_db(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS embeddings (
                    text_hash TEXT PRIMARY KEY,
                    embedding_data BLOB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
    
    def _hash_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±"""
        return hashlib.sha256(text.encode()).hexdigest()
    
    def get(self, text: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ"""
        text_hash = self._hash_text(text)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT embedding_data FROM embeddings WHERE text_hash = ?",
                (text_hash,)
            )
            result = cursor.fetchone()
            
            if result:
                return pickle.loads(result[0])
        
        return None
    
    def set(self, text: str, embedding: List[float]):
        """ìºì‹œì— ì„ë² ë”© ì €ì¥"""
        text_hash = self._hash_text(text)
        embedding_data = pickle.dumps(embedding)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO embeddings (text_hash, embedding_data) VALUES (?, ?)",
                (text_hash, embedding_data)
            )
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            conn.execute("""
                DELETE FROM embeddings WHERE text_hash IN (
                    SELECT text_hash FROM embeddings 
                    ORDER BY created_at ASC 
                    LIMIT MAX(0, (SELECT COUNT(*) FROM embeddings) - ?)
                )
            """, (self.max_entries,))
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM embeddings")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM embeddings")
            count = cursor.fetchone()[0]
            
            cursor.execute("SELECT SUM(LENGTH(embedding_data)) FROM embeddings")
            size_bytes = cursor.fetchone()[0] or 0
        
        return {
            "entry_count": count,
            "size_mb": size_bytes / (1024 * 1024),
            "max_entries": self.max_entries
        }
```

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 8.1 ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

#### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```python
# ë¬¸ì œ: CUDA out of memory
# í•´ê²°: ëª¨ë¸ ë¡œë”© ì‹œ ë©”ëª¨ë¦¬ ìµœì í™”
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ì ˆì•½
    device_map="auto",          # ìë™ ë””ë°”ì´ìŠ¤ ë°°ì¹˜
    load_in_8bit=True,         # 8bit ì–‘ìí™”
    # load_in_4bit=True,       # ê·¹ë„ì˜ ë©”ëª¨ë¦¬ ì ˆì•½ í•„ìš” ì‹œ
)
```

#### í† í¬ë‚˜ì´ì € ì˜¤ë¥˜
```python
# ë¬¸ì œ: tokenizer.json not found
# í•´ê²°: í† í¬ë‚˜ì´ì € íŒŒì¼ í™•ì¸ ë° ì¬ë‹¤ìš´ë¡œë“œ
if not (model_path / "tokenizer.json").exists():
    print("í† í¬ë‚˜ì´ì € íŒŒì¼ ëˆ„ë½")
    # ë°±ì—… í† í¬ë‚˜ì´ì € ì‚¬ìš©
    tokenizer = AutoTokenizer.from_pretrained(
        "bert-base-multilingual-cased",
        local_files_only=True
    )
```

#### ì„±ëŠ¥ ì €í•˜
```python
# ë¬¸ì œ: ëŠë¦° ì¶”ë¡  ì†ë„
# í•´ê²°: ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹± í™œìš©
def optimize_inference():
    # 1. ë°°ì¹˜ í¬ê¸° ì¡°ì •
    batch_size = 4 if torch.cuda.is_available() else 1
    
    # 2. ì»´íŒŒì¼ ìµœì í™” (PyTorch 2.0+)
    if hasattr(torch, 'compile'):
        model = torch.compile(model)
    
    # 3. ìºì‹± í™œìš©
    cache = EmbeddingCache()
    
    return batch_size, model, cache
```

### 8.2 ë¡œê·¸ ì„¤ì •

```python
# config/logging_config.py
"""ì˜¤í”„ë¼ì¸ í™˜ê²½ ë¡œê¹… ì„¤ì •"""

import logging
import sys
from pathlib import Path

def setup_offline_logging(log_level: str = "INFO", log_file: str = "/logs/ragtrace.log"):
    """ì˜¤í”„ë¼ì¸ í™˜ê²½ìš© ë¡œê¹… ì„¤ì •"""
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_path = Path(log_file)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # íŠ¹ì • ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("torch").setLevel(logging.WARNING)
    logging.getLogger("FlagEmbedding").setLevel(logging.INFO)
    
    logger = logging.getLogger("RAGTrace")
    logger.info("ì˜¤í”„ë¼ì¸ í™˜ê²½ ë¡œê¹… ì„¤ì • ì™„ë£Œ")
    
    return logger
```

### 8.3 ê±´ê°• ìƒíƒœ ì²´í¬

```python
# scripts/health_check.py
"""ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸"""

import torch
import psutil
import time
from pathlib import Path

def check_system_health():
    """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ í™•ì¸"""
    health_report = {
        "timestamp": time.time(),
        "status": "healthy",
        "issues": []
    }
    
    # CPU ì‚¬ìš©ë¥  í™•ì¸
    cpu_percent = psutil.cpu_percent(interval=1)
    if cpu_percent > 90:
        health_report["issues"].append(f"ë†’ì€ CPU ì‚¬ìš©ë¥ : {cpu_percent}%")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
    memory = psutil.virtual_memory()
    if memory.percent > 90:
        health_report["issues"].append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent}%")
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
        if gpu_memory > 0.9:
            health_report["issues"].append(f"ë†’ì€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {gpu_memory*100:.1f}%")
    
    # ëª¨ë¸ íŒŒì¼ í™•ì¸
    model_paths = [
        Path("/models/hcx-005"),
        Path("/models/bge-m3")
    ]
    
    for path in model_paths:
        if not path.exists():
            health_report["issues"].append(f"ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {path}")
    
    # ì „ì²´ ìƒíƒœ ê²°ì •
    if health_report["issues"]:
        health_report["status"] = "warning" if len(health_report["issues"]) < 3 else "critical"
    
    return health_report

if __name__ == "__main__":
    report = check_system_health()
    print(f"ì‹œìŠ¤í…œ ìƒíƒœ: {report['status']}")
    if report['issues']:
        print("ë°œê²¬ëœ ë¬¸ì œ:")
        for issue in report['issues']:
            print(f"  - {issue}")
    else:
        print("ëª¨ë“  ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ ì¤‘")
```

---

## ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ í†µí•´ íì‡„ë§ í™˜ê²½ì—ì„œ HCX-005 LLMê³¼ BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ê³ ë ¤ì‚¬í•­:

1. **ì‚¬ì „ ì¤€ë¹„**: ëª¨ë“  ëª¨ë¸ íŒŒì¼ê³¼ ì˜ì¡´ì„±ì„ ì¸í„°ë„· ì—°ê²° í™˜ê²½ì—ì„œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ
2. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì œí•œëœ ë¦¬ì†ŒìŠ¤ì—ì„œ íš¨ìœ¨ì ì¸ ëª¨ë¸ ìš´ì˜ì„ ìœ„í•œ ìµœì í™”
3. **ìºì‹± í™œìš©**: ë°˜ë³µì ì¸ ì‘ì—…ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì§€ëŠ¥ì  ìºì‹±
4. **ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœì™€ ì„±ëŠ¥ ì§€ì†ì  ëª¨ë‹ˆí„°ë§

íì‡„ë§ í™˜ê²½ì˜ íŠ¹ì„±ìƒ ë¬¸ì œ ë°œìƒ ì‹œ ì™¸ë¶€ ì§€ì›ì´ ì œí•œë˜ë¯€ë¡œ, ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ì™€ ë¬¸ì„œí™”ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.