# 사용자 행동 시나리오 기반 잠재적 위험 분석 및 개선 보고서

## 1. 개요

이 문서는 RAGTrace 사용자의 예상 행동 시나리오를 정의하고, 각 시나리오에서 발생할 수 있는 잠재적 문제점과 위험 요소를 코드 수준에서 분석합니다. 이 분석의 목적은 "Happy Path"(성공 경로)를 넘어, 엣지 케이스와 예외 상황에서 발생할 수 있는 사용자 경험(UX) 저하 요인을 사전에 식별하고, 시스템의 견고성을 높이기 위한 구체적인 개선 방안을 도출하는 데 있습니다.

---

## 2. 핵심 사용자 워크플로우

RAGTrace의 핵심 기능은 **"특정 데이터셋과 프롬프트 설정을 사용하여 RAG 시스템의 성능을 평가하는 것"** 이며, 이 과정에서 다양한 예외 상황이 발생할 수 있습니다.

---

## 3. 시나리오별 잠재적 위험 및 개선 방안

### 시나리오 1: 데이터 유효성 문제 (Invalid Data)

-   **상황**: 사용자가 준비한 데이터 파일에 필수 필드(`question`, `contexts` 등)가 누락되었거나, JSON 형식이 깨졌거나, 특정 행의 값이 비어있는 경우.
-   **잠재적 위험 및 코드 동작 분석**:
    -   `file_adapter.py`는 포괄적인 `except Exception`으로 오류를 처리하고 빈 리스트(`[]`)를 반환합니다.
    -   `RunEvaluationUseCase`는 빈 리스트를 받고 "평가 데이터가 없습니다." 라는 일반적인 오류만 발생시킵니다.
    -   **사용자 경험 문제**: 사용자는 **왜 데이터가 유효하지 않은지, 파일의 어느 부분에 문제가 있는지 전혀 알 수 없어** 반복적으로 실패하며 불편을 겪게 됩니다. "Garbage In, Garbage Out"을 넘어, 시스템이 사용자에게 유용한 피드백을 주지 못하는 상황입니다.
-   **개선 방안**: **데이터 사전 검증 (Pre-flight Check) 기능 도입**
    -   평가 로직 실행 전에, 데이터의 스키마(필수 컬럼 존재 여부)와 내용(빈 값, 짧은 텍스트 여부)을 먼저 검증하는 `DataContentValidator`와 같은 서비스를 도입합니다.
    -   문제가 발견되면, "항목 5의 'contexts' 필드가 비어있습니다."와 같이 사용자 친화적인 언어로 명확하게 문제를 알려주고, 평가 진행 여부를 선택하게 하거나 안전하게 프로세스를 중단시킵니다.

### 시나리오 2: 외부 API 장애 (External API Failure)

-   **상황**: 답변 생성을 위해 호출하는 외부 LLM API(예: Gemini)가 일시적으로 불안정하거나, 사용자의 API 키가 잘못되었거나, 할당된 요청량(Quota)을 초과한 경우.
-   **잠재적 위험 및 코드 동작 분석**:
    -   `RunEvaluationUseCase`는 `try-except`로 개별 API 호출 실패를 처리하지만, 실패한 항목의 답변을 빈 문자열(`""`)로 설정하고 다음으로 넘어갑니다.
    -   **사용자 경험 문제**: 평가는 성공한 것처럼 보이지만, 일부 답변이 누락되어 **전체 점수가 비정상적으로 낮게 나옵니다.** 사용자는 낮은 점수의 원인이 API 장애였다는 사실을 인지하지 못하고, RAG 시스템 자체의 성능이 낮다고 오해하여 결과의 신뢰성에 큰 타격을 줍니다.
-   **개선 방안**: **방어적 설계 및 실패 정보의 명시적 전달**
    -   `RunEvaluationUseCase` 내에서 답변 생성 실패 횟수를 집계하고, 상세한 오류 내역을 기록합니다.
    -   `EvaluationResult` 도메인 엔티티에 `generation_failures`, `api_failure_details` 같은 메타데이터 필드를 추가합니다.
    -   최종 결과 보고 시, "주의: 총 100개 중 15개 답변 생성 실패. (상세: API Quota Exceeded)" 와 같이 명확한 경고 메시지를 표시하여 사용자가 결과의 신뢰도를 판단할 수 있도록 돕습니다.

### 시나리오 3: 대용량 데이터 처리 (Large-Scale Data)

-   **상황**: 사용자가 수백, 수천 건 이상의 대규모 데이터셋으로 평가를 시도하는 경우.
-   **잠재적 위험 및 코드 동작 분석**:
    -   현재 시스템은 `RunEvaluationUseCase.execute()`를 **동기(Synchronous) 방식**으로 처리합니다.
    -   **사용자 경험 문제**: 평가가 진행되는 수 분, 혹은 수십 분 동안 **UI는 아무런 반응 없이 멈추게 됩니다(Freezing)**. 사용자는 시스템이 정상 동작하는지, 다운된 것인지 알 수 없어 답답함을 느끼며, 웹 브라우저의 타임아웃으로 연결이 끊길 수도 있습니다.
-   **개선 방안**: **비동기 처리 및 백그라운드 실행 도입**
    -   `Celery`나 `FastAPI`의 `BackgroundTasks`와 같은 태스크 큐를 도입하여, 평가 작업을 백그라운드에서 비동기적으로 처리합니다.
    -   사용자가 평가를 요청하면 시스템은 즉시 작업 ID를 반환하고, 사용자는 별도의 명령어로 진행 상태를 확인하거나 작업 완료 알림을 받을 수 있습니다.
    -   진행 상태(예: "답변 생성 중 (50/500)...")를 DB나 캐시에 기록하고, UI가 이를 폴링하여 사용자에게 보여주는 방식을 고려할 수 있습니다.

---

## 4. 결론

RAGTrace 프로젝트의 아키텍처는 기술적으로 견고하지만, 실제 사용자가 마주할 **데이터의 불완전성, 외부 시스템의 불안정성, 대기 시간** 등에 대한 방어적인 설계 보강이 필요합니다.

위에 제안된 개선 방안, 특히 **데이터 사전 검증**과 **API 장애 처리의 명시적 결과 반영**은 시스템의 신뢰성을 높이고 사용자의 혼란을 줄이는 데 핵심적인 역할을 합니다. 이를 통해 RAGTrace는 기술적으로 우수한 도구를 넘어, 개발자들이 믿고 사용할 수 있는 실용적이고 안정적인 평가 플랫폼으로 완성될 것입니다. 