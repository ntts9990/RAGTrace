# 사용자 행동 시나리오 분석 및 잠재적 오류 보고서

## 1. 개요

이 문서는 현재 RAGTrace 프로젝트의 아키텍처를 기반으로, 실제 사용자가 시스템과 상호작용하는 과정에서 발생할 수 있는 다양한 시나리오를 예측하고, 각 상황에서 코드의 동작과 잠재적인 문제를 분석합니다.

분석의 목적은 "Happy Path" (정상적인 성공 경로)를 넘어, 엣지 케이스(Edge Case)와 예외 상황에서 발생할 수 있는 사용자 경험(UX) 저하 요인을 사전에 식별하고, 시스템의 견고성을 한층 더 높이기 위한 개선 방안을 도출하는 것입니다.

---

## 2. 핵심 사용자 워크플로우 분석

RAGTrace의 핵심 기능은 **"특정 데이터셋과 프롬프트 설정을 사용하여 RAG 시스템의 성능을 평가하는 것"** 입니다. 이 과정은 다음과 같은 사용자 행동 흐름으로 정의할 수 있습니다.

1.  **데이터셋 선택/업로드**: 사용자는 평가를 원하는 데이터셋(`dataset_name`)을 UI(웹 대시보드)에서 선택합니다. 이 데이터셋은 사전에 정의된 경로에 JSON 파일 형태로 존재해야 합니다.
2.  **평가 옵션 선택**: 사용자는 필요에 따라 특정 평가 프롬프트(`prompt_type`)를 선택합니다. 선택하지 않으면 기본값이 사용됩니다.
3.  **평가 실행**: 사용자가 '평가 시작' 버튼을 클릭하면, `run_evaluation_use_case.execute()` 메서드가 호출됩니다.
4.  **시스템 내부 처리**:
    a.  **답변 생성**: 시스템은 데이터셋을 로드하고, 답변(`answer`)이 비어있는 항목에 대해 `LlmPort`를 통해 답변을 생성합니다.
    b.  **성능 평가**: 모든 답변이 준비되면, `EvaluationRunnerPort`를 통해 `Ragas` 평가를 실행하여 각 지표의 점수를 계산합니다.
5.  **결과 확인**: 처리가 완료되면, 사용자는 UI를 통해 종합 점수 및 개별 항목의 평가 점수를 확인합니다.

---

## 3. 시나리오별 잠재적 문제 및 개선 방안

위 워크플로우의 각 단계에서 발생할 수 있는 예외적인 상황과, 그로 인해 사용자가 겪을 수 있는 문제점을 코드 기반으로 분석하고 개선 방안을 제안합니다.

### 시나리오 1: 유효하지 않은 형식의 데이터를 업로드하는 경우

-   **상황**: 사용자가 선택한 데이터셋 JSON 파일의 특정 항목에 필수 필드(`question`, `contexts`, `ground_truth`)가 누락되었거나, JSON 형식 자체가 깨진 경우.
-   **현재 코드 동작 분석**:
    -   `src/infrastructure/repository/file_adapter.py`의 `load_data` 메서드는 `json.JSONDecodeError`나 `EvaluationData` 모델(Pydantic)의 유효성 검사 실패(`ValidationError`)를 포괄적인 `except Exception` 블록에서 처리합니다.
    -   오류 발생 시, 콘솔에만 로그를 출력하고 빈 리스트(`[]`)를 반환합니다.
    -   `RunEvaluationUseCase`는 빈 리스트를 받고 `EvaluationError("평가 데이터가 없습니다.")`라는 일반적인 오류를 발생시킵니다.
-   **사용자 경험 문제**: 사용자는 "평가 데이터가 없다"는 메시지만 볼 뿐, **왜 데이터가 유효하지 않은지, 파일의 어느 부분에 문제가 있는지 전혀 알 수 없습니다.** 이로 인해 사용자는 반복적으로 실패하며 큰 불편을 겪게 됩니다.
-   **개선 제안**:
    1.  **구체적인 예외 처리**: `file_adapter.py`에서 `ValidationError`를 별도로 `except`하여, 어떤 필드가 누락되었는지에 대한 상세한 오류 메시지를 생성합니다.
    2.  **상세 오류 정보 전달**: 생성된 상세 오류 메시지를 담은 커스텀 예외(`InvalidDataFormatError`)를 발생시켜 `UseCase`를 통해 UI 계층까지 전달합니다.
    3.  **사용자 친화적 피드백**: UI는 "3번째 항목에 'question' 필드가 없습니다." 와 같이 사용자가 즉시 수정할 수 있는 구체적인 피드백을 표시해 줍니다.

### 시나리오 2: 외부 API(Gemini) 장애가 발생하는 경우

-   **상황**: 평가 실행 중, 답변 생성을 위해 호출하는 Gemini API 서버가 응답하지 않거나, 사용자의 API 키가 유효하지 않거나, 할당량(Quota)을 초과한 경우.
-   **현재 코드 동작 분석**:
    -   `src/application/use_cases/run_evaluation.py`의 `execute` 메서드 내 답변 생성 루프는 개별적인 `try-except` 블록으로 감싸여 있습니다.
    -   API 호출 실패 시, 콘솔에 오류 로그를 남기고 해당 항목의 답변을 빈 문자열(`""`)로 설정한 후, 다음 항목으로 넘어갑니다.
    -   최종 평가는 **답변이 비어있는 상태로 진행**됩니다.
-   **사용자 경험 문제**:
    -   평가는 성공적으로 완료된 것처럼 보이지만, 실제로는 일부 항목이 실패했기 때문에 **전체 점수가 비정상적으로 낮게 나올 수 있습니다.**
    -   사용자는 낮은 점수의 원인이 API 장애였다는 사실을 인지하지 못하고, RAG 시스템 자체의 성능이 낮다고 오해할 수 있습니다. 신뢰성에 큰 타격을 줍니다.
-   **개선 제안**:
    1.  **실패 카운팅**: `UseCase` 내에서 답변 생성 실패 횟수를 집계합니다.
    2.  **결과 객체 확장**: `EvaluationResult` 도메인 엔티티에 `generation_failures` (답변 생성 실패 수), `generation_successes` 같은 메타데이터 필드를 추가합니다.
    3.  **명시적인 경고 표시**: UI는 최종 결과와 함께 "주의: 총 100개 항목 중 15개의 답변 생성에 실패했습니다. API 상태 또는 할당량을 확인하세요." 와 같은 명확한 경고 메시지를 표시하여 사용자가 결과의 신뢰도를 판단할 수 있도록 돕습니다.

### 시나리오 3: 대용량 데이터셋을 평가하는 경우

-   **상황**: 사용자가 수백, 수천 건 이상의 대규모 데이터셋에 대한 평가를 실행하는 경우.
-   **현재 코드 동작 분석**:
    -   `RunEvaluationUseCase.execute()`는 **동기(Synchronous) 방식**으로 동작합니다. 즉, 데이터 로딩, 모든 답변 생성, 전체 평가 과정이 끝날 때까지 메서드는 반환되지 않습니다.
-   **사용자 경험 문제**:
    -   평가가 진행되는 수 분, 혹은 수십 분 동안 **UI는 아무런 반응 없이 멈춰있게 됩니다(Freezing)**.
    -   사용자는 시스템이 정상 동작하는지, 다운된 것인지 알 수 없어 답답함을 느끼며, 웹 브라우저의 타임아웃으로 인해 연결이 끊길 수도 있습니다.
-   **개선 제안**:
    1.  **비동기 태스크 큐 도입**: `Celery`나 `Dramatiq` 같은 백그라운드 태스크 큐를 도입하여, `execute`와 같은 장시간 실행되는 작업을 워커(Worker) 프로세스에 위임합니다.
    2.  **즉각적인 피드백**: UI는 '평가 시작' 클릭 시 즉시 "평가 작업이 백그라운드에서 시작되었습니다. 완료되면 알려드리겠습니다." 와 같은 메시지를 사용자에게 보여줍니다.
    3.  **진행 상태 업데이트 (선택사항)**: 더 나은 경험을 위해, `UseCase`가 진행 상태(예: "답변 생성 중 (50/500)...", "평가 진행 중...")를 데이터베이스나 캐시에 기록하고, UI가 이를 주기적으로 폴링(Polling)하거나 웹소켓(WebSocket)을 통해 실시간으로 업데이트를 받아 진행률을 표시해 줄 수 있습니다.

### 시나리오 4: 내용이 부실한 데이터를 업로드하는 경우

-   **상황**: 데이터셋의 내용은 유효한 형식이지만, `question`이 비어있거나 `contexts` 리스트가 비어있는 등, 평가의 의미를 퇴색시키는 데이터가 포함된 경우.
-   **현재 코드 동작 분석**:
    -   현재 코드는 데이터의 내용 자체에 대한 유효성 검사를 수행하지 않습니다.
    -   `contexts`가 비어있으면 근거할 컨텍스트 없이 답변이 생성(환각 발생 가능성 높음)되고, `question`이 비어있으면 의미 없는 답변이 생성됩니다.
-   **사용자 경험 문제**: "Garbage In, Garbage Out". 시스템은 정상적으로 동작하지만, 결과는 무의미합니다. 사용자는 왜 점수가 이상하게 나왔는지 원인을 파악하기 어렵고, 데이터 준비의 중요성을 간과할 수 있습니다.
-   **개선 제안**:
    1.  **데이터 사전 검증 (Pre-flight Check)**: `UseCase`의 `execute` 메서드 초반에, `load_data` 이후 데이터의 내용을 검증하는 단계를 추가합니다.
    2.  **검증 로직**: 각 `EvaluationData` 객체를 순회하며 `if not data.question:` 이나 `if not data.contexts:` 와 같은 조건을 검사합니다.
    3.  **사용자에게 선택권 부여**: 검증 단계에서 발견된 문제점들(예: "10개 항목에서 컨텍스트가 비어있습니다.")을 사용자에게 경고 메시지로 보여주고, "무시하고 계속 진행" 또는 "취소" 옵션을 제공합니다. 이는 사용자가 데이터의 문제를 인지하고 수정할 기회를 줍니다.

---

## 4. 종합 결론

현재 RAGTrace 프로젝트의 아키텍처는 **기술적으로 매우 견고하고 확장 가능하게 잘 설계**되어 있습니다. 핵심 로직과 계층 분리는 이상적입니다.

그러나 실제 사용자가 마주할 현실적인 문제들, 즉 **데이터의 불완전성, 외부 시스템의 불안정성, 대기 시간** 등에 대한 방어적인 설계는 아직 보완할 점이 있습니다.

위에 제안된 개선 방안들은 **시스템의 신뢰성을 높이고 사용자의 혼란을 줄이는 데** 중점을 둡니다. 이러한 사용자 중심의 예외 처리를 강화함으로써, RAGTrace는 기술적으로 우수한 도구를 넘어, 개발자들이 믿고 사용할 수 있는 실용적이고 안정적인 평가 플랫폼으로 완성될 것입니다. 